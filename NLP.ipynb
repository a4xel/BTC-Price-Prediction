{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load External Lib For nlp bitcoin analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow for machine learning\n",
    "import tensorflow as tf\n",
    "import swifter\n",
    "# Numpy and pandas for dataset manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Seaborn and matplotlib to make graphics \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Skelarn metrics to get confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to create dataset and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create random dataset X, Y with different lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subsample_sequence(df, length):\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length`.\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    last_possible = df.shape[0] - length\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    df_sample = df[random_start: random_start+length]\n",
    "    return df_sample\n",
    "\n",
    "def split_subsample_sequence(df, length,y):\n",
    "    '''Create one single random (X,y) pair'''\n",
    "    df_subsample = subsample_sequence(df, length)\n",
    "    y_sample = df_subsample.iloc[length -1][y]\n",
    "    X_sample = df_subsample[0:length -1].drop(columns=y)\n",
    "    X_sample = X_sample.values\n",
    "    return np.array(X_sample), np.array(y_sample)\n",
    "\n",
    "def get_X_y_random(df, n_sequences, length, y_class):\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X, y = [], []\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = split_subsample_sequence(df, length,y_class)\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create continous dataset X, Y with same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_same(dataframe, x_number_data,  step, y_class):\n",
    "    ''' Return a continuous dataset X y with X_number of data in row of x and y'''\n",
    "    x= []\n",
    "    y= []\n",
    "\n",
    "    for i in range(0,len(dataframe)-x_number_data,step):\n",
    "        \n",
    "        x.append(dataframe.iloc[i:i+x_number_data].to_numpy())\n",
    "        y.append(dataframe[y_class].iloc[i+x_number_data])\n",
    "        \n",
    "    return np.array(x) , np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check confusion matrix and learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_get_conf_learning(model,history, X_test, y_test ):\n",
    "    \n",
    "    # Predict class for X_test\n",
    "    predictions = model.predict(X_test)\n",
    "    # Affect class\n",
    "    y_mean_pred = [1 * (x[0]>=0.5) for x in predictions]\n",
    "    # Create confusion matrix\n",
    "    cf_matrix = confusion_matrix( y_test, y_mean_pred)\n",
    "\n",
    "    #Create subplot\n",
    "    fig, axes = plt.subplots(1, 2,  figsize=(15,4))\n",
    "\n",
    "    # Confusion matrix plotting\n",
    "    axes[0].set_title(' Confusion Matrix with labels\\n\\n');\n",
    "    sns.heatmap(ax =axes[0], data = cf_matrix, annot=True, cmap='Reds')\n",
    "\n",
    "\n",
    "    # Learning curves plotting\n",
    "    loss = history.history[\"loss\"]\n",
    "    loss_val = history.history[\"val_loss\"]\n",
    "    sns.lineplot(ax=axes[1],data =[loss,loss_val])\n",
    "    axes[1].set_title('Learning curves \\n\\n')\n",
    "    axes[1].legend(labels=[\"Train set\",\"Test set\"], title = \"Learning curves\")\n",
    "    \n",
    "    #Print classification report\n",
    "    print(classification_report(y_test, y_mean_pred))\n",
    "    #Show graphics\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, process and merge bitcoin and reddit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load daily aggregation of reddit sentimental analysis dataset\n",
    "daily_reddit = pd.read_csv(\"data_reddit_btc/reddit_day.csv\")\n",
    "\n",
    "#Load bitcoin daily analysis dataset\n",
    "daily_btc = pd.read_csv(\"data_reddit_btc/DatasetChrisDaily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>tenkan_sen_shifted</th>\n",
       "      <th>kijun_sen_shifted</th>\n",
       "      <th>chikou_shifted</th>\n",
       "      <th>close_shifted</th>\n",
       "      <th>indice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>147.488007</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.444000</td>\n",
       "      <td>144.539993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.979996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.559998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.479996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.520004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.744995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-12.754997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23.790001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.300003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-34.550003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date        High         Low        Open       Close  \\\n",
       "0           0  2013-04-29  147.488007  134.000000  134.444000  144.539993   \n",
       "1           1  2013-04-30  146.929993  134.050003  144.000000  139.000000   \n",
       "2           2  2013-05-01  139.889999  107.720001  139.000000  116.989998   \n",
       "3           3  2013-05-02  125.599998   92.281898  116.379997  105.209999   \n",
       "4           4  2013-05-03  108.127998   79.099998  106.250000   97.750000   \n",
       "\n",
       "   Volume  tenkan_sen  kijun_sen  senkou_span_a  senkou_span_b  chikou_span  \\\n",
       "0     0.0         NaN        NaN            NaN            NaN   131.979996   \n",
       "1     0.0         NaN        NaN            NaN            NaN   133.479996   \n",
       "2     0.0         NaN        NaN            NaN            NaN   129.744995   \n",
       "3     0.0         NaN        NaN            NaN            NaN   129.000000   \n",
       "4     0.0         NaN        NaN            NaN            NaN   132.300003   \n",
       "\n",
       "   diff_kijun  diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  \\\n",
       "0         NaN          NaN    12.559998             0              0   \n",
       "1         NaN          NaN     5.520004             0              0   \n",
       "2         NaN          NaN   -12.754997             0              0   \n",
       "3         NaN          NaN   -23.790001             0              0   \n",
       "4         NaN          NaN   -34.550003             0              0   \n",
       "\n",
       "   chikou_signal  tenkan_sen_shifted  kijun_sen_shifted  chikou_shifted  \\\n",
       "0              0                 NaN                NaN             NaN   \n",
       "1              0                 NaN                NaN             NaN   \n",
       "2              0                 NaN                NaN             NaN   \n",
       "3              0                 NaN                NaN             NaN   \n",
       "4              0                 NaN                NaN             NaN   \n",
       "\n",
       "   close_shifted  indice  \n",
       "0            NaN       0  \n",
       "1            NaN       0  \n",
       "2            NaN       0  \n",
       "3            NaN       0  \n",
       "4            NaN       0  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show daily_btc database\n",
    "daily_btc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'High', 'Low', 'Open', 'Close', 'Volume', 'tenkan_sen',\n",
      "       'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'chikou_span',\n",
      "       'diff_kijun', 'diff_tenkan', 'diff_chikou', 'kijun_signal',\n",
      "       'tenkan_signal', 'chikou_signal', 'indice'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0', 'tenkan_sen_shifted', 'kijun_sen_shifted', 'chikou_shifted', 'close_shifted'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/1xvg6x991594dlshm5qcjr7w0000gn/T/ipykernel_74778/4001366560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Drop no necessary keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m daily_btc.drop(columns=['Unnamed: 0',  'tenkan_sen_shifted',\n\u001b[0m\u001b[1;32m      6\u001b[0m        'kijun_sen_shifted', 'chikou_shifted', 'close_shifted'], inplace=True)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4952\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4953\u001b[0m         \"\"\"\n\u001b[0;32m-> 4954\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4955\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4956\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4267\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6644\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6645\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6646\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Unnamed: 0', 'tenkan_sen_shifted', 'kijun_sen_shifted', 'chikou_shifted', 'close_shifted'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Check btc dataset keys\n",
    "print(daily_btc.keys())\n",
    "\n",
    "#Drop no necessary keys\n",
    "daily_btc.drop(columns=['Unnamed: 0',  'tenkan_sen_shifted',\n",
    "       'kijun_sen_shifted', 'chikou_shifted', 'close_shifted'], inplace=True)\n",
    "\n",
    "daily_btc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>8.406436</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>59.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>807.596</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.091042</td>\n",
       "      <td>106.729</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>241.5044</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.202279</td>\n",
       "      <td>100.300454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>0.221794</td>\n",
       "      <td>436.318864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>3606.952</td>\n",
       "      <td>6.537687</td>\n",
       "      <td>92.950</td>\n",
       "      <td>0.540065</td>\n",
       "      <td>1.899358</td>\n",
       "      <td>526.023</td>\n",
       "      <td>37.584</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>4.894707</td>\n",
       "      <td>912.0243</td>\n",
       "      <td>40.8155</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>1.918625</td>\n",
       "      <td>458.130558</td>\n",
       "      <td>24.70000</td>\n",
       "      <td>1.890892</td>\n",
       "      <td>3.280482</td>\n",
       "      <td>1841.728790</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.285408</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>277.987</td>\n",
       "      <td>17.050</td>\n",
       "      <td>974</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1003.080017</td>\n",
       "      <td>958.698975</td>\n",
       "      <td>963.658020</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>147775008.0</td>\n",
       "      <td>932.752014</td>\n",
       "      <td>881.415009</td>\n",
       "      <td>745.597763</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>116.910004</td>\n",
       "      <td>65.572998</td>\n",
       "      <td>78.575012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>44.739341</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>173.499</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>2340.209</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>306.287</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.569902</td>\n",
       "      <td>679.5298</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.186729</td>\n",
       "      <td>269.620143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>0.214772</td>\n",
       "      <td>1253.644989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.206413</td>\n",
       "      <td>17502.086</td>\n",
       "      <td>39.521719</td>\n",
       "      <td>1713.998</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>5.131474</td>\n",
       "      <td>2405.074</td>\n",
       "      <td>128.940</td>\n",
       "      <td>2.127162</td>\n",
       "      <td>25.927811</td>\n",
       "      <td>5998.5965</td>\n",
       "      <td>1124.5091</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>9.536864</td>\n",
       "      <td>2478.505829</td>\n",
       "      <td>290.46875</td>\n",
       "      <td>3.178987</td>\n",
       "      <td>19.136392</td>\n",
       "      <td>8964.742385</td>\n",
       "      <td>826.867708</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>2.455393</td>\n",
       "      <td>1161.867</td>\n",
       "      <td>71.424</td>\n",
       "      <td>2820</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1031.390015</td>\n",
       "      <td>996.702026</td>\n",
       "      <td>998.617004</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>222184992.0</td>\n",
       "      <td>946.907013</td>\n",
       "      <td>898.401001</td>\n",
       "      <td>746.116516</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>921.590027</td>\n",
       "      <td>123.348999</td>\n",
       "      <td>74.842987</td>\n",
       "      <td>100.159973</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>29.575341</td>\n",
       "      <td>9814.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>101.066</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>1389.482</td>\n",
       "      <td>0.104272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.087896</td>\n",
       "      <td>183.454</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>433.3606</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>169.272860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>756.365240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.851201</td>\n",
       "      <td>8120.911</td>\n",
       "      <td>22.470165</td>\n",
       "      <td>457.140</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>1356.676</td>\n",
       "      <td>189.144</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>15.705079</td>\n",
       "      <td>1527.9880</td>\n",
       "      <td>240.2550</td>\n",
       "      <td>1.087970</td>\n",
       "      <td>17.264859</td>\n",
       "      <td>1821.262586</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>2.816607</td>\n",
       "      <td>15.955071</td>\n",
       "      <td>4714.999394</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.404212</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>676.651</td>\n",
       "      <td>110.028</td>\n",
       "      <td>1674</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1044.079956</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>185168000.0</td>\n",
       "      <td>970.488983</td>\n",
       "      <td>904.745972</td>\n",
       "      <td>746.543015</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.495972</td>\n",
       "      <td>139.093994</td>\n",
       "      <td>73.350983</td>\n",
       "      <td>124.343994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>17.128804</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>125.065</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>1571.329</td>\n",
       "      <td>0.106123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>209.624</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>433.0382</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.206345</td>\n",
       "      <td>191.342678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>873.635908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.674829</td>\n",
       "      <td>7004.225</td>\n",
       "      <td>13.556661</td>\n",
       "      <td>460.631</td>\n",
       "      <td>0.504505</td>\n",
       "      <td>2.465480</td>\n",
       "      <td>961.587</td>\n",
       "      <td>68.885</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>7.153452</td>\n",
       "      <td>1501.3400</td>\n",
       "      <td>182.1425</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>3.057769</td>\n",
       "      <td>689.644645</td>\n",
       "      <td>76.87500</td>\n",
       "      <td>1.916536</td>\n",
       "      <td>4.794911</td>\n",
       "      <td>3652.918407</td>\n",
       "      <td>110.797619</td>\n",
       "      <td>0.338716</td>\n",
       "      <td>1.856071</td>\n",
       "      <td>645.592</td>\n",
       "      <td>68.885</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1159.420044</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>344945984.0</td>\n",
       "      <td>1031.837524</td>\n",
       "      <td>962.416016</td>\n",
       "      <td>752.111511</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>920.382019</td>\n",
       "      <td>192.313965</td>\n",
       "      <td>122.892456</td>\n",
       "      <td>234.347961</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>8.126814</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>103.742</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>1430.305</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>184.961</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.552156</td>\n",
       "      <td>419.5827</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>180.724524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>0.229343</td>\n",
       "      <td>751.068731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987909</td>\n",
       "      <td>5136.216</td>\n",
       "      <td>5.895492</td>\n",
       "      <td>89.270</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>2.164643</td>\n",
       "      <td>784.089</td>\n",
       "      <td>61.320</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>5.255419</td>\n",
       "      <td>1307.0479</td>\n",
       "      <td>104.6719</td>\n",
       "      <td>0.445339</td>\n",
       "      <td>3.005284</td>\n",
       "      <td>765.537930</td>\n",
       "      <td>102.20000</td>\n",
       "      <td>1.685690</td>\n",
       "      <td>4.534309</td>\n",
       "      <td>2897.701636</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>0.256962</td>\n",
       "      <td>0.913094</td>\n",
       "      <td>441.718</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1719</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1191.099976</td>\n",
       "      <td>910.416992</td>\n",
       "      <td>1156.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>510199008.0</td>\n",
       "      <td>1050.758484</td>\n",
       "      <td>978.255981</td>\n",
       "      <td>754.891266</td>\n",
       "      <td>704.777008</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>35.124023</td>\n",
       "      <td>-37.378479</td>\n",
       "      <td>42.976990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  score_mean  score_std  score_sum  score_max  v_neg_mean  \\\n",
       "0  2017-01-01    4.095483   8.406436     3989.0      116.0    0.061278   \n",
       "1  2017-01-02    7.143617  44.739341    20145.0     1859.0    0.061524   \n",
       "2  2017-01-03    5.862605  29.575341     9814.0      570.0    0.060374   \n",
       "3  2017-01-04    4.271773  17.128804     8142.0      599.0    0.065616   \n",
       "4  2017-01-05    3.549738   8.126814     6102.0      146.0    0.060350   \n",
       "\n",
       "   v_neg_std  v_neg_sum  v_neg_max  v_neu_mean  v_neu_sum  v_neu_std  \\\n",
       "0   0.074932     59.685      0.674    0.829154    807.596   0.108736   \n",
       "1   0.067844    173.499      0.531    0.829861   2340.209   0.101984   \n",
       "2   0.071586    101.066      0.579    0.830037   1389.482   0.104272   \n",
       "3   0.073589    125.065      0.582    0.824412   1571.329   0.106123   \n",
       "4   0.072556    103.742      0.565    0.832056   1430.305   0.109600   \n",
       "\n",
       "   v_neu_max  v_pos_mean  v_pos_std  v_pos_sum  v_pos_max  v_compound_mean  \\\n",
       "0        1.0    0.109578   0.091042    106.729      0.579         0.247951   \n",
       "1        1.0    0.108612   0.087565    306.287      0.674         0.240968   \n",
       "2        1.0    0.109590   0.087896    183.454      0.636         0.258877   \n",
       "3        1.0    0.109981   0.090355    209.624      0.735         0.227197   \n",
       "4        1.0    0.107598   0.092280    184.961      0.627         0.244085   \n",
       "\n",
       "   v_compound_std  v_compound_sum  v_compound_max  t_pol_mean  t_pol_std  \\\n",
       "0        0.550243        241.5044          0.9984    0.102978   0.202279   \n",
       "1        0.569902        679.5298          0.9997    0.095610   0.186729   \n",
       "2        0.560860        433.3606          0.9987    0.101119   0.209033   \n",
       "3        0.575732        433.0382          0.9998    0.100390   0.206345   \n",
       "4        0.552156        419.5827          0.9984    0.105134   0.199678   \n",
       "\n",
       "    t_pol_sum  t_pol_max  t_sub_mean  t_sub_std    t_sub_sum  t_sub_max  \\\n",
       "0  100.300454        1.0    0.447966   0.221794   436.318864        1.0   \n",
       "1  269.620143        1.0    0.444555   0.214772  1253.644989        1.0   \n",
       "2  169.272860        1.0    0.451831   0.216360   756.365240        1.0   \n",
       "3  191.342678        1.0    0.458361   0.207941   873.635908        1.0   \n",
       "4  180.724524        1.0    0.436922   0.229343   751.068731        1.0   \n",
       "\n",
       "   v_neu_score_mean  v_neu_score_sum  v_neu_score_std  v_neu_score_max  \\\n",
       "0          3.703236         3606.952         6.537687           92.950   \n",
       "1          6.206413        17502.086        39.521719         1713.998   \n",
       "2          4.851201         8120.911        22.470165          457.140   \n",
       "3          3.674829         7004.225        13.556661          460.631   \n",
       "4          2.987909         5136.216         5.895492           89.270   \n",
       "\n",
       "   v_pos_score_mean  v_pos_score_std  v_pos_score_sum  v_pos_score_max  \\\n",
       "0          0.540065         1.899358          526.023           37.584   \n",
       "1          0.852863         5.131474         2405.074          128.940   \n",
       "2          0.810440         7.089782         1356.676          189.144   \n",
       "3          0.504505         2.465480          961.587           68.885   \n",
       "4          0.456131         2.164643          784.089           61.320   \n",
       "\n",
       "   v_compound_score_mean  v_compound_score_std  v_compound_score_sum  \\\n",
       "0               0.936370              4.894707              912.0243   \n",
       "1               2.127162             25.927811             5998.5965   \n",
       "2               0.912777             15.705079             1527.9880   \n",
       "3               0.787692              7.153452             1501.3400   \n",
       "4               0.760354              5.255419             1307.0479   \n",
       "\n",
       "   v_compound_score_max  t_pol_score_mean  t_pol_score_std  t_pol_score_sum  \\\n",
       "0               40.8155          0.470360         1.918625       458.130558   \n",
       "1             1124.5091          0.878903         9.536864      2478.505829   \n",
       "2              240.2550          1.087970        17.264859      1821.262586   \n",
       "3              182.1425          0.361828         3.057769       689.644645   \n",
       "4              104.6719          0.445339         3.005284       765.537930   \n",
       "\n",
       "   t_pol_score_max  t_sub_score_mean  t_sub_score_std  t_sub_score_sum  \\\n",
       "0         24.70000          1.890892         3.280482      1841.728790   \n",
       "1        290.46875          3.178987        19.136392      8964.742385   \n",
       "2        456.00000          2.816607        15.955071      4714.999394   \n",
       "3         76.87500          1.916536         4.794911      3652.918407   \n",
       "4        102.20000          1.685690         4.534309      2897.701636   \n",
       "\n",
       "   t_sub_score_max  v_neg_score_mean  v_neg_score_std  v_neg_score_sum  \\\n",
       "0        32.400000          0.285408         0.883918          277.987   \n",
       "1       826.867708          0.412010         2.455393         1161.867   \n",
       "2       399.000000          0.404212         3.511884          676.651   \n",
       "3       110.797619          0.338716         1.856071          645.592   \n",
       "4        87.600000          0.256962         0.913094          441.718   \n",
       "\n",
       "   v_neg_score_max  date_count        Date         High          Low  \\\n",
       "0           17.050         974  2017-01-01  1003.080017   958.698975   \n",
       "1           71.424        2820  2017-01-02  1031.390015   996.702026   \n",
       "2          110.028        1674  2017-01-03  1044.079956  1021.599976   \n",
       "3           68.885        1906  2017-01-04  1159.420044  1044.400024   \n",
       "4           16.800        1719  2017-01-05  1191.099976   910.416992   \n",
       "\n",
       "          Open        Close       Volume   tenkan_sen   kijun_sen  \\\n",
       "0   963.658020   998.325012  147775008.0   932.752014  881.415009   \n",
       "1   998.617004  1021.750000  222184992.0   946.907013  898.401001   \n",
       "2  1021.599976  1043.839966  185168000.0   970.488983  904.745972   \n",
       "3  1044.400024  1154.729980  344945984.0  1031.837524  962.416016   \n",
       "4  1156.729980  1013.380005  510199008.0  1050.758484  978.255981   \n",
       "\n",
       "   senkou_span_a  senkou_span_b  chikou_span  diff_kijun  diff_tenkan  \\\n",
       "0     745.597763     704.654510   919.750000  116.910004    65.572998   \n",
       "1     746.116516     704.654510   921.590027  123.348999    74.842987   \n",
       "2     746.543015     704.654510   919.495972  139.093994    73.350983   \n",
       "3     752.111511     704.654510   920.382019  192.313965   122.892456   \n",
       "4     754.891266     704.777008   970.403015   35.124023   -37.378479   \n",
       "\n",
       "   diff_chikou  kijun_signal  tenkan_signal  chikou_signal  indice  date_time  \n",
       "0    78.575012             1              1              1       0 2017-01-01  \n",
       "1   100.159973             1              1              1       0 2017-01-02  \n",
       "2   124.343994             1              1              1       0 2017-01-03  \n",
       "3   234.347961             1              1              1       0 2017-01-04  \n",
       "4    42.976990             0              0              0       0 2017-01-05  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inner Merge bitcoin and reddit Dataset to keep only the existing continuous date\n",
    "daily_BR = daily_reddit.merge(daily_btc, how='inner', left_on='date', right_on='Date').copy()\n",
    "\n",
    "# Convert date in pd.datetime format\n",
    "daily_BR[\"date_time\"]=pd.to_datetime(daily_BR[\"date\"],format='%Y-%m-%d')  \n",
    "\n",
    "# Sort values on date_time\n",
    "daily_BR =daily_BR.sort_values(by=\"date_time\")\n",
    "\n",
    "#Check data\n",
    "daily_BR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare different dataset to start deep learning prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit sentimental analysis aggregation score of vader and textblob of the day by mean > dataset\n",
    "mean_br_df = daily_BR[[ 'score_mean', \n",
    "       'v_neg_mean','v_neu_mean',\n",
    "       'v_pos_mean','v_compound_mean', 't_pol_mean', 't_sub_mean',\n",
    "        'High', 'Low', 'Open', 'Close', 'Volume']].copy()\n",
    "\n",
    "# Reddit sentimental analysis aggregation score of vader and textblob of the day by std > dataset\n",
    "std_br_df = daily_BR[[ 'score_std', \n",
    "       'v_neg_std','v_neu_std',\n",
    "       'v_pos_std','v_compound_std', 't_pol_std', 't_sub_std',\n",
    "        'High', 'Low', 'Open', 'Close', 'Volume']].copy()\n",
    "\n",
    "# Reddit sentimental analysis aggregation score of vader and textblob of the day by max > dataset\n",
    "max_br_df = daily_BR[[ 'score_max', \n",
    "       'v_neg_max','v_neu_max',\n",
    "       'v_pos_max','v_compound_max', 't_pol_max', 't_sub_max',\n",
    "        'High', 'Low', 'Open', 'Close', 'Volume']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the predict column Y define as Class,  1 if Close of the day is superior to the Open of the day else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1095/1095 [00:00<00:00, 61979.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1095/1095 [00:00<00:00, 50365.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1095/1095 [00:00<00:00, 60714.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict column for mean dataset\n",
    "mean_br_df[\"class\"]= mean_br_df[[\"Close\",\"Open\"]].swifter.apply(lambda x: 1 if x.Close > x.Open else 0, axis=1)\n",
    "# Predict column for std dataset\n",
    "std_br_df[\"class\"]= std_br_df[[\"Close\",\"Open\"]].swifter.apply(lambda x: 1 if x.Close > x.Open else 0, axis=1)\n",
    "# Predict column for max dataset\n",
    "max_br_df[\"class\"]= max_br_df[[\"Close\",\"Open\"]].swifter.apply(lambda x: 1 if x.Close > x.Open else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X, Y subsample for different agg function of reddit sentimental scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X an y for the mean analysis\n",
    "X_mean,y_mean =get_X_y_random(mean_br_df, 4000,5,\"class\")\n",
    "# X an y for the sstd analysis\n",
    "X_std,y_std =get_X_y_random(std_br_df, 4000,5,\"class\")\n",
    "# X an y for the max analysis\n",
    "X_max,y_max =get_X_y_random(max_br_df, 4000,5,\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to convert a list of y predict to a list of list of y predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reshape_y(y):\n",
    "#     '''Reshape y for cnn convention\n",
    "#     Get a list return a list of list\n",
    "#     '''\n",
    "#     return y.reshape(y.shape[0],1)\n",
    "\n",
    "# # Reshape predict list values\n",
    "# y_mean = reshape_y(y_mean)\n",
    "# y_std = reshape_y(y_std)\n",
    "# y_max = reshape_y(y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300, 4, 12) (1700, 4, 12) (2300,) (1700,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from sklearn.utils import shuffle\n",
    "# X, y = shuffle(X, y, random_state=4)\n",
    "\n",
    "train_size = 2300\n",
    "\n",
    "#mean dataset\n",
    "X_mean_train = X_mean[:train_size]\n",
    "X_mean_test = X_mean[train_size:]\n",
    "y_mean_train = y_mean[:train_size]\n",
    "y_mean_test=y_mean[train_size:]\n",
    "\n",
    "#std dataset\n",
    "X_std_train = X_std[:train_size]\n",
    "X_std_test = X_std[train_size:]\n",
    "y_std_train = y_std[:train_size]\n",
    "y_std_test =y_std[train_size:]\n",
    "\n",
    "#max dataset\n",
    "X_max_train = X_max[:train_size]\n",
    "X_max_test = X_max[train_size:]\n",
    "y_max_train = y_max[:train_size]\n",
    "y_max_test=y_max[train_size:]\n",
    "\n",
    "#Test shape of train and test\n",
    "print(X_max_train.shape, X_max_test.shape, y_max_train.shape, y_max_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the balanced of binary class in created datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'max y train distribution')"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the balanced of y pred dataset\n",
    "fig, axes = plt.subplots(3, 2, sharex=True, figsize=(6,8))\n",
    "fig.suptitle('Distribution of different y ')\n",
    "\n",
    "# Y Mean test\n",
    "sns.histplot(ax=axes[0][0], data=y_mean_test)\n",
    "axes[0][0].set_title(\"Mean y test distribution\")\n",
    "\n",
    "# Y Std test\n",
    "sns.histplot(ax=axes[1][0], data=y_std_test, )\n",
    "axes[1][0].set_title(\"std y test distribution\")\n",
    "\n",
    "# Y max test\n",
    "sns.histplot(ax=axes[2][0], data=y_max_test, )\n",
    "axes[2][0].set_title(\"max y test distribution\")\n",
    "\n",
    "# Y Mean train\n",
    "sns.histplot(ax=axes[0][1], data=y_mean_train)\n",
    "axes[0][1].set_title(\"Mean y train distribution\")\n",
    "\n",
    "# Y Std train\n",
    "sns.histplot(ax=axes[1][1], data=y_std_train, )\n",
    "axes[1][1].set_title(\"std y train distribution\")\n",
    "\n",
    "# Y max train\n",
    "sns.histplot(ax=axes[2][1], data=y_max_train, )\n",
    "axes[2][1].set_title(\"max y train distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily analysis lstm with a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensor flow to create neural network lstm\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def init_model(X):\n",
    "    model = models.Sequential()\n",
    "    neurons = 16\n",
    "    model.add(layers.LSTM(neurons,return_sequences=True, input_shape=(X.shape[1],X.shape[2] )))\n",
    "    model.add(layers.LSTM(8, input_shape=(X.shape[1],X.shape[2] )))\n",
    "    model.add(layers.Dense(4, activation='linear'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First exploration experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean scores and technical features (with ichimoku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "920/920 [==============================] - 3s 1ms/step - loss: 0.7152 - accuracy: 0.4989 - val_loss: 0.6923 - val_accuracy: 0.5152\n",
      "Epoch 2/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.7010 - accuracy: 0.4967 - val_loss: 0.6869 - val_accuracy: 0.5196\n",
      "Epoch 3/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.5375 - val_loss: 0.6847 - val_accuracy: 0.5587\n",
      "Epoch 4/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5451 - val_loss: 0.6853 - val_accuracy: 0.5630\n",
      "Epoch 5/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5408 - val_loss: 0.6859 - val_accuracy: 0.5478\n",
      "Epoch 6/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6864 - accuracy: 0.5467 - val_loss: 0.6863 - val_accuracy: 0.5478\n",
      "Epoch 7/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6853 - accuracy: 0.5375 - val_loss: 0.6864 - val_accuracy: 0.5478\n",
      "Epoch 8/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6849 - accuracy: 0.5505 - val_loss: 0.6863 - val_accuracy: 0.5435\n",
      "Epoch 9/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5538 - val_loss: 0.6864 - val_accuracy: 0.5435\n",
      "Epoch 10/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5516 - val_loss: 0.6868 - val_accuracy: 0.5435\n",
      "Epoch 11/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6812 - accuracy: 0.5609 - val_loss: 0.6865 - val_accuracy: 0.5435\n",
      "Epoch 12/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6853 - accuracy: 0.5560 - val_loss: 0.6860 - val_accuracy: 0.5435\n",
      "Epoch 13/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6873 - accuracy: 0.5413 - val_loss: 0.6868 - val_accuracy: 0.5478\n",
      "Epoch 14/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5647 - val_loss: 0.6872 - val_accuracy: 0.5435\n",
      "Epoch 15/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6834 - accuracy: 0.5571 - val_loss: 0.6859 - val_accuracy: 0.5435\n",
      "Epoch 16/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6838 - accuracy: 0.5511 - val_loss: 0.6858 - val_accuracy: 0.5435\n",
      "Epoch 17/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6853 - accuracy: 0.5511 - val_loss: 0.6858 - val_accuracy: 0.5435\n",
      "Epoch 18/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.5489 - val_loss: 0.6861 - val_accuracy: 0.5370\n",
      "Epoch 19/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6829 - accuracy: 0.5636 - val_loss: 0.6857 - val_accuracy: 0.5370\n",
      "Epoch 20/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5560 - val_loss: 0.6861 - val_accuracy: 0.5370\n",
      "Epoch 21/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6842 - accuracy: 0.5440 - val_loss: 0.6857 - val_accuracy: 0.5435\n",
      "Epoch 22/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6852 - accuracy: 0.5380 - val_loss: 0.6877 - val_accuracy: 0.5217\n",
      "Epoch 23/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6873 - accuracy: 0.5185 - val_loss: 0.6885 - val_accuracy: 0.5217\n",
      "Epoch 24/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6838 - accuracy: 0.5467 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 25/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5353 - val_loss: 0.6884 - val_accuracy: 0.5217\n",
      "Epoch 26/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6842 - accuracy: 0.5408 - val_loss: 0.6887 - val_accuracy: 0.5217\n",
      "Epoch 27/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6846 - accuracy: 0.5375 - val_loss: 0.6881 - val_accuracy: 0.5283\n",
      "Epoch 28/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6849 - accuracy: 0.5435 - val_loss: 0.6885 - val_accuracy: 0.5283\n",
      "Epoch 29/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6867 - accuracy: 0.5228 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 30/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6857 - accuracy: 0.5223 - val_loss: 0.6882 - val_accuracy: 0.5217\n",
      "Epoch 31/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6860 - accuracy: 0.5174 - val_loss: 0.6879 - val_accuracy: 0.5217\n",
      "Epoch 32/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5359 - val_loss: 0.6881 - val_accuracy: 0.5217\n",
      "Epoch 33/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5359 - val_loss: 0.6878 - val_accuracy: 0.5304\n",
      "Epoch 34/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6846 - accuracy: 0.5283 - val_loss: 0.6876 - val_accuracy: 0.5217\n",
      "Epoch 35/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6861 - accuracy: 0.5261 - val_loss: 0.6877 - val_accuracy: 0.5217\n",
      "Epoch 36/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6834 - accuracy: 0.5473 - val_loss: 0.6876 - val_accuracy: 0.5304\n",
      "Epoch 37/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6854 - accuracy: 0.5386 - val_loss: 0.6877 - val_accuracy: 0.5217\n",
      "Epoch 38/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6832 - accuracy: 0.5435 - val_loss: 0.6877 - val_accuracy: 0.5283\n",
      "Epoch 39/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6834 - accuracy: 0.5451 - val_loss: 0.6879 - val_accuracy: 0.5261\n",
      "Epoch 40/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6835 - accuracy: 0.5315 - val_loss: 0.6878 - val_accuracy: 0.5283\n",
      "Epoch 41/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6835 - accuracy: 0.5500 - val_loss: 0.6880 - val_accuracy: 0.5217\n",
      "Epoch 42/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5505 - val_loss: 0.6879 - val_accuracy: 0.5217\n",
      "Epoch 43/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.5386 - val_loss: 0.6877 - val_accuracy: 0.5283\n",
      "Epoch 44/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6837 - accuracy: 0.5277 - val_loss: 0.6880 - val_accuracy: 0.5217\n",
      "Epoch 45/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6848 - accuracy: 0.5538 - val_loss: 0.6893 - val_accuracy: 0.5217\n",
      "Epoch 46/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6858 - accuracy: 0.5353 - val_loss: 0.6882 - val_accuracy: 0.5217\n",
      "Epoch 47/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6825 - accuracy: 0.5489 - val_loss: 0.6881 - val_accuracy: 0.5239\n",
      "Epoch 48/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6849 - accuracy: 0.5359 - val_loss: 0.6881 - val_accuracy: 0.5283\n",
      "Epoch 49/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5582 - val_loss: 0.6895 - val_accuracy: 0.5283\n",
      "Epoch 50/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6849 - accuracy: 0.5310 - val_loss: 0.6895 - val_accuracy: 0.5239\n",
      "Epoch 51/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6832 - accuracy: 0.5511 - val_loss: 0.6897 - val_accuracy: 0.5239\n",
      "Epoch 52/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6853 - accuracy: 0.5375 - val_loss: 0.6887 - val_accuracy: 0.5239\n",
      "Epoch 53/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6851 - accuracy: 0.5397 - val_loss: 0.6882 - val_accuracy: 0.5304\n",
      "Epoch 54/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6850 - accuracy: 0.5234 - val_loss: 0.6879 - val_accuracy: 0.5304\n",
      "Epoch 55/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5250 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 56/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5440 - val_loss: 0.6880 - val_accuracy: 0.5217\n",
      "Epoch 57/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6831 - accuracy: 0.5413 - val_loss: 0.6878 - val_accuracy: 0.5304\n",
      "Epoch 58/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5277 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 59/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5397 - val_loss: 0.6880 - val_accuracy: 0.5283\n",
      "Epoch 60/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6850 - accuracy: 0.5234 - val_loss: 0.6879 - val_accuracy: 0.5283\n",
      "Epoch 61/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6847 - accuracy: 0.5337 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 62/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5451 - val_loss: 0.6878 - val_accuracy: 0.5304\n",
      "Epoch 63/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5418 - val_loss: 0.6880 - val_accuracy: 0.5239\n",
      "Epoch 64/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5386 - val_loss: 0.6879 - val_accuracy: 0.5283\n",
      "Epoch 65/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6841 - accuracy: 0.5321 - val_loss: 0.6878 - val_accuracy: 0.5304\n",
      "Epoch 66/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5310 - val_loss: 0.6875 - val_accuracy: 0.5304\n",
      "Epoch 67/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5348 - val_loss: 0.6877 - val_accuracy: 0.5283\n",
      "Epoch 68/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6847 - accuracy: 0.5245 - val_loss: 0.6878 - val_accuracy: 0.5239\n",
      "Epoch 69/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5299 - val_loss: 0.6881 - val_accuracy: 0.5239\n",
      "Epoch 70/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5337 - val_loss: 0.6877 - val_accuracy: 0.5239\n",
      "Epoch 71/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6828 - accuracy: 0.5424 - val_loss: 0.6875 - val_accuracy: 0.5304\n",
      "Epoch 72/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6841 - accuracy: 0.5413 - val_loss: 0.6874 - val_accuracy: 0.5239\n",
      "Epoch 73/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6841 - accuracy: 0.5293 - val_loss: 0.6872 - val_accuracy: 0.5304\n",
      "Epoch 74/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5467 - val_loss: 0.6874 - val_accuracy: 0.5239\n",
      "Epoch 75/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.5413 - val_loss: 0.6879 - val_accuracy: 0.5239\n",
      "Epoch 76/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6837 - accuracy: 0.5446 - val_loss: 0.6874 - val_accuracy: 0.5283\n",
      "Epoch 77/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6852 - accuracy: 0.5440 - val_loss: 0.6874 - val_accuracy: 0.5239\n",
      "Epoch 78/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6846 - accuracy: 0.5326 - val_loss: 0.6877 - val_accuracy: 0.5239\n",
      "Epoch 79/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6834 - accuracy: 0.5440 - val_loss: 0.6882 - val_accuracy: 0.5239\n",
      "Epoch 80/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6848 - accuracy: 0.5234 - val_loss: 0.6877 - val_accuracy: 0.5239\n",
      "Epoch 81/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6841 - accuracy: 0.5370 - val_loss: 0.6877 - val_accuracy: 0.5304\n",
      "Epoch 82/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6848 - accuracy: 0.5315 - val_loss: 0.6876 - val_accuracy: 0.5304\n",
      "Epoch 83/250\n",
      "920/920 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5348 - val_loss: 0.6875 - val_accuracy: 0.5283\n"
     ]
    }
   ],
   "source": [
    "# Impor tensorflow early stopping callback to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model_mean_v1 = init_model(X_mean_train)\n",
    "#Define early stopping parmaters\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "\n",
    "#Start training the neural network\n",
    "history_mean_v1 = model_mean_v1.fit(X_mean_train, y_mean_train, \n",
    "          epochs=250, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=es,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model mean version 1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.50      0.52       827\n",
      "         1.0       0.56      0.61      0.58       873\n",
      "\n",
      "    accuracy                           0.56      1700\n",
      "   macro avg       0.56      0.55      0.55      1700\n",
      "weighted avg       0.56      0.56      0.55      1700\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_get_conf_learning(model_mean_v1,history_mean_v1,X_mean_test, y_mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std scores and technical features (with ichimoku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "920/920 - 3s - loss: 0.6880 - accuracy: 0.5380 - val_loss: 0.6776 - val_accuracy: 0.5913\n",
      "Epoch 2/250\n",
      "920/920 - 1s - loss: 0.6845 - accuracy: 0.5549 - val_loss: 0.6742 - val_accuracy: 0.5957\n",
      "Epoch 3/250\n",
      "920/920 - 2s - loss: 0.6831 - accuracy: 0.5451 - val_loss: 0.6775 - val_accuracy: 0.5239\n",
      "Epoch 4/250\n",
      "920/920 - 1s - loss: 0.6824 - accuracy: 0.5576 - val_loss: 0.6735 - val_accuracy: 0.5957\n",
      "Epoch 5/250\n",
      "920/920 - 2s - loss: 0.6828 - accuracy: 0.5603 - val_loss: 0.6771 - val_accuracy: 0.5978\n",
      "Epoch 6/250\n",
      "920/920 - 1s - loss: 0.6820 - accuracy: 0.5522 - val_loss: 0.6749 - val_accuracy: 0.6022\n",
      "Epoch 7/250\n",
      "920/920 - 1s - loss: 0.6829 - accuracy: 0.5549 - val_loss: 0.6742 - val_accuracy: 0.6000\n",
      "Epoch 8/250\n",
      "920/920 - 2s - loss: 0.6832 - accuracy: 0.5582 - val_loss: 0.6723 - val_accuracy: 0.6022\n",
      "Epoch 9/250\n",
      "920/920 - 2s - loss: 0.6821 - accuracy: 0.5674 - val_loss: 0.6736 - val_accuracy: 0.6000\n",
      "Epoch 10/250\n",
      "920/920 - 1s - loss: 0.6813 - accuracy: 0.5663 - val_loss: 0.6747 - val_accuracy: 0.6000\n",
      "Epoch 11/250\n",
      "920/920 - 2s - loss: 0.6828 - accuracy: 0.5701 - val_loss: 0.6741 - val_accuracy: 0.6000\n",
      "Epoch 12/250\n",
      "920/920 - 2s - loss: 0.6821 - accuracy: 0.5674 - val_loss: 0.6740 - val_accuracy: 0.6000\n",
      "Epoch 13/250\n",
      "920/920 - 1s - loss: 0.6819 - accuracy: 0.5609 - val_loss: 0.6715 - val_accuracy: 0.6000\n",
      "Epoch 14/250\n",
      "920/920 - 2s - loss: 0.6802 - accuracy: 0.5717 - val_loss: 0.6711 - val_accuracy: 0.6000\n",
      "Epoch 15/250\n",
      "920/920 - 1s - loss: 0.6803 - accuracy: 0.5783 - val_loss: 0.6704 - val_accuracy: 0.6022\n",
      "Epoch 16/250\n",
      "920/920 - 2s - loss: 0.6815 - accuracy: 0.5717 - val_loss: 0.6709 - val_accuracy: 0.6000\n",
      "Epoch 17/250\n",
      "920/920 - 1s - loss: 0.6805 - accuracy: 0.5663 - val_loss: 0.6699 - val_accuracy: 0.6000\n",
      "Epoch 18/250\n",
      "920/920 - 1s - loss: 0.6820 - accuracy: 0.5582 - val_loss: 0.6780 - val_accuracy: 0.5522\n",
      "Epoch 19/250\n",
      "920/920 - 1s - loss: 0.6818 - accuracy: 0.5554 - val_loss: 0.6772 - val_accuracy: 0.5391\n",
      "Epoch 20/250\n",
      "920/920 - 1s - loss: 0.6840 - accuracy: 0.5478 - val_loss: 0.6742 - val_accuracy: 0.5935\n",
      "Epoch 21/250\n",
      "920/920 - 1s - loss: 0.6827 - accuracy: 0.5527 - val_loss: 0.6731 - val_accuracy: 0.5891\n",
      "Epoch 22/250\n",
      "920/920 - 1s - loss: 0.6834 - accuracy: 0.5516 - val_loss: 0.6758 - val_accuracy: 0.5935\n",
      "Epoch 23/250\n",
      "920/920 - 1s - loss: 0.6843 - accuracy: 0.5560 - val_loss: 0.6737 - val_accuracy: 0.5891\n",
      "Epoch 24/250\n",
      "920/920 - 1s - loss: 0.6821 - accuracy: 0.5549 - val_loss: 0.6749 - val_accuracy: 0.5935\n",
      "Epoch 25/250\n",
      "920/920 - 1s - loss: 0.6821 - accuracy: 0.5614 - val_loss: 0.6740 - val_accuracy: 0.5935\n",
      "Epoch 26/250\n",
      "920/920 - 1s - loss: 0.6827 - accuracy: 0.5571 - val_loss: 0.6737 - val_accuracy: 0.5935\n",
      "Epoch 27/250\n",
      "920/920 - 1s - loss: 0.6850 - accuracy: 0.5576 - val_loss: 0.6740 - val_accuracy: 0.5935\n",
      "Epoch 28/250\n",
      "920/920 - 1s - loss: 0.6830 - accuracy: 0.5543 - val_loss: 0.6719 - val_accuracy: 0.5891\n",
      "Epoch 29/250\n",
      "920/920 - 1s - loss: 0.6812 - accuracy: 0.5603 - val_loss: 0.6714 - val_accuracy: 0.5891\n",
      "Epoch 30/250\n",
      "920/920 - 1s - loss: 0.6822 - accuracy: 0.5587 - val_loss: 0.6726 - val_accuracy: 0.5957\n",
      "Epoch 31/250\n",
      "920/920 - 1s - loss: 0.6819 - accuracy: 0.5609 - val_loss: 0.6729 - val_accuracy: 0.5935\n",
      "Epoch 32/250\n",
      "920/920 - 1s - loss: 0.6828 - accuracy: 0.5603 - val_loss: 0.6716 - val_accuracy: 0.5935\n",
      "Epoch 33/250\n",
      "920/920 - 1s - loss: 0.6815 - accuracy: 0.5543 - val_loss: 0.6711 - val_accuracy: 0.5935\n",
      "Epoch 34/250\n",
      "920/920 - 2s - loss: 0.6810 - accuracy: 0.5614 - val_loss: 0.6706 - val_accuracy: 0.5935\n",
      "Epoch 35/250\n",
      "920/920 - 1s - loss: 0.6811 - accuracy: 0.5576 - val_loss: 0.6731 - val_accuracy: 0.5913\n",
      "Epoch 36/250\n",
      "920/920 - 1s - loss: 0.6801 - accuracy: 0.5592 - val_loss: 0.6689 - val_accuracy: 0.5935\n",
      "Epoch 37/250\n",
      "920/920 - 1s - loss: 0.6805 - accuracy: 0.5576 - val_loss: 0.6685 - val_accuracy: 0.5957\n",
      "Epoch 38/250\n",
      "920/920 - 1s - loss: 0.6785 - accuracy: 0.5625 - val_loss: 0.6675 - val_accuracy: 0.5978\n",
      "Epoch 39/250\n",
      "920/920 - 1s - loss: 0.6781 - accuracy: 0.5630 - val_loss: 0.6665 - val_accuracy: 0.5957\n",
      "Epoch 40/250\n",
      "920/920 - 1s - loss: 0.6792 - accuracy: 0.5630 - val_loss: 0.6674 - val_accuracy: 0.6000\n",
      "Epoch 41/250\n",
      "920/920 - 1s - loss: 0.6762 - accuracy: 0.5641 - val_loss: 0.6676 - val_accuracy: 0.5717\n",
      "Epoch 42/250\n",
      "920/920 - 1s - loss: 0.6766 - accuracy: 0.5674 - val_loss: 0.6678 - val_accuracy: 0.6022\n",
      "Epoch 43/250\n",
      "920/920 - 1s - loss: 0.6761 - accuracy: 0.5554 - val_loss: 0.6649 - val_accuracy: 0.6065\n",
      "Epoch 44/250\n",
      "920/920 - 1s - loss: 0.6765 - accuracy: 0.5717 - val_loss: 0.6641 - val_accuracy: 0.6022\n",
      "Epoch 45/250\n",
      "920/920 - 1s - loss: 0.6760 - accuracy: 0.5685 - val_loss: 0.6653 - val_accuracy: 0.6022\n",
      "Epoch 46/250\n",
      "920/920 - 1s - loss: 0.6765 - accuracy: 0.5701 - val_loss: 0.6633 - val_accuracy: 0.6065\n",
      "Epoch 47/250\n",
      "920/920 - 1s - loss: 0.6748 - accuracy: 0.5587 - val_loss: 0.6626 - val_accuracy: 0.6022\n",
      "Epoch 48/250\n",
      "920/920 - 1s - loss: 0.6750 - accuracy: 0.5609 - val_loss: 0.6629 - val_accuracy: 0.6022\n",
      "Epoch 49/250\n",
      "920/920 - 2s - loss: 0.6749 - accuracy: 0.5674 - val_loss: 0.6614 - val_accuracy: 0.6065\n",
      "Epoch 50/250\n",
      "920/920 - 1s - loss: 0.6764 - accuracy: 0.5625 - val_loss: 0.6646 - val_accuracy: 0.6065\n",
      "Epoch 51/250\n",
      "920/920 - 1s - loss: 0.6740 - accuracy: 0.5609 - val_loss: 0.6651 - val_accuracy: 0.6087\n",
      "Epoch 52/250\n",
      "920/920 - 1s - loss: 0.6733 - accuracy: 0.5739 - val_loss: 0.6661 - val_accuracy: 0.6087\n",
      "Epoch 53/250\n",
      "920/920 - 1s - loss: 0.6738 - accuracy: 0.5668 - val_loss: 0.6636 - val_accuracy: 0.6065\n",
      "Epoch 54/250\n",
      "920/920 - 1s - loss: 0.6737 - accuracy: 0.5658 - val_loss: 0.6623 - val_accuracy: 0.6109\n",
      "Epoch 55/250\n",
      "920/920 - 1s - loss: 0.6750 - accuracy: 0.5652 - val_loss: 0.6614 - val_accuracy: 0.6087\n",
      "Epoch 56/250\n",
      "920/920 - 1s - loss: 0.6729 - accuracy: 0.5587 - val_loss: 0.6644 - val_accuracy: 0.5761\n",
      "Epoch 57/250\n",
      "920/920 - 1s - loss: 0.6727 - accuracy: 0.5598 - val_loss: 0.6620 - val_accuracy: 0.6043\n",
      "Epoch 58/250\n",
      "920/920 - 1s - loss: 0.6741 - accuracy: 0.5701 - val_loss: 0.6606 - val_accuracy: 0.6000\n",
      "Epoch 59/250\n",
      "920/920 - 1s - loss: 0.6722 - accuracy: 0.5679 - val_loss: 0.6618 - val_accuracy: 0.6087\n",
      "Epoch 60/250\n",
      "920/920 - 1s - loss: 0.6727 - accuracy: 0.5690 - val_loss: 0.6605 - val_accuracy: 0.6087\n",
      "Epoch 61/250\n",
      "920/920 - 1s - loss: 0.6708 - accuracy: 0.5750 - val_loss: 0.6607 - val_accuracy: 0.6022\n",
      "Epoch 62/250\n",
      "920/920 - 1s - loss: 0.6734 - accuracy: 0.5652 - val_loss: 0.6622 - val_accuracy: 0.6087\n",
      "Epoch 63/250\n",
      "920/920 - 1s - loss: 0.6707 - accuracy: 0.5620 - val_loss: 0.6620 - val_accuracy: 0.6109\n",
      "Epoch 64/250\n",
      "920/920 - 1s - loss: 0.6717 - accuracy: 0.5685 - val_loss: 0.6599 - val_accuracy: 0.6087\n",
      "Epoch 65/250\n",
      "920/920 - 1s - loss: 0.6700 - accuracy: 0.5707 - val_loss: 0.6598 - val_accuracy: 0.6109\n",
      "Epoch 66/250\n",
      "920/920 - 1s - loss: 0.6709 - accuracy: 0.5625 - val_loss: 0.6610 - val_accuracy: 0.6130\n",
      "Epoch 67/250\n",
      "920/920 - 1s - loss: 0.6713 - accuracy: 0.5728 - val_loss: 0.6613 - val_accuracy: 0.6109\n",
      "Epoch 68/250\n",
      "920/920 - 1s - loss: 0.6674 - accuracy: 0.5761 - val_loss: 0.6606 - val_accuracy: 0.6043\n",
      "Epoch 69/250\n",
      "920/920 - 1s - loss: 0.6688 - accuracy: 0.5723 - val_loss: 0.6607 - val_accuracy: 0.6130\n",
      "Epoch 70/250\n",
      "920/920 - 1s - loss: 0.6673 - accuracy: 0.5636 - val_loss: 0.6632 - val_accuracy: 0.6065\n",
      "Epoch 71/250\n",
      "920/920 - 1s - loss: 0.6665 - accuracy: 0.5685 - val_loss: 0.6610 - val_accuracy: 0.6152\n",
      "Epoch 72/250\n",
      "920/920 - 1s - loss: 0.6683 - accuracy: 0.5679 - val_loss: 0.6584 - val_accuracy: 0.6109\n",
      "Epoch 73/250\n",
      "920/920 - 1s - loss: 0.6686 - accuracy: 0.5793 - val_loss: 0.6664 - val_accuracy: 0.5370\n",
      "Epoch 74/250\n",
      "920/920 - 2s - loss: 0.6675 - accuracy: 0.5712 - val_loss: 0.6652 - val_accuracy: 0.5826\n",
      "Epoch 75/250\n",
      "920/920 - 1s - loss: 0.6677 - accuracy: 0.5685 - val_loss: 0.6641 - val_accuracy: 0.6109\n",
      "Epoch 76/250\n",
      "920/920 - 1s - loss: 0.6666 - accuracy: 0.5728 - val_loss: 0.6609 - val_accuracy: 0.6043\n",
      "Epoch 77/250\n",
      "920/920 - 1s - loss: 0.6676 - accuracy: 0.5674 - val_loss: 0.6629 - val_accuracy: 0.6109\n",
      "Epoch 78/250\n",
      "920/920 - 1s - loss: 0.6643 - accuracy: 0.5761 - val_loss: 0.6680 - val_accuracy: 0.5848\n",
      "Epoch 79/250\n",
      "920/920 - 1s - loss: 0.6661 - accuracy: 0.5739 - val_loss: 0.6663 - val_accuracy: 0.5891\n",
      "Epoch 80/250\n",
      "920/920 - 1s - loss: 0.6657 - accuracy: 0.5484 - val_loss: 0.6621 - val_accuracy: 0.6109\n",
      "Epoch 81/250\n",
      "920/920 - 1s - loss: 0.6636 - accuracy: 0.5821 - val_loss: 0.6600 - val_accuracy: 0.6087\n",
      "Epoch 82/250\n",
      "920/920 - 1s - loss: 0.6643 - accuracy: 0.5728 - val_loss: 0.6642 - val_accuracy: 0.5804\n",
      "Epoch 83/250\n",
      "920/920 - 1s - loss: 0.6617 - accuracy: 0.5717 - val_loss: 0.6596 - val_accuracy: 0.6065\n",
      "Epoch 84/250\n",
      "920/920 - 1s - loss: 0.6644 - accuracy: 0.5620 - val_loss: 0.6598 - val_accuracy: 0.6087\n",
      "Epoch 85/250\n",
      "920/920 - 1s - loss: 0.6624 - accuracy: 0.5777 - val_loss: 0.6617 - val_accuracy: 0.6087\n",
      "Epoch 86/250\n",
      "920/920 - 1s - loss: 0.6627 - accuracy: 0.5745 - val_loss: 0.6600 - val_accuracy: 0.6065\n",
      "Epoch 87/250\n",
      "920/920 - 1s - loss: 0.6620 - accuracy: 0.5696 - val_loss: 0.6628 - val_accuracy: 0.6065\n",
      "Epoch 88/250\n",
      "920/920 - 1s - loss: 0.6612 - accuracy: 0.5755 - val_loss: 0.6643 - val_accuracy: 0.6130\n",
      "Epoch 89/250\n",
      "920/920 - 1s - loss: 0.6635 - accuracy: 0.5582 - val_loss: 0.6626 - val_accuracy: 0.6130\n",
      "Epoch 90/250\n",
      "920/920 - 1s - loss: 0.6619 - accuracy: 0.5772 - val_loss: 0.6618 - val_accuracy: 0.6130\n",
      "Epoch 91/250\n",
      "920/920 - 1s - loss: 0.6621 - accuracy: 0.5739 - val_loss: 0.6598 - val_accuracy: 0.6130\n",
      "Epoch 92/250\n",
      "920/920 - 1s - loss: 0.6594 - accuracy: 0.5663 - val_loss: 0.6581 - val_accuracy: 0.6109\n",
      "Epoch 93/250\n",
      "920/920 - 1s - loss: 0.6612 - accuracy: 0.5777 - val_loss: 0.6658 - val_accuracy: 0.5826\n",
      "Epoch 94/250\n",
      "920/920 - 1s - loss: 0.6603 - accuracy: 0.5723 - val_loss: 0.6602 - val_accuracy: 0.6109\n",
      "Epoch 95/250\n",
      "920/920 - 1s - loss: 0.6587 - accuracy: 0.5750 - val_loss: 0.6649 - val_accuracy: 0.5870\n",
      "Epoch 96/250\n",
      "920/920 - 1s - loss: 0.6579 - accuracy: 0.5750 - val_loss: 0.6615 - val_accuracy: 0.6087\n",
      "Epoch 97/250\n",
      "920/920 - 1s - loss: 0.6610 - accuracy: 0.5641 - val_loss: 0.6609 - val_accuracy: 0.6109\n",
      "Epoch 98/250\n",
      "920/920 - 1s - loss: 0.6603 - accuracy: 0.5745 - val_loss: 0.6618 - val_accuracy: 0.6152\n",
      "Epoch 99/250\n",
      "920/920 - 1s - loss: 0.6586 - accuracy: 0.5717 - val_loss: 0.6616 - val_accuracy: 0.5891\n",
      "Epoch 100/250\n",
      "920/920 - 1s - loss: 0.6594 - accuracy: 0.5668 - val_loss: 0.6601 - val_accuracy: 0.6152\n",
      "Epoch 101/250\n",
      "920/920 - 1s - loss: 0.6588 - accuracy: 0.5647 - val_loss: 0.6590 - val_accuracy: 0.6130\n",
      "Epoch 102/250\n",
      "920/920 - 1s - loss: 0.6597 - accuracy: 0.5799 - val_loss: 0.6643 - val_accuracy: 0.5848\n",
      "Epoch 103/250\n",
      "920/920 - 1s - loss: 0.6578 - accuracy: 0.5793 - val_loss: 0.6648 - val_accuracy: 0.5826\n",
      "Epoch 104/250\n",
      "920/920 - 1s - loss: 0.6596 - accuracy: 0.5772 - val_loss: 0.6590 - val_accuracy: 0.6152\n",
      "Epoch 105/250\n",
      "920/920 - 2s - loss: 0.6583 - accuracy: 0.5745 - val_loss: 0.6623 - val_accuracy: 0.5870\n",
      "Epoch 106/250\n",
      "920/920 - 1s - loss: 0.6570 - accuracy: 0.5766 - val_loss: 0.6740 - val_accuracy: 0.5826\n",
      "Epoch 107/250\n",
      "920/920 - 1s - loss: 0.6579 - accuracy: 0.5658 - val_loss: 0.6649 - val_accuracy: 0.6087\n",
      "Epoch 108/250\n",
      "920/920 - 1s - loss: 0.6545 - accuracy: 0.5717 - val_loss: 0.6659 - val_accuracy: 0.5848\n",
      "Epoch 109/250\n",
      "920/920 - 1s - loss: 0.6572 - accuracy: 0.5783 - val_loss: 0.6700 - val_accuracy: 0.5870\n",
      "Epoch 110/250\n",
      "920/920 - 1s - loss: 0.6565 - accuracy: 0.5755 - val_loss: 0.6667 - val_accuracy: 0.5935\n",
      "Epoch 111/250\n",
      "920/920 - 1s - loss: 0.6612 - accuracy: 0.5728 - val_loss: 0.6618 - val_accuracy: 0.6130\n",
      "Epoch 112/250\n",
      "920/920 - 1s - loss: 0.6605 - accuracy: 0.5707 - val_loss: 0.6650 - val_accuracy: 0.5848\n",
      "Epoch 113/250\n",
      "920/920 - 1s - loss: 0.6586 - accuracy: 0.5750 - val_loss: 0.6623 - val_accuracy: 0.6109\n",
      "Epoch 114/250\n",
      "920/920 - 1s - loss: 0.6563 - accuracy: 0.5745 - val_loss: 0.6646 - val_accuracy: 0.6109\n",
      "Epoch 115/250\n",
      "920/920 - 1s - loss: 0.6594 - accuracy: 0.5750 - val_loss: 0.6720 - val_accuracy: 0.5870\n",
      "Epoch 116/250\n",
      "920/920 - 1s - loss: 0.6553 - accuracy: 0.5815 - val_loss: 0.6638 - val_accuracy: 0.6130\n",
      "Epoch 117/250\n",
      "920/920 - 2s - loss: 0.6575 - accuracy: 0.5652 - val_loss: 0.6656 - val_accuracy: 0.5826\n",
      "Epoch 118/250\n",
      "920/920 - 1s - loss: 0.6578 - accuracy: 0.5685 - val_loss: 0.6628 - val_accuracy: 0.6152\n",
      "Epoch 119/250\n",
      "920/920 - 1s - loss: 0.6565 - accuracy: 0.5690 - val_loss: 0.6652 - val_accuracy: 0.6087\n",
      "Epoch 120/250\n",
      "920/920 - 1s - loss: 0.6543 - accuracy: 0.5745 - val_loss: 0.6704 - val_accuracy: 0.5848\n",
      "Epoch 121/250\n",
      "920/920 - 1s - loss: 0.6579 - accuracy: 0.5712 - val_loss: 0.6693 - val_accuracy: 0.5848\n",
      "Epoch 122/250\n",
      "920/920 - 1s - loss: 0.6549 - accuracy: 0.5815 - val_loss: 0.6703 - val_accuracy: 0.5848\n",
      "Epoch 123/250\n",
      "920/920 - 2s - loss: 0.6583 - accuracy: 0.5712 - val_loss: 0.6651 - val_accuracy: 0.6065\n",
      "Epoch 124/250\n",
      "920/920 - 1s - loss: 0.6575 - accuracy: 0.5717 - val_loss: 0.6628 - val_accuracy: 0.6130\n",
      "Epoch 125/250\n",
      "920/920 - 1s - loss: 0.6569 - accuracy: 0.5761 - val_loss: 0.6758 - val_accuracy: 0.5348\n",
      "Epoch 126/250\n",
      "920/920 - 1s - loss: 0.6581 - accuracy: 0.5636 - val_loss: 0.6633 - val_accuracy: 0.6130\n",
      "Epoch 127/250\n",
      "920/920 - 1s - loss: 0.6551 - accuracy: 0.5723 - val_loss: 0.6680 - val_accuracy: 0.5978\n",
      "Epoch 128/250\n",
      "920/920 - 1s - loss: 0.6573 - accuracy: 0.5717 - val_loss: 0.6615 - val_accuracy: 0.6130\n",
      "Epoch 129/250\n",
      "920/920 - 1s - loss: 0.6564 - accuracy: 0.5728 - val_loss: 0.6667 - val_accuracy: 0.5891\n",
      "Epoch 130/250\n",
      "920/920 - 1s - loss: 0.6549 - accuracy: 0.5739 - val_loss: 0.6676 - val_accuracy: 0.5935\n",
      "Epoch 131/250\n",
      "920/920 - 1s - loss: 0.6564 - accuracy: 0.5685 - val_loss: 0.6641 - val_accuracy: 0.5870\n",
      "Epoch 132/250\n",
      "920/920 - 1s - loss: 0.6545 - accuracy: 0.5723 - val_loss: 0.6636 - val_accuracy: 0.6109\n",
      "Epoch 133/250\n",
      "920/920 - 1s - loss: 0.6571 - accuracy: 0.5712 - val_loss: 0.6639 - val_accuracy: 0.6152\n",
      "Epoch 134/250\n",
      "920/920 - 1s - loss: 0.6573 - accuracy: 0.5799 - val_loss: 0.6658 - val_accuracy: 0.6174\n",
      "Epoch 135/250\n",
      "920/920 - 1s - loss: 0.6560 - accuracy: 0.5690 - val_loss: 0.6599 - val_accuracy: 0.6109\n",
      "Epoch 136/250\n",
      "920/920 - 1s - loss: 0.6534 - accuracy: 0.5696 - val_loss: 0.6620 - val_accuracy: 0.6109\n",
      "Epoch 137/250\n",
      "920/920 - 1s - loss: 0.6550 - accuracy: 0.5783 - val_loss: 0.6655 - val_accuracy: 0.5826\n",
      "Epoch 138/250\n",
      "920/920 - 1s - loss: 0.6560 - accuracy: 0.5734 - val_loss: 0.6719 - val_accuracy: 0.5391\n",
      "Epoch 139/250\n",
      "920/920 - 1s - loss: 0.6541 - accuracy: 0.5630 - val_loss: 0.6617 - val_accuracy: 0.6109\n",
      "Epoch 140/250\n",
      "920/920 - 1s - loss: 0.6535 - accuracy: 0.5701 - val_loss: 0.6626 - val_accuracy: 0.6130\n",
      "Epoch 141/250\n",
      "920/920 - 1s - loss: 0.6535 - accuracy: 0.5793 - val_loss: 0.6630 - val_accuracy: 0.6109\n",
      "Epoch 142/250\n",
      "920/920 - 1s - loss: 0.6567 - accuracy: 0.5717 - val_loss: 0.6645 - val_accuracy: 0.6130\n",
      "Epoch 143/250\n",
      "920/920 - 1s - loss: 0.6562 - accuracy: 0.5679 - val_loss: 0.6653 - val_accuracy: 0.5848\n",
      "Epoch 144/250\n",
      "920/920 - 1s - loss: 0.6525 - accuracy: 0.5821 - val_loss: 0.6693 - val_accuracy: 0.5891\n",
      "Epoch 145/250\n",
      "920/920 - 1s - loss: 0.6555 - accuracy: 0.5647 - val_loss: 0.6630 - val_accuracy: 0.6109\n",
      "Epoch 146/250\n",
      "920/920 - 1s - loss: 0.6551 - accuracy: 0.5788 - val_loss: 0.6702 - val_accuracy: 0.5391\n",
      "Epoch 147/250\n",
      "920/920 - 1s - loss: 0.6550 - accuracy: 0.5668 - val_loss: 0.6648 - val_accuracy: 0.5870\n",
      "Epoch 148/250\n",
      "920/920 - 1s - loss: 0.6533 - accuracy: 0.5679 - val_loss: 0.6625 - val_accuracy: 0.6087\n",
      "Epoch 149/250\n",
      "920/920 - 1s - loss: 0.6539 - accuracy: 0.5755 - val_loss: 0.6687 - val_accuracy: 0.5630\n",
      "Epoch 150/250\n",
      "920/920 - 1s - loss: 0.6542 - accuracy: 0.5750 - val_loss: 0.6639 - val_accuracy: 0.6130\n",
      "Epoch 151/250\n",
      "920/920 - 1s - loss: 0.6550 - accuracy: 0.5842 - val_loss: 0.6645 - val_accuracy: 0.6130\n",
      "Epoch 152/250\n",
      "920/920 - 1s - loss: 0.6541 - accuracy: 0.5652 - val_loss: 0.6619 - val_accuracy: 0.6109\n",
      "Epoch 153/250\n",
      "920/920 - 1s - loss: 0.6554 - accuracy: 0.5739 - val_loss: 0.6655 - val_accuracy: 0.6174\n",
      "Epoch 154/250\n",
      "920/920 - 1s - loss: 0.6525 - accuracy: 0.5788 - val_loss: 0.6675 - val_accuracy: 0.5870\n",
      "Epoch 155/250\n",
      "920/920 - 1s - loss: 0.6533 - accuracy: 0.5788 - val_loss: 0.6687 - val_accuracy: 0.5913\n",
      "Epoch 156/250\n",
      "920/920 - 1s - loss: 0.6527 - accuracy: 0.5728 - val_loss: 0.6623 - val_accuracy: 0.6130\n",
      "Epoch 157/250\n",
      "920/920 - 1s - loss: 0.6545 - accuracy: 0.5793 - val_loss: 0.6627 - val_accuracy: 0.6130\n",
      "Epoch 158/250\n",
      "920/920 - 1s - loss: 0.6549 - accuracy: 0.5793 - val_loss: 0.6692 - val_accuracy: 0.5957\n",
      "Epoch 159/250\n",
      "920/920 - 1s - loss: 0.6546 - accuracy: 0.5712 - val_loss: 0.6631 - val_accuracy: 0.6109\n",
      "Epoch 160/250\n",
      "920/920 - 1s - loss: 0.6537 - accuracy: 0.5783 - val_loss: 0.6649 - val_accuracy: 0.6130\n",
      "Epoch 161/250\n",
      "920/920 - 1s - loss: 0.6529 - accuracy: 0.5625 - val_loss: 0.6685 - val_accuracy: 0.5935\n",
      "Epoch 162/250\n",
      "920/920 - 2s - loss: 0.6551 - accuracy: 0.5772 - val_loss: 0.6658 - val_accuracy: 0.5891\n",
      "Epoch 163/250\n",
      "920/920 - 1s - loss: 0.6534 - accuracy: 0.5793 - val_loss: 0.6624 - val_accuracy: 0.6152\n",
      "Epoch 164/250\n",
      "920/920 - 1s - loss: 0.6525 - accuracy: 0.5788 - val_loss: 0.6638 - val_accuracy: 0.6196\n",
      "Epoch 165/250\n",
      "920/920 - 1s - loss: 0.6533 - accuracy: 0.5755 - val_loss: 0.6633 - val_accuracy: 0.6130\n",
      "Epoch 166/250\n",
      "920/920 - 1s - loss: 0.6532 - accuracy: 0.5690 - val_loss: 0.6667 - val_accuracy: 0.6130\n",
      "Epoch 167/250\n",
      "920/920 - 1s - loss: 0.6531 - accuracy: 0.5793 - val_loss: 0.6670 - val_accuracy: 0.6174\n",
      "Epoch 168/250\n",
      "920/920 - 1s - loss: 0.6535 - accuracy: 0.5674 - val_loss: 0.6661 - val_accuracy: 0.6174\n",
      "Epoch 169/250\n",
      "920/920 - 1s - loss: 0.6510 - accuracy: 0.5717 - val_loss: 0.6646 - val_accuracy: 0.6109\n",
      "Epoch 170/250\n",
      "920/920 - 1s - loss: 0.6519 - accuracy: 0.5788 - val_loss: 0.6684 - val_accuracy: 0.5891\n",
      "Epoch 171/250\n",
      "920/920 - 1s - loss: 0.6520 - accuracy: 0.5707 - val_loss: 0.6663 - val_accuracy: 0.6130\n",
      "Epoch 172/250\n",
      "920/920 - 1s - loss: 0.6524 - accuracy: 0.5663 - val_loss: 0.6652 - val_accuracy: 0.6109\n"
     ]
    }
   ],
   "source": [
    "model_std_v1 = init_model(X_std_train)\n",
    "# Define early stopping parameters\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "\n",
    "#Train the model\n",
    "history_std_v1 = model_std_v1.fit(X_std_train, y_std_train, \n",
    "          epochs=250, \n",
    "          batch_size=2, \n",
    "          verbose=2, \n",
    "          callbacks=es,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model std version 1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.12      0.20       788\n",
      "         1.0       0.56      0.96      0.70       912\n",
      "\n",
      "    accuracy                           0.57      1700\n",
      "   macro avg       0.62      0.54      0.45      1700\n",
      "weighted avg       0.62      0.57      0.47      1700\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_get_conf_learning(model_std_v1,history_std_v1,X_std_test, y_std_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max scores and technical features (with ichimoku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "920/920 [==============================] - 4s 2ms/step - loss: 0.6948 - accuracy: 0.5168 - val_loss: 0.6939 - val_accuracy: 0.5217\n",
      "Epoch 2/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5473 - val_loss: 0.6941 - val_accuracy: 0.5217\n",
      "Epoch 3/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6896 - accuracy: 0.5375 - val_loss: 0.6922 - val_accuracy: 0.5261\n",
      "Epoch 4/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5266 - val_loss: 0.6916 - val_accuracy: 0.5261\n",
      "Epoch 5/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6870 - accuracy: 0.5370 - val_loss: 0.6949 - val_accuracy: 0.5261\n",
      "Epoch 6/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5408 - val_loss: 0.6909 - val_accuracy: 0.5304\n",
      "Epoch 7/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5408 - val_loss: 0.6903 - val_accuracy: 0.5304\n",
      "Epoch 8/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6869 - accuracy: 0.5462 - val_loss: 0.6895 - val_accuracy: 0.5261\n",
      "Epoch 9/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6866 - accuracy: 0.5429 - val_loss: 0.6897 - val_accuracy: 0.5304\n",
      "Epoch 10/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5543 - val_loss: 0.6904 - val_accuracy: 0.5239\n",
      "Epoch 11/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.5446 - val_loss: 0.6916 - val_accuracy: 0.5196\n",
      "Epoch 12/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6835 - accuracy: 0.5495 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 13/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6822 - accuracy: 0.5473 - val_loss: 0.6919 - val_accuracy: 0.4870\n",
      "Epoch 14/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6789 - accuracy: 0.5630 - val_loss: 0.6903 - val_accuracy: 0.4935\n",
      "Epoch 15/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6805 - accuracy: 0.5505 - val_loss: 0.6898 - val_accuracy: 0.5261\n",
      "Epoch 16/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6797 - accuracy: 0.5538 - val_loss: 0.6921 - val_accuracy: 0.5174\n",
      "Epoch 17/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6810 - accuracy: 0.5571 - val_loss: 0.6920 - val_accuracy: 0.5283\n",
      "Epoch 18/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6785 - accuracy: 0.5625 - val_loss: 0.6901 - val_accuracy: 0.5174\n",
      "Epoch 19/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6785 - accuracy: 0.5620 - val_loss: 0.6849 - val_accuracy: 0.5283\n",
      "Epoch 20/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6810 - accuracy: 0.5549 - val_loss: 0.6822 - val_accuracy: 0.5130\n",
      "Epoch 21/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6805 - accuracy: 0.5538 - val_loss: 0.6852 - val_accuracy: 0.5348\n",
      "Epoch 22/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6792 - accuracy: 0.5440 - val_loss: 0.6878 - val_accuracy: 0.5304\n",
      "Epoch 23/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6760 - accuracy: 0.5630 - val_loss: 0.6852 - val_accuracy: 0.5326\n",
      "Epoch 24/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6778 - accuracy: 0.5609 - val_loss: 0.6824 - val_accuracy: 0.4978\n",
      "Epoch 25/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6777 - accuracy: 0.5582 - val_loss: 0.6820 - val_accuracy: 0.5043\n",
      "Epoch 26/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6775 - accuracy: 0.5549 - val_loss: 0.6828 - val_accuracy: 0.5348\n",
      "Epoch 27/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6753 - accuracy: 0.5603 - val_loss: 0.6827 - val_accuracy: 0.5326\n",
      "Epoch 28/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6748 - accuracy: 0.5630 - val_loss: 0.6831 - val_accuracy: 0.4870\n",
      "Epoch 29/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6757 - accuracy: 0.5462 - val_loss: 0.6818 - val_accuracy: 0.5413\n",
      "Epoch 30/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6741 - accuracy: 0.5668 - val_loss: 0.6892 - val_accuracy: 0.5348\n",
      "Epoch 31/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6711 - accuracy: 0.5652 - val_loss: 0.6822 - val_accuracy: 0.5391\n",
      "Epoch 32/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6727 - accuracy: 0.5647 - val_loss: 0.6838 - val_accuracy: 0.5391\n",
      "Epoch 33/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6703 - accuracy: 0.5750 - val_loss: 0.6827 - val_accuracy: 0.5543\n",
      "Epoch 34/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6714 - accuracy: 0.5783 - val_loss: 0.6830 - val_accuracy: 0.5413\n",
      "Epoch 35/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6723 - accuracy: 0.5723 - val_loss: 0.6801 - val_accuracy: 0.5522\n",
      "Epoch 36/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6701 - accuracy: 0.5777 - val_loss: 0.6814 - val_accuracy: 0.5543\n",
      "Epoch 37/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.5739 - val_loss: 0.6821 - val_accuracy: 0.5500\n",
      "Epoch 38/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6689 - accuracy: 0.5745 - val_loss: 0.6817 - val_accuracy: 0.5543\n",
      "Epoch 39/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6654 - accuracy: 0.5826 - val_loss: 0.6794 - val_accuracy: 0.5630\n",
      "Epoch 40/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6616 - accuracy: 0.5761 - val_loss: 0.6897 - val_accuracy: 0.5435\n",
      "Epoch 41/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6631 - accuracy: 0.5815 - val_loss: 0.6813 - val_accuracy: 0.5609\n",
      "Epoch 42/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6623 - accuracy: 0.5755 - val_loss: 0.6776 - val_accuracy: 0.5283\n",
      "Epoch 43/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6610 - accuracy: 0.5880 - val_loss: 0.6809 - val_accuracy: 0.5587\n",
      "Epoch 44/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6602 - accuracy: 0.5804 - val_loss: 0.6756 - val_accuracy: 0.5652\n",
      "Epoch 45/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6614 - accuracy: 0.5685 - val_loss: 0.6776 - val_accuracy: 0.5609\n",
      "Epoch 46/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6616 - accuracy: 0.5870 - val_loss: 0.6823 - val_accuracy: 0.4978\n",
      "Epoch 47/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6617 - accuracy: 0.5701 - val_loss: 0.6787 - val_accuracy: 0.5478\n",
      "Epoch 48/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6582 - accuracy: 0.5875 - val_loss: 0.6764 - val_accuracy: 0.5630\n",
      "Epoch 49/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5821 - val_loss: 0.6781 - val_accuracy: 0.5609\n",
      "Epoch 50/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6579 - accuracy: 0.5701 - val_loss: 0.6825 - val_accuracy: 0.5543\n",
      "Epoch 51/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6630 - accuracy: 0.5837 - val_loss: 0.6753 - val_accuracy: 0.5717\n",
      "Epoch 52/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6615 - accuracy: 0.5853 - val_loss: 0.6752 - val_accuracy: 0.5696\n",
      "Epoch 53/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6589 - accuracy: 0.5799 - val_loss: 0.6764 - val_accuracy: 0.5630\n",
      "Epoch 54/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6612 - accuracy: 0.5783 - val_loss: 0.6756 - val_accuracy: 0.5696\n",
      "Epoch 55/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6581 - accuracy: 0.5848 - val_loss: 0.6802 - val_accuracy: 0.5587\n",
      "Epoch 56/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6596 - accuracy: 0.5755 - val_loss: 0.6829 - val_accuracy: 0.5609\n",
      "Epoch 57/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.5815 - val_loss: 0.6807 - val_accuracy: 0.5630\n",
      "Epoch 58/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6562 - accuracy: 0.5859 - val_loss: 0.6748 - val_accuracy: 0.5717\n",
      "Epoch 59/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6546 - accuracy: 0.5837 - val_loss: 0.6765 - val_accuracy: 0.5652\n",
      "Epoch 60/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6557 - accuracy: 0.5712 - val_loss: 0.6824 - val_accuracy: 0.5609\n",
      "Epoch 61/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6539 - accuracy: 0.5842 - val_loss: 0.6794 - val_accuracy: 0.5696\n",
      "Epoch 62/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6541 - accuracy: 0.5810 - val_loss: 0.6776 - val_accuracy: 0.5413\n",
      "Epoch 63/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6550 - accuracy: 0.5793 - val_loss: 0.6840 - val_accuracy: 0.5696\n",
      "Epoch 64/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6556 - accuracy: 0.5696 - val_loss: 0.6813 - val_accuracy: 0.5652\n",
      "Epoch 65/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6532 - accuracy: 0.5821 - val_loss: 0.6808 - val_accuracy: 0.5696\n",
      "Epoch 66/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6525 - accuracy: 0.5777 - val_loss: 0.6836 - val_accuracy: 0.5565\n",
      "Epoch 67/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6517 - accuracy: 0.5810 - val_loss: 0.6821 - val_accuracy: 0.5696\n",
      "Epoch 68/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6521 - accuracy: 0.5837 - val_loss: 0.6886 - val_accuracy: 0.5565\n",
      "Epoch 69/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6539 - accuracy: 0.5793 - val_loss: 0.6809 - val_accuracy: 0.5696\n",
      "Epoch 70/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6513 - accuracy: 0.5766 - val_loss: 0.6859 - val_accuracy: 0.5696\n",
      "Epoch 71/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6489 - accuracy: 0.5918 - val_loss: 0.6781 - val_accuracy: 0.5696\n",
      "Epoch 72/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6486 - accuracy: 0.5973 - val_loss: 0.6792 - val_accuracy: 0.5717\n",
      "Epoch 73/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6487 - accuracy: 0.5837 - val_loss: 0.6803 - val_accuracy: 0.5370\n",
      "Epoch 74/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6487 - accuracy: 0.5897 - val_loss: 0.6804 - val_accuracy: 0.5370\n",
      "Epoch 75/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6488 - accuracy: 0.5701 - val_loss: 0.6910 - val_accuracy: 0.5565\n",
      "Epoch 76/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6489 - accuracy: 0.5902 - val_loss: 0.6831 - val_accuracy: 0.5674\n",
      "Epoch 77/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6452 - accuracy: 0.5995 - val_loss: 0.6877 - val_accuracy: 0.5674\n",
      "Epoch 78/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6473 - accuracy: 0.5842 - val_loss: 0.6860 - val_accuracy: 0.5370\n",
      "Epoch 79/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6466 - accuracy: 0.5783 - val_loss: 0.6869 - val_accuracy: 0.5652\n",
      "Epoch 80/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6458 - accuracy: 0.5935 - val_loss: 0.6895 - val_accuracy: 0.5674\n",
      "Epoch 81/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6471 - accuracy: 0.5875 - val_loss: 0.6870 - val_accuracy: 0.5630\n",
      "Epoch 82/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6461 - accuracy: 0.5832 - val_loss: 0.6882 - val_accuracy: 0.5630\n",
      "Epoch 83/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6486 - accuracy: 0.5837 - val_loss: 0.6874 - val_accuracy: 0.5630\n",
      "Epoch 84/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6460 - accuracy: 0.5880 - val_loss: 0.6901 - val_accuracy: 0.5630\n",
      "Epoch 85/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6438 - accuracy: 0.5897 - val_loss: 0.6892 - val_accuracy: 0.5522\n",
      "Epoch 86/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6459 - accuracy: 0.5859 - val_loss: 0.6884 - val_accuracy: 0.5652\n",
      "Epoch 87/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6444 - accuracy: 0.5837 - val_loss: 0.6908 - val_accuracy: 0.5652\n",
      "Epoch 88/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6452 - accuracy: 0.5832 - val_loss: 0.6902 - val_accuracy: 0.5652\n",
      "Epoch 89/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6442 - accuracy: 0.5821 - val_loss: 0.6897 - val_accuracy: 0.5348\n",
      "Epoch 90/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6448 - accuracy: 0.5924 - val_loss: 0.6932 - val_accuracy: 0.5391\n",
      "Epoch 91/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6421 - accuracy: 0.5739 - val_loss: 0.6893 - val_accuracy: 0.5674\n",
      "Epoch 92/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6417 - accuracy: 0.5908 - val_loss: 0.7007 - val_accuracy: 0.5674\n",
      "Epoch 93/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6430 - accuracy: 0.5935 - val_loss: 0.6975 - val_accuracy: 0.5630\n",
      "Epoch 94/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6413 - accuracy: 0.5940 - val_loss: 0.6955 - val_accuracy: 0.5304\n",
      "Epoch 95/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6423 - accuracy: 0.5728 - val_loss: 0.6969 - val_accuracy: 0.5609\n",
      "Epoch 96/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6424 - accuracy: 0.5946 - val_loss: 0.6953 - val_accuracy: 0.5609\n",
      "Epoch 97/250\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 0.6429 - accuracy: 0.5967 - val_loss: 0.6988 - val_accuracy: 0.5652\n",
      "Epoch 98/250\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 0.6411 - accuracy: 0.5946 - val_loss: 0.6954 - val_accuracy: 0.5630\n",
      "Epoch 99/250\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 0.6428 - accuracy: 0.5864 - val_loss: 0.6974 - val_accuracy: 0.5652\n",
      "Epoch 100/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6408 - accuracy: 0.5935 - val_loss: 0.7061 - val_accuracy: 0.5196\n",
      "Epoch 101/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6413 - accuracy: 0.5853 - val_loss: 0.6971 - val_accuracy: 0.5652\n",
      "Epoch 102/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6409 - accuracy: 0.5946 - val_loss: 0.6970 - val_accuracy: 0.5609\n",
      "Epoch 103/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6414 - accuracy: 0.5772 - val_loss: 0.6988 - val_accuracy: 0.5565\n",
      "Epoch 104/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6393 - accuracy: 0.5978 - val_loss: 0.7012 - val_accuracy: 0.5587\n",
      "Epoch 105/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6380 - accuracy: 0.5891 - val_loss: 0.6997 - val_accuracy: 0.5587\n",
      "Epoch 106/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6384 - accuracy: 0.5918 - val_loss: 0.7078 - val_accuracy: 0.5109\n",
      "Epoch 107/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6388 - accuracy: 0.5886 - val_loss: 0.7041 - val_accuracy: 0.5543\n",
      "Epoch 108/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6389 - accuracy: 0.5924 - val_loss: 0.7034 - val_accuracy: 0.5543\n",
      "Epoch 109/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6414 - accuracy: 0.5924 - val_loss: 0.7089 - val_accuracy: 0.5565\n",
      "Epoch 110/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6386 - accuracy: 0.5880 - val_loss: 0.7012 - val_accuracy: 0.5543\n",
      "Epoch 111/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6363 - accuracy: 0.5951 - val_loss: 0.7070 - val_accuracy: 0.5587\n",
      "Epoch 112/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6391 - accuracy: 0.5897 - val_loss: 0.7038 - val_accuracy: 0.5326\n",
      "Epoch 113/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6348 - accuracy: 0.6005 - val_loss: 0.7065 - val_accuracy: 0.5522\n",
      "Epoch 114/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6345 - accuracy: 0.5995 - val_loss: 0.7127 - val_accuracy: 0.5239\n",
      "Epoch 115/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6362 - accuracy: 0.6000 - val_loss: 0.7067 - val_accuracy: 0.5500\n",
      "Epoch 116/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6365 - accuracy: 0.5924 - val_loss: 0.7080 - val_accuracy: 0.5304\n",
      "Epoch 117/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6356 - accuracy: 0.5978 - val_loss: 0.7133 - val_accuracy: 0.5587\n",
      "Epoch 118/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6368 - accuracy: 0.5913 - val_loss: 0.7085 - val_accuracy: 0.5457\n",
      "Epoch 119/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6373 - accuracy: 0.5978 - val_loss: 0.7120 - val_accuracy: 0.5239\n",
      "Epoch 120/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6350 - accuracy: 0.6005 - val_loss: 0.7161 - val_accuracy: 0.5565\n",
      "Epoch 121/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6350 - accuracy: 0.5984 - val_loss: 0.7088 - val_accuracy: 0.5565\n",
      "Epoch 122/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6344 - accuracy: 0.5897 - val_loss: 0.7080 - val_accuracy: 0.5565\n",
      "Epoch 123/250\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 0.6341 - accuracy: 0.5924 - val_loss: 0.7133 - val_accuracy: 0.5565\n",
      "Epoch 124/250\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 0.6360 - accuracy: 0.5832 - val_loss: 0.7134 - val_accuracy: 0.5565\n",
      "Epoch 125/250\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 0.6350 - accuracy: 0.5908 - val_loss: 0.7104 - val_accuracy: 0.5543\n",
      "Epoch 126/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6349 - accuracy: 0.6098 - val_loss: 0.7069 - val_accuracy: 0.5543\n",
      "Epoch 127/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6334 - accuracy: 0.6022 - val_loss: 0.7072 - val_accuracy: 0.5174\n",
      "Epoch 128/250\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 0.6351 - accuracy: 0.5891 - val_loss: 0.7086 - val_accuracy: 0.5565\n"
     ]
    }
   ],
   "source": [
    "model_max = init_model(X_max_train)\n",
    "\n",
    "es = EarlyStopping(patience=70, restore_best_weights=True,monitor='val_loss')\n",
    "history_max_v1 = model_max.fit(X_max_train, y_max_train, \n",
    "          epochs=250, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=es,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model max version 1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.16      0.25       794\n",
      "         1.0       0.56      0.93      0.69       906\n",
      "\n",
      "    accuracy                           0.57      1700\n",
      "   macro avg       0.60      0.54      0.47      1700\n",
      "weighted avg       0.60      0.57      0.49      1700\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_get_conf_learning(model_max,history_max_v1,X_max_test, y_max_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check differences between three last models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy score:  0.5558823347091675\n",
      "std accuracy score: 0.5664705634117126\n",
      "max accuracy score: 0.5664705634117126\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean accuracy score:  {model_mean_v1.evaluate(X_mean_test,y_mean_test,verbose=0)[1]}\")\n",
    "print(f\"std accuracy score: {model_std_v1.evaluate(X_std_test,y_std_test,verbose=0)[1]}\")\n",
    "print(f\"max accuracy score: {model_max.evaluate(X_max_test,y_max_test,verbose=0)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second exploration experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['class'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/1xvg6x991594dlshm5qcjr7w0000gn/T/ipykernel_74778/3215969808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_2_mean = daily_BR[['date', 'score_mean', \n\u001b[0m\u001b[1;32m      2\u001b[0m        \u001b[0;34m'v_neg_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v_neu_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0;34m'v_pos_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v_compound_mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't_pol_mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't_sub_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'Close'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        ,'class']].copy()\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5780\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5782\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5784\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5844\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5847\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['class'] not in index\""
     ]
    }
   ],
   "source": [
    "test_2_mean = daily_BR[['date', 'score_mean', \n",
    "       'v_neg_mean','v_neu_mean',\n",
    "       'v_pos_mean','v_compound_mean', 't_pol_mean', 't_sub_mean',\n",
    "        'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_std = daily_BR[['date', 'score_std', \n",
    "       'v_neg_std','v_neu_std',\n",
    "       'v_pos_std','v_compound_std', 't_pol_std', 't_sub_std',\n",
    "        'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_max = daily_BR[['date', 'score_max', \n",
    "       'v_neg_max','v_neu_max',\n",
    "       'v_pos_max','v_compound_max', 't_pol_max', 't_sub_max',\n",
    "       'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_sum = daily_BR[['date', 'score_sum', \n",
    "       'v_neg_sum','v_neu_sum',\n",
    "       'v_pos_sum','v_compound_sum', 't_pol_sum', 't_sub_sum',\n",
    "       'Close'\n",
    "       ,'class']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_2_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/1xvg6x991594dlshm5qcjr7w0000gn/T/ipykernel_74778/202380739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mget_X_y_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_2_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX2_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2_std\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mget_X_y_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_2_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX3_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my3_max\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mget_X_y_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_2_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX3_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my3_sum\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mget_X_y_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_2_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_2_mean' is not defined"
     ]
    }
   ],
   "source": [
    "X_mean,y_mean =get_X_y_random(test_2_mean.drop(columns=\"date\"), 1500,6)\n",
    "X2_std,y2_std =get_X_y_random(test_2_std.drop(columns=\"date\"), 1500,6)\n",
    "X3_max,y3_max =get_X_y_random(test_2_max.drop(columns=\"date\"), 1500,6)\n",
    "X3_sum,y3_sum =get_X_y_random(test_2_sum.drop(columns=\"date\"), 1500,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_mean, y_mean = shuffle(X_mean, y_mean, random_state=4)\n",
    "X2_std, y2_std = shuffle(X2_std, y2_std, random_state=4)\n",
    "X3_max, y3_max = shuffle(X3_max, y3_max, random_state=4)\n",
    "X3_sum, y3_sum = shuffle(X3_sum, y3_sum, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1200\n",
    "\n",
    "\n",
    "X_train_2 = X_mean[:train_size]\n",
    "X_test_2 = X_mean[train_size:]\n",
    "y_train_2 = y_mean[:train_size]\n",
    "y_test_2=y_mean[train_size:]\n",
    "\n",
    "#std\n",
    "X_train_std_2 = X2_std[:train_size]\n",
    "X_test_std_2 = X2_std[train_size:]\n",
    "y_train_std_2 = y2_std[:train_size]\n",
    "y_test_std_2 =y2_std[train_size:]\n",
    "\n",
    "#max\n",
    "X_train_max_2 = X3_max[:train_size]\n",
    "X_test_max_2 = X3_max[train_size:]\n",
    "y_train_max_2 = y3_max[:train_size]\n",
    "y_test_max_2=y3_max[train_size:]\n",
    "\n",
    "#sum\n",
    "X_train_sum_2 = X3_sum[:train_size]\n",
    "X_test_sum_2 = X3_sum[train_size:]\n",
    "y_train_sum_2 = y3_sum[:train_size]\n",
    "y_test_sum_2=y3_sum[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = models.Sequential()\n",
    "    neurons = 32\n",
    "\n",
    "    model.add(layers.LSTM(neurons,input_shape=(X_mean.shape[1],X_mean.shape[2] ), activation=\"tanh\"))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5607 - val_loss: 0.6977 - val_accuracy: 0.5139\n",
      "Epoch 2/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5702 - val_loss: 0.7006 - val_accuracy: 0.5139\n",
      "Epoch 3/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.5702 - val_loss: 0.6999 - val_accuracy: 0.5139\n",
      "Epoch 4/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.7010 - val_accuracy: 0.5139\n",
      "Epoch 5/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5702 - val_loss: 0.7030 - val_accuracy: 0.5139\n",
      "Epoch 6/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.6978 - val_accuracy: 0.5139\n",
      "Epoch 7/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.6976 - val_accuracy: 0.5139\n",
      "Epoch 8/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.5702 - val_loss: 0.6996 - val_accuracy: 0.5139\n",
      "Epoch 9/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.7033 - val_accuracy: 0.5139\n",
      "Epoch 10/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5702 - val_loss: 0.7009 - val_accuracy: 0.5139\n",
      "Epoch 11/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6959 - val_accuracy: 0.5139\n",
      "Epoch 12/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.5702 - val_loss: 0.6968 - val_accuracy: 0.5139\n",
      "Epoch 13/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6987 - val_accuracy: 0.5139\n",
      "Epoch 14/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.5702 - val_loss: 0.7031 - val_accuracy: 0.5139\n",
      "Epoch 15/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.6967 - val_accuracy: 0.5139\n",
      "Epoch 16/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5702 - val_loss: 0.7009 - val_accuracy: 0.5139\n",
      "Epoch 17/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.5702 - val_loss: 0.7000 - val_accuracy: 0.5139\n",
      "Epoch 18/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5702 - val_loss: 0.7019 - val_accuracy: 0.5139\n",
      "Epoch 19/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.5702 - val_loss: 0.6987 - val_accuracy: 0.5139\n",
      "Epoch 20/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6999 - val_accuracy: 0.5139\n",
      "Epoch 21/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6998 - val_accuracy: 0.5139\n",
      "Epoch 22/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.7022 - val_accuracy: 0.5139\n",
      "Epoch 23/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5702 - val_loss: 0.7054 - val_accuracy: 0.5139\n",
      "Epoch 24/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6971 - val_accuracy: 0.5139\n",
      "Epoch 25/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.6966 - val_accuracy: 0.5139\n",
      "Epoch 26/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6844 - accuracy: 0.5702 - val_loss: 0.7001 - val_accuracy: 0.5139\n",
      "Epoch 27/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6972 - val_accuracy: 0.5139\n",
      "Epoch 28/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5702 - val_loss: 0.6990 - val_accuracy: 0.5139\n",
      "Epoch 29/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.5702 - val_loss: 0.6996 - val_accuracy: 0.5139\n",
      "Epoch 30/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6850 - accuracy: 0.5702 - val_loss: 0.7062 - val_accuracy: 0.5139\n",
      "Epoch 31/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5702 - val_loss: 0.7010 - val_accuracy: 0.5139\n",
      "Epoch 32/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5702 - val_loss: 0.6985 - val_accuracy: 0.5139\n",
      "Epoch 33/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.6992 - val_accuracy: 0.5139\n",
      "Epoch 34/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.7007 - val_accuracy: 0.5139\n",
      "Epoch 35/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.7016 - val_accuracy: 0.5139\n",
      "Epoch 36/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.6998 - val_accuracy: 0.5139\n",
      "Epoch 37/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.5702 - val_loss: 0.6966 - val_accuracy: 0.5139\n",
      "Epoch 38/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6971 - val_accuracy: 0.5139\n",
      "Epoch 39/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.7005 - val_accuracy: 0.5139\n",
      "Epoch 40/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6984 - val_accuracy: 0.5139\n",
      "Epoch 41/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.7021 - val_accuracy: 0.5139\n",
      "Epoch 42/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.6993 - val_accuracy: 0.5139\n",
      "Epoch 43/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6999 - val_accuracy: 0.5139\n",
      "Epoch 44/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.7002 - val_accuracy: 0.5139\n",
      "Epoch 45/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.7008 - val_accuracy: 0.5139\n",
      "Epoch 46/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6997 - val_accuracy: 0.5139\n",
      "Epoch 47/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.6991 - val_accuracy: 0.5139\n",
      "Epoch 48/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6978 - val_accuracy: 0.5139\n",
      "Epoch 49/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6993 - val_accuracy: 0.5139\n",
      "Epoch 50/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.7009 - val_accuracy: 0.5139\n",
      "Epoch 51/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6994 - val_accuracy: 0.5139\n",
      "Epoch 52/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5702 - val_loss: 0.7029 - val_accuracy: 0.5139\n",
      "Epoch 53/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5702 - val_loss: 0.7015 - val_accuracy: 0.5139\n",
      "Epoch 54/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.7017 - val_accuracy: 0.5139\n",
      "Epoch 55/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.7002 - val_accuracy: 0.5139\n",
      "Epoch 56/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6988 - val_accuracy: 0.5139\n",
      "Epoch 57/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.6984 - val_accuracy: 0.5139\n",
      "Epoch 58/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6994 - val_accuracy: 0.5139\n",
      "Epoch 59/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.6990 - val_accuracy: 0.5139\n",
      "Epoch 60/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6994 - val_accuracy: 0.5139\n",
      "Epoch 61/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6973 - val_accuracy: 0.5139\n",
      "Epoch 62/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.6973 - val_accuracy: 0.5139\n",
      "Epoch 63/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6988 - val_accuracy: 0.5139\n",
      "Epoch 64/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6980 - val_accuracy: 0.5139\n",
      "Epoch 65/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6978 - val_accuracy: 0.5139\n",
      "Epoch 66/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.7011 - val_accuracy: 0.5139\n",
      "Epoch 67/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6996 - val_accuracy: 0.5139\n",
      "Epoch 68/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6981 - val_accuracy: 0.5139\n",
      "Epoch 69/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6992 - val_accuracy: 0.5139\n",
      "Epoch 70/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6978 - val_accuracy: 0.5139\n",
      "Epoch 71/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6993 - val_accuracy: 0.5139\n",
      "Epoch 72/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6975 - val_accuracy: 0.5139\n",
      "Epoch 73/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6988 - val_accuracy: 0.5139\n",
      "Epoch 74/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.6984 - val_accuracy: 0.5139\n",
      "Epoch 75/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6996 - val_accuracy: 0.5139\n",
      "Epoch 76/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6990 - val_accuracy: 0.5139\n",
      "Epoch 77/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6999 - val_accuracy: 0.5139\n",
      "Epoch 78/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6990 - val_accuracy: 0.5139\n",
      "Epoch 79/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.7002 - val_accuracy: 0.5139\n",
      "Epoch 80/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.7008 - val_accuracy: 0.5139\n",
      "Epoch 81/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.5702 - val_loss: 0.7008 - val_accuracy: 0.5139\n",
      "Epoch 82/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6996 - val_accuracy: 0.5139\n",
      "Epoch 83/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6982 - val_accuracy: 0.5139\n",
      "Epoch 84/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6986 - val_accuracy: 0.5139\n",
      "Epoch 85/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5702 - val_loss: 0.6998 - val_accuracy: 0.5139\n",
      "Epoch 86/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5702 - val_loss: 0.6991 - val_accuracy: 0.5139\n",
      "Epoch 87/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6982 - val_accuracy: 0.5139\n",
      "Epoch 88/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6990 - val_accuracy: 0.5139\n",
      "Epoch 89/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6977 - val_accuracy: 0.5139\n",
      "Epoch 90/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5702 - val_loss: 0.6988 - val_accuracy: 0.5139\n",
      "Epoch 91/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5702 - val_loss: 0.6995 - val_accuracy: 0.5139\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_mean_3 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_mean_3 = model_mean_3.fit(X_train_2, y_train_2, \n",
    "          epochs=500, \n",
    "          batch_size=4, \n",
    "          verbose=1, \n",
    "          callbacks=[es],\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  1/480 [..............................] - ETA: 5:32 - loss: 0.7000 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 21:44:17.556911: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-06-08 21:44:17.556927: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79/480 [===>..........................] - ETA: 3s - loss: 0.6937 - accuracy: 0.5316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 21:44:17.961989: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-06-08 21:44:17.964346: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-06-08 21:44:17.966668: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17\n",
      "2022-06-08 21:44:17.968002: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.trace.json.gz\n",
      "2022-06-08 21:44:17.969994: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17\n",
      "2022-06-08 21:44:17.970105: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.memory_profile.json.gz\n",
      "2022-06-08 21:44:17.970696: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17Dumped tool data for xplane.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_44_17/MacBook-Air-de-christopher.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5552 - val_loss: 0.6991 - val_accuracy: 0.4750\n",
      "Epoch 2/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5562 - val_loss: 0.7151 - val_accuracy: 0.4750\n",
      "Epoch 3/500\n",
      "480/480 [==============================] - 2s 3ms/step - loss: 0.6896 - accuracy: 0.5562 - val_loss: 0.7026 - val_accuracy: 0.4750\n",
      "Epoch 4/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6905 - accuracy: 0.5562 - val_loss: 0.7044 - val_accuracy: 0.4750\n",
      "Epoch 5/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6896 - accuracy: 0.5562 - val_loss: 0.7084 - val_accuracy: 0.4750\n",
      "Epoch 6/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5500 - val_loss: 0.7145 - val_accuracy: 0.4750\n",
      "Epoch 7/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6890 - accuracy: 0.5562 - val_loss: 0.7025 - val_accuracy: 0.4750\n",
      "Epoch 8/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5562 - val_loss: 0.7022 - val_accuracy: 0.4750\n",
      "Epoch 9/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6880 - accuracy: 0.5562 - val_loss: 0.7175 - val_accuracy: 0.4750\n",
      "Epoch 10/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6896 - accuracy: 0.5521 - val_loss: 0.7138 - val_accuracy: 0.4750\n",
      "Epoch 11/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6907 - accuracy: 0.5375 - val_loss: 0.6960 - val_accuracy: 0.4750\n",
      "Epoch 12/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5542 - val_loss: 0.7015 - val_accuracy: 0.4750\n",
      "Epoch 13/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6894 - accuracy: 0.5562 - val_loss: 0.7068 - val_accuracy: 0.4750\n",
      "Epoch 14/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6891 - accuracy: 0.5500 - val_loss: 0.6945 - val_accuracy: 0.4750\n",
      "Epoch 15/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6887 - accuracy: 0.5479 - val_loss: 0.7140 - val_accuracy: 0.4750\n",
      "Epoch 16/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6898 - accuracy: 0.5500 - val_loss: 0.7055 - val_accuracy: 0.4750\n",
      "Epoch 17/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6907 - accuracy: 0.5562 - val_loss: 0.7041 - val_accuracy: 0.4750\n",
      "Epoch 18/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6886 - accuracy: 0.5562 - val_loss: 0.7019 - val_accuracy: 0.4750\n",
      "Epoch 19/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6885 - accuracy: 0.5500 - val_loss: 0.7056 - val_accuracy: 0.4750\n",
      "Epoch 20/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6886 - accuracy: 0.5458 - val_loss: 0.7078 - val_accuracy: 0.4750\n",
      "Epoch 21/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.5490 - val_loss: 0.7193 - val_accuracy: 0.4750\n",
      "Epoch 22/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6899 - accuracy: 0.5562 - val_loss: 0.7031 - val_accuracy: 0.4750\n",
      "Epoch 23/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6899 - accuracy: 0.5562 - val_loss: 0.6977 - val_accuracy: 0.4750\n",
      "Epoch 24/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5562 - val_loss: 0.7058 - val_accuracy: 0.4750\n",
      "Epoch 25/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6876 - accuracy: 0.5458 - val_loss: 0.7211 - val_accuracy: 0.4750\n",
      "Epoch 26/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6893 - accuracy: 0.5562 - val_loss: 0.7056 - val_accuracy: 0.4750\n",
      "Epoch 27/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.5562 - val_loss: 0.7002 - val_accuracy: 0.4750\n",
      "Epoch 28/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.5562 - val_loss: 0.6995 - val_accuracy: 0.4750\n",
      "Epoch 29/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.5500 - val_loss: 0.7119 - val_accuracy: 0.4750\n",
      "Epoch 30/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.7099 - val_accuracy: 0.4750\n",
      "Epoch 31/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6898 - accuracy: 0.5562 - val_loss: 0.7032 - val_accuracy: 0.4750\n",
      "Epoch 32/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6880 - accuracy: 0.5562 - val_loss: 0.7095 - val_accuracy: 0.4750\n",
      "Epoch 33/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6878 - accuracy: 0.5562 - val_loss: 0.6970 - val_accuracy: 0.4750\n",
      "Epoch 34/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6884 - accuracy: 0.5562 - val_loss: 0.7003 - val_accuracy: 0.4750\n",
      "Epoch 35/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7091 - val_accuracy: 0.4750\n",
      "Epoch 36/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6886 - accuracy: 0.5562 - val_loss: 0.7008 - val_accuracy: 0.4750\n",
      "Epoch 37/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.5521 - val_loss: 0.7081 - val_accuracy: 0.4750\n",
      "Epoch 38/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6890 - accuracy: 0.5500 - val_loss: 0.7045 - val_accuracy: 0.4750\n",
      "Epoch 39/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6865 - accuracy: 0.5562 - val_loss: 0.7133 - val_accuracy: 0.4750\n",
      "Epoch 40/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7090 - val_accuracy: 0.4750\n",
      "Epoch 41/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6884 - accuracy: 0.5562 - val_loss: 0.7076 - val_accuracy: 0.4750\n",
      "Epoch 42/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.7010 - val_accuracy: 0.4750\n",
      "Epoch 43/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6875 - accuracy: 0.5562 - val_loss: 0.7054 - val_accuracy: 0.4750\n",
      "Epoch 44/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.6996 - val_accuracy: 0.4750\n",
      "Epoch 45/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.6991 - val_accuracy: 0.4750\n",
      "Epoch 46/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.5500 - val_loss: 0.7061 - val_accuracy: 0.4750\n",
      "Epoch 47/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6886 - accuracy: 0.5500 - val_loss: 0.6986 - val_accuracy: 0.4750\n",
      "Epoch 48/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6878 - accuracy: 0.5458 - val_loss: 0.7006 - val_accuracy: 0.4750\n",
      "Epoch 49/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6875 - accuracy: 0.5562 - val_loss: 0.6946 - val_accuracy: 0.4750\n",
      "Epoch 50/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5448 - val_loss: 0.7104 - val_accuracy: 0.4750\n",
      "Epoch 51/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6883 - accuracy: 0.5542 - val_loss: 0.7124 - val_accuracy: 0.4750\n",
      "Epoch 52/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.5562 - val_loss: 0.7003 - val_accuracy: 0.4750\n",
      "Epoch 53/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6881 - accuracy: 0.5562 - val_loss: 0.6987 - val_accuracy: 0.4750\n",
      "Epoch 54/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.5562 - val_loss: 0.7089 - val_accuracy: 0.4750\n",
      "Epoch 55/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6878 - accuracy: 0.5562 - val_loss: 0.7031 - val_accuracy: 0.4750\n",
      "Epoch 56/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6880 - accuracy: 0.5562 - val_loss: 0.7058 - val_accuracy: 0.4750\n",
      "Epoch 57/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6871 - accuracy: 0.5562 - val_loss: 0.7030 - val_accuracy: 0.4750\n",
      "Epoch 58/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7045 - val_accuracy: 0.4750\n",
      "Epoch 59/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6875 - accuracy: 0.5562 - val_loss: 0.7041 - val_accuracy: 0.4750\n",
      "Epoch 60/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6874 - accuracy: 0.5562 - val_loss: 0.7021 - val_accuracy: 0.4750\n",
      "Epoch 61/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.6947 - val_accuracy: 0.4750\n",
      "Epoch 62/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6879 - accuracy: 0.5562 - val_loss: 0.6979 - val_accuracy: 0.4750\n",
      "Epoch 63/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7032 - val_accuracy: 0.4750\n",
      "Epoch 64/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7026 - val_accuracy: 0.4750\n",
      "Epoch 65/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6866 - accuracy: 0.5562 - val_loss: 0.7126 - val_accuracy: 0.4750\n",
      "Epoch 66/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7031 - val_accuracy: 0.4750\n",
      "Epoch 67/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6885 - accuracy: 0.5562 - val_loss: 0.7018 - val_accuracy: 0.4750\n",
      "Epoch 68/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6869 - accuracy: 0.5562 - val_loss: 0.7007 - val_accuracy: 0.4750\n",
      "Epoch 69/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.7025 - val_accuracy: 0.4750\n",
      "Epoch 70/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6883 - accuracy: 0.5479 - val_loss: 0.7022 - val_accuracy: 0.4750\n",
      "Epoch 71/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7026 - val_accuracy: 0.4750\n",
      "Epoch 72/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7050 - val_accuracy: 0.4750\n",
      "Epoch 73/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6870 - accuracy: 0.5562 - val_loss: 0.7029 - val_accuracy: 0.4750\n",
      "Epoch 74/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7067 - val_accuracy: 0.4750\n",
      "Epoch 75/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6869 - accuracy: 0.5562 - val_loss: 0.7017 - val_accuracy: 0.4750\n",
      "Epoch 76/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6874 - accuracy: 0.5562 - val_loss: 0.7047 - val_accuracy: 0.4750\n",
      "Epoch 77/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6869 - accuracy: 0.5562 - val_loss: 0.7002 - val_accuracy: 0.4750\n",
      "Epoch 78/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7034 - val_accuracy: 0.4750\n",
      "Epoch 79/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6873 - accuracy: 0.5562 - val_loss: 0.7028 - val_accuracy: 0.4750\n",
      "Epoch 80/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5562 - val_loss: 0.7006 - val_accuracy: 0.4750\n",
      "Epoch 81/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6869 - accuracy: 0.5562 - val_loss: 0.7024 - val_accuracy: 0.4750\n",
      "Epoch 82/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.7025 - val_accuracy: 0.4750\n",
      "Epoch 83/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6866 - accuracy: 0.5562 - val_loss: 0.7068 - val_accuracy: 0.4750\n",
      "Epoch 84/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6873 - accuracy: 0.5562 - val_loss: 0.7012 - val_accuracy: 0.4750\n",
      "Epoch 85/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6868 - accuracy: 0.5562 - val_loss: 0.7057 - val_accuracy: 0.4750\n",
      "Epoch 86/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6867 - accuracy: 0.5562 - val_loss: 0.7001 - val_accuracy: 0.4750\n",
      "Epoch 87/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6871 - accuracy: 0.5562 - val_loss: 0.7029 - val_accuracy: 0.4750\n",
      "Epoch 88/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6874 - accuracy: 0.5562 - val_loss: 0.7044 - val_accuracy: 0.4750\n",
      "Epoch 89/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5562 - val_loss: 0.7052 - val_accuracy: 0.4750\n",
      "Epoch 90/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6866 - accuracy: 0.5562 - val_loss: 0.7108 - val_accuracy: 0.4750\n",
      "Epoch 91/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6866 - accuracy: 0.5562 - val_loss: 0.6984 - val_accuracy: 0.4750\n",
      "Epoch 92/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.7000 - val_accuracy: 0.4750\n",
      "Epoch 93/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6874 - accuracy: 0.5562 - val_loss: 0.7048 - val_accuracy: 0.4750\n",
      "Epoch 94/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7038 - val_accuracy: 0.4750\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_std_3 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_std_3 = model_std_3.fit(X_train_std_2, y_train_std_2, \n",
    "          epochs=500, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=[es,tensorboard_callback],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "103/480 [=====>........................] - ETA: 0s - loss: 0.6839 - accuracy: 0.5680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 21:45:23.262934: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-06-08 21:45:23.262951: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-06-08 21:45:23.299785: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-06-08 21:45:23.302069: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-06-08 21:45:23.303937: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23\n",
      "2022-06-08 21:45:23.305267: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.trace.json.gz\n",
      "2022-06-08 21:45:23.307294: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23\n",
      "2022-06-08 21:45:23.307456: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.memory_profile.json.gz\n",
      "2022-06-08 21:45:23.307987: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23Dumped tool data for xplane.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_45_23/MacBook-Air-de-christopher.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 2s 3ms/step - loss: 0.6934 - accuracy: 0.5240 - val_loss: 0.7002 - val_accuracy: 0.4625\n",
      "Epoch 2/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5063 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 3/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5240 - val_loss: 0.6855 - val_accuracy: 0.5583\n",
      "Epoch 4/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5323 - val_loss: 0.6829 - val_accuracy: 0.5667\n",
      "Epoch 5/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6864 - accuracy: 0.5458 - val_loss: 0.6880 - val_accuracy: 0.5625\n",
      "Epoch 6/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5490 - val_loss: 0.6907 - val_accuracy: 0.4708\n",
      "Epoch 7/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6839 - accuracy: 0.5240 - val_loss: 0.6940 - val_accuracy: 0.4708\n",
      "Epoch 8/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5437 - val_loss: 0.6984 - val_accuracy: 0.4833\n",
      "Epoch 9/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.5156 - val_loss: 0.6897 - val_accuracy: 0.5542\n",
      "Epoch 10/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5437 - val_loss: 0.6909 - val_accuracy: 0.5667\n",
      "Epoch 11/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5260 - val_loss: 0.6936 - val_accuracy: 0.4708\n",
      "Epoch 12/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6860 - accuracy: 0.5135 - val_loss: 0.6898 - val_accuracy: 0.5500\n",
      "Epoch 13/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5094 - val_loss: 0.6902 - val_accuracy: 0.5500\n",
      "Epoch 14/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6847 - accuracy: 0.5417 - val_loss: 0.6887 - val_accuracy: 0.5708\n",
      "Epoch 15/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6850 - accuracy: 0.5167 - val_loss: 0.6917 - val_accuracy: 0.5500\n",
      "Epoch 16/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6875 - accuracy: 0.5281 - val_loss: 0.6895 - val_accuracy: 0.5667\n",
      "Epoch 17/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6832 - accuracy: 0.5375 - val_loss: 0.6939 - val_accuracy: 0.5583\n",
      "Epoch 18/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6828 - accuracy: 0.5323 - val_loss: 0.6905 - val_accuracy: 0.5375\n",
      "Epoch 19/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6813 - accuracy: 0.5375 - val_loss: 0.6896 - val_accuracy: 0.5542\n",
      "Epoch 20/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5458 - val_loss: 0.6933 - val_accuracy: 0.5417\n",
      "Epoch 21/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6862 - accuracy: 0.5365 - val_loss: 0.6943 - val_accuracy: 0.5583\n",
      "Epoch 22/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.5427 - val_loss: 0.6963 - val_accuracy: 0.5417\n",
      "Epoch 23/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6827 - accuracy: 0.5479 - val_loss: 0.7014 - val_accuracy: 0.4542\n",
      "Epoch 24/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6829 - accuracy: 0.5365 - val_loss: 0.6986 - val_accuracy: 0.5375\n",
      "Epoch 25/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6813 - accuracy: 0.5469 - val_loss: 0.7037 - val_accuracy: 0.4542\n",
      "Epoch 26/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5531 - val_loss: 0.7033 - val_accuracy: 0.4542\n",
      "Epoch 27/500\n",
      "480/480 [==============================] - 2s 3ms/step - loss: 0.6808 - accuracy: 0.5615 - val_loss: 0.6940 - val_accuracy: 0.5333\n",
      "Epoch 28/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.5552 - val_loss: 0.6956 - val_accuracy: 0.5292\n",
      "Epoch 29/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6813 - accuracy: 0.5500 - val_loss: 0.7004 - val_accuracy: 0.5167\n",
      "Epoch 30/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6815 - accuracy: 0.5594 - val_loss: 0.7024 - val_accuracy: 0.5042\n",
      "Epoch 31/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6817 - accuracy: 0.5448 - val_loss: 0.7025 - val_accuracy: 0.5083\n",
      "Epoch 32/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6800 - accuracy: 0.5542 - val_loss: 0.7047 - val_accuracy: 0.5167\n",
      "Epoch 33/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6813 - accuracy: 0.5521 - val_loss: 0.7074 - val_accuracy: 0.5083\n",
      "Epoch 34/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6799 - accuracy: 0.5594 - val_loss: 0.7073 - val_accuracy: 0.5042\n",
      "Epoch 35/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6817 - accuracy: 0.5417 - val_loss: 0.7023 - val_accuracy: 0.5083\n",
      "Epoch 36/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6805 - accuracy: 0.5437 - val_loss: 0.7099 - val_accuracy: 0.5042\n",
      "Epoch 37/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6792 - accuracy: 0.5479 - val_loss: 0.7029 - val_accuracy: 0.5125\n",
      "Epoch 38/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5448 - val_loss: 0.7135 - val_accuracy: 0.4875\n",
      "Epoch 39/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6826 - accuracy: 0.5500 - val_loss: 0.7036 - val_accuracy: 0.5167\n",
      "Epoch 40/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6821 - accuracy: 0.5521 - val_loss: 0.6994 - val_accuracy: 0.5167\n",
      "Epoch 41/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6819 - accuracy: 0.5500 - val_loss: 0.6974 - val_accuracy: 0.5167\n",
      "Epoch 42/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6824 - accuracy: 0.5521 - val_loss: 0.7015 - val_accuracy: 0.5167\n",
      "Epoch 43/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.5542 - val_loss: 0.7055 - val_accuracy: 0.5292\n",
      "Epoch 44/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5500 - val_loss: 0.7050 - val_accuracy: 0.5292\n",
      "Epoch 45/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6831 - accuracy: 0.5448 - val_loss: 0.7000 - val_accuracy: 0.5167\n",
      "Epoch 46/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6826 - accuracy: 0.5521 - val_loss: 0.6994 - val_accuracy: 0.5208\n",
      "Epoch 47/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6842 - accuracy: 0.5521 - val_loss: 0.7048 - val_accuracy: 0.5167\n",
      "Epoch 48/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6874 - accuracy: 0.5375 - val_loss: 0.6995 - val_accuracy: 0.5292\n",
      "Epoch 49/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6886 - accuracy: 0.5219 - val_loss: 0.6981 - val_accuracy: 0.5292\n",
      "Epoch 50/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6867 - accuracy: 0.5292 - val_loss: 0.6974 - val_accuracy: 0.5292\n",
      "Epoch 51/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6852 - accuracy: 0.5104 - val_loss: 0.6941 - val_accuracy: 0.5583\n",
      "Epoch 52/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6863 - accuracy: 0.5198 - val_loss: 0.6952 - val_accuracy: 0.5583\n",
      "Epoch 53/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6851 - accuracy: 0.5250 - val_loss: 0.6956 - val_accuracy: 0.5583\n",
      "Epoch 54/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6844 - accuracy: 0.5229 - val_loss: 0.6928 - val_accuracy: 0.5583\n",
      "Epoch 55/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6842 - accuracy: 0.5344 - val_loss: 0.6948 - val_accuracy: 0.5583\n",
      "Epoch 56/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6835 - accuracy: 0.5219 - val_loss: 0.6931 - val_accuracy: 0.5583\n",
      "Epoch 57/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5344 - val_loss: 0.6946 - val_accuracy: 0.5708\n",
      "Epoch 58/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6833 - accuracy: 0.5375 - val_loss: 0.6983 - val_accuracy: 0.4875\n",
      "Epoch 59/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.5198 - val_loss: 0.6953 - val_accuracy: 0.5708\n",
      "Epoch 60/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6831 - accuracy: 0.5417 - val_loss: 0.6948 - val_accuracy: 0.5708\n",
      "Epoch 61/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6839 - accuracy: 0.5302 - val_loss: 0.6932 - val_accuracy: 0.5708\n",
      "Epoch 62/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6833 - accuracy: 0.5219 - val_loss: 0.6916 - val_accuracy: 0.5708\n",
      "Epoch 63/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6831 - accuracy: 0.5437 - val_loss: 0.6905 - val_accuracy: 0.5708\n",
      "Epoch 64/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6818 - accuracy: 0.5427 - val_loss: 0.6946 - val_accuracy: 0.5417\n",
      "Epoch 65/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6807 - accuracy: 0.5490 - val_loss: 0.6936 - val_accuracy: 0.5417\n",
      "Epoch 66/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6795 - accuracy: 0.5604 - val_loss: 0.6904 - val_accuracy: 0.5500\n",
      "Epoch 67/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6800 - accuracy: 0.5646 - val_loss: 0.6926 - val_accuracy: 0.5417\n",
      "Epoch 68/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6785 - accuracy: 0.5500 - val_loss: 0.6960 - val_accuracy: 0.5500\n",
      "Epoch 69/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6790 - accuracy: 0.5656 - val_loss: 0.6939 - val_accuracy: 0.5500\n",
      "Epoch 70/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6792 - accuracy: 0.5656 - val_loss: 0.6960 - val_accuracy: 0.5500\n",
      "Epoch 71/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6790 - accuracy: 0.5635 - val_loss: 0.6960 - val_accuracy: 0.5417\n",
      "Epoch 72/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6801 - accuracy: 0.5437 - val_loss: 0.6945 - val_accuracy: 0.5417\n",
      "Epoch 73/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.5542 - val_loss: 0.6903 - val_accuracy: 0.5792\n",
      "Epoch 74/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6794 - accuracy: 0.5302 - val_loss: 0.6942 - val_accuracy: 0.5667\n",
      "Epoch 75/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.5448 - val_loss: 0.6934 - val_accuracy: 0.5667\n",
      "Epoch 76/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6805 - accuracy: 0.5521 - val_loss: 0.6934 - val_accuracy: 0.5708\n",
      "Epoch 77/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6822 - accuracy: 0.5406 - val_loss: 0.6977 - val_accuracy: 0.5708\n",
      "Epoch 78/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6808 - accuracy: 0.5375 - val_loss: 0.6977 - val_accuracy: 0.5542\n",
      "Epoch 79/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6804 - accuracy: 0.5417 - val_loss: 0.6920 - val_accuracy: 0.5708\n",
      "Epoch 80/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6780 - accuracy: 0.5458 - val_loss: 0.6918 - val_accuracy: 0.5708\n",
      "Epoch 81/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6795 - accuracy: 0.5177 - val_loss: 0.6951 - val_accuracy: 0.5667\n",
      "Epoch 82/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6798 - accuracy: 0.5437 - val_loss: 0.6899 - val_accuracy: 0.5708\n",
      "Epoch 83/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6782 - accuracy: 0.5385 - val_loss: 0.6911 - val_accuracy: 0.5708\n",
      "Epoch 84/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6786 - accuracy: 0.5427 - val_loss: 0.6909 - val_accuracy: 0.5708\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_max_3 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_max_3 = model_max_3.fit(X_train_max_2, y_train_max_2, \n",
    "          epochs=500, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=[es,tensorboard_callback],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 84/480 [====>.........................] - ETA: 1s - loss: 0.7385 - accuracy: 0.4107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 21:46:25.481113: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-06-08 21:46:25.481131: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-06-08 21:46:25.516954: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-06-08 21:46:25.519385: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-06-08 21:46:25.521566: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25\n",
      "2022-06-08 21:46:25.523011: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.trace.json.gz\n",
      "2022-06-08 21:46:25.525374: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25\n",
      "2022-06-08 21:46:25.525552: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.memory_profile.json.gz\n",
      "2022-06-08 21:46:25.526087: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25Dumped tool data for xplane.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_21_46_25/MacBook-Air-de-christopher.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 2s 3ms/step - loss: 0.7023 - accuracy: 0.4708 - val_loss: 0.6830 - val_accuracy: 0.5375\n",
      "Epoch 2/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5510 - val_loss: 0.7028 - val_accuracy: 0.5083\n",
      "Epoch 3/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6876 - accuracy: 0.5479 - val_loss: 0.6948 - val_accuracy: 0.5375\n",
      "Epoch 4/500\n",
      "480/480 [==============================] - 2s 3ms/step - loss: 0.6875 - accuracy: 0.5542 - val_loss: 0.6901 - val_accuracy: 0.5458\n",
      "Epoch 5/500\n",
      "480/480 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5500 - val_loss: 0.6925 - val_accuracy: 0.5250\n",
      "Epoch 6/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.5698 - val_loss: 0.6985 - val_accuracy: 0.5167\n",
      "Epoch 7/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.5469 - val_loss: 0.6892 - val_accuracy: 0.5250\n",
      "Epoch 8/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6839 - accuracy: 0.5375 - val_loss: 0.6929 - val_accuracy: 0.5167\n",
      "Epoch 9/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.5615 - val_loss: 0.6945 - val_accuracy: 0.4875\n",
      "Epoch 10/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.5375 - val_loss: 0.6862 - val_accuracy: 0.5250\n",
      "Epoch 11/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6852 - accuracy: 0.5333 - val_loss: 0.6955 - val_accuracy: 0.5208\n",
      "Epoch 12/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6867 - accuracy: 0.5479 - val_loss: 0.6937 - val_accuracy: 0.4833\n",
      "Epoch 13/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5417 - val_loss: 0.6936 - val_accuracy: 0.5208\n",
      "Epoch 14/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6869 - accuracy: 0.5552 - val_loss: 0.6961 - val_accuracy: 0.5250\n",
      "Epoch 15/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6871 - accuracy: 0.5396 - val_loss: 0.6959 - val_accuracy: 0.5250\n",
      "Epoch 16/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
      "Epoch 17/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.5406 - val_loss: 0.6934 - val_accuracy: 0.5250\n",
      "Epoch 18/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6833 - accuracy: 0.5521 - val_loss: 0.7120 - val_accuracy: 0.4958\n",
      "Epoch 19/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6866 - accuracy: 0.5396 - val_loss: 0.7019 - val_accuracy: 0.5042\n",
      "Epoch 20/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6865 - accuracy: 0.5365 - val_loss: 0.7047 - val_accuracy: 0.5250\n",
      "Epoch 21/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6881 - accuracy: 0.5375 - val_loss: 0.6952 - val_accuracy: 0.5250\n",
      "Epoch 22/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.5396 - val_loss: 0.6947 - val_accuracy: 0.5250\n",
      "Epoch 23/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6859 - accuracy: 0.5375 - val_loss: 0.6945 - val_accuracy: 0.5375\n",
      "Epoch 24/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5583 - val_loss: 0.7052 - val_accuracy: 0.4833\n",
      "Epoch 25/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6859 - accuracy: 0.5375 - val_loss: 0.6985 - val_accuracy: 0.5500\n",
      "Epoch 26/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6834 - accuracy: 0.5562 - val_loss: 0.6962 - val_accuracy: 0.5292\n",
      "Epoch 27/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6823 - accuracy: 0.5479 - val_loss: 0.6904 - val_accuracy: 0.5250\n",
      "Epoch 28/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6816 - accuracy: 0.5510 - val_loss: 0.6924 - val_accuracy: 0.5500\n",
      "Epoch 29/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6812 - accuracy: 0.5573 - val_loss: 0.6962 - val_accuracy: 0.4833\n",
      "Epoch 30/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6826 - accuracy: 0.5323 - val_loss: 0.6947 - val_accuracy: 0.5500\n",
      "Epoch 31/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6815 - accuracy: 0.5510 - val_loss: 0.6938 - val_accuracy: 0.5417\n",
      "Epoch 32/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6818 - accuracy: 0.5604 - val_loss: 0.6942 - val_accuracy: 0.5417\n",
      "Epoch 33/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6811 - accuracy: 0.5417 - val_loss: 0.6957 - val_accuracy: 0.5417\n",
      "Epoch 34/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.6970 - val_accuracy: 0.5292\n",
      "Epoch 35/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6823 - accuracy: 0.5531 - val_loss: 0.6952 - val_accuracy: 0.5292\n",
      "Epoch 36/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5542 - val_loss: 0.6956 - val_accuracy: 0.5500\n",
      "Epoch 37/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6816 - accuracy: 0.5583 - val_loss: 0.6982 - val_accuracy: 0.5250\n",
      "Epoch 38/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6812 - accuracy: 0.5542 - val_loss: 0.6961 - val_accuracy: 0.5500\n",
      "Epoch 39/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6829 - accuracy: 0.5448 - val_loss: 0.6949 - val_accuracy: 0.5500\n",
      "Epoch 40/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6799 - accuracy: 0.5510 - val_loss: 0.6947 - val_accuracy: 0.5500\n",
      "Epoch 41/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6800 - accuracy: 0.5469 - val_loss: 0.7018 - val_accuracy: 0.5250\n",
      "Epoch 42/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6794 - accuracy: 0.5500 - val_loss: 0.7023 - val_accuracy: 0.5250\n",
      "Epoch 43/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6781 - accuracy: 0.5396 - val_loss: 0.6948 - val_accuracy: 0.5500\n",
      "Epoch 44/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5510 - val_loss: 0.6969 - val_accuracy: 0.5250\n",
      "Epoch 45/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5365 - val_loss: 0.6941 - val_accuracy: 0.5375\n",
      "Epoch 46/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5469 - val_loss: 0.6934 - val_accuracy: 0.5375\n",
      "Epoch 47/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6797 - accuracy: 0.5437 - val_loss: 0.6981 - val_accuracy: 0.5375\n",
      "Epoch 48/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5490 - val_loss: 0.6938 - val_accuracy: 0.5375\n",
      "Epoch 49/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6821 - accuracy: 0.5365 - val_loss: 0.7012 - val_accuracy: 0.5375\n",
      "Epoch 50/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6837 - accuracy: 0.5406 - val_loss: 0.6969 - val_accuracy: 0.5375\n",
      "Epoch 51/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6812 - accuracy: 0.5427 - val_loss: 0.6969 - val_accuracy: 0.5375\n",
      "Epoch 52/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6802 - accuracy: 0.5365 - val_loss: 0.6986 - val_accuracy: 0.5375\n",
      "Epoch 53/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5552 - val_loss: 0.6952 - val_accuracy: 0.5375\n",
      "Epoch 54/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5542 - val_loss: 0.6963 - val_accuracy: 0.5375\n",
      "Epoch 55/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6814 - accuracy: 0.5427 - val_loss: 0.6969 - val_accuracy: 0.5375\n",
      "Epoch 56/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5542 - val_loss: 0.6970 - val_accuracy: 0.5375\n",
      "Epoch 57/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5594 - val_loss: 0.6960 - val_accuracy: 0.5375\n",
      "Epoch 58/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5375 - val_loss: 0.6970 - val_accuracy: 0.5375\n",
      "Epoch 59/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6807 - accuracy: 0.5531 - val_loss: 0.6962 - val_accuracy: 0.5375\n",
      "Epoch 60/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5521 - val_loss: 0.6987 - val_accuracy: 0.5375\n",
      "Epoch 61/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6813 - accuracy: 0.5552 - val_loss: 0.6977 - val_accuracy: 0.5375\n",
      "Epoch 62/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6812 - accuracy: 0.5479 - val_loss: 0.6959 - val_accuracy: 0.5375\n",
      "Epoch 63/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6823 - accuracy: 0.5479 - val_loss: 0.6967 - val_accuracy: 0.5375\n",
      "Epoch 64/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6810 - accuracy: 0.5448 - val_loss: 0.6966 - val_accuracy: 0.5375\n",
      "Epoch 65/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6815 - accuracy: 0.5531 - val_loss: 0.6972 - val_accuracy: 0.5375\n",
      "Epoch 66/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5552 - val_loss: 0.6972 - val_accuracy: 0.5375\n",
      "Epoch 67/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.5542 - val_loss: 0.6961 - val_accuracy: 0.5375\n",
      "Epoch 68/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6813 - accuracy: 0.5531 - val_loss: 0.6967 - val_accuracy: 0.5375\n",
      "Epoch 69/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6815 - accuracy: 0.5427 - val_loss: 0.6979 - val_accuracy: 0.5375\n",
      "Epoch 70/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.5552 - val_loss: 0.6962 - val_accuracy: 0.5375\n",
      "Epoch 71/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5552 - val_loss: 0.6978 - val_accuracy: 0.5375\n",
      "Epoch 72/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5448 - val_loss: 0.6964 - val_accuracy: 0.5375\n",
      "Epoch 73/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5552 - val_loss: 0.6968 - val_accuracy: 0.5375\n",
      "Epoch 74/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6815 - accuracy: 0.5531 - val_loss: 0.6973 - val_accuracy: 0.5375\n",
      "Epoch 75/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.5510 - val_loss: 0.6963 - val_accuracy: 0.5375\n",
      "Epoch 76/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5562 - val_loss: 0.6965 - val_accuracy: 0.5375\n",
      "Epoch 77/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6805 - accuracy: 0.5406 - val_loss: 0.6966 - val_accuracy: 0.5375\n",
      "Epoch 78/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5500 - val_loss: 0.6969 - val_accuracy: 0.5375\n",
      "Epoch 79/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6816 - accuracy: 0.5552 - val_loss: 0.6957 - val_accuracy: 0.5375\n",
      "Epoch 80/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6802 - accuracy: 0.5521 - val_loss: 0.6987 - val_accuracy: 0.5375\n",
      "Epoch 81/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6821 - accuracy: 0.5542 - val_loss: 0.6976 - val_accuracy: 0.5375\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_sum_3 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_sum_3 = model_sum_3.fit(X_train_sum_2, y_train_sum_2, \n",
    "          epochs=500, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=[es,tensorboard_callback],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2,X_train_std_2,X_test_std_2,y_train_std_2,y_test_std_2,\\\n",
    "X_train_max_2,X_test_max_2,y_train_max_2,y_test_max_2,X_train_sum_2,X_test_sum_2,y_train_sum_2,y_test_sum_2\\\n",
    "=create_all_data(test_2_mean,test_2_std,test_2_max,test_2_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5367\n",
      "mean score [0.6910333037376404, 0.5366666913032532]\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5333\n",
      "std score [0.6934347152709961, 0.5333333611488342]\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5800\n",
      " max [0.685415506362915, 0.5799999833106995]\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.4733\n",
      " sum [0.6928543448448181, 0.47333332896232605]\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean score {model_mean_3.evaluate(X_test_2,y_test_2)}\")\n",
    "print(f\"std score {model_std_3.evaluate(X_test_std_2,y_test_std_2)}\")\n",
    "print(f\" max {model_max_3.evaluate(X_test_max_2,y_test_max_2)}\")\n",
    "print(f\" sum {model_sum_3.evaluate(X_test_sum_2,y_test_sum_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test du shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'score_mean', 'score_std', 'score_sum', 'score_max',\n",
       "       'v_neg_mean', 'v_neg_std', 'v_neg_sum', 'v_neg_max', 'v_neu_mean',\n",
       "       'v_neu_sum', 'v_neu_std', 'v_neu_max', 'v_pos_mean', 'v_pos_std',\n",
       "       'v_pos_sum', 'v_pos_max', 'v_compound_mean', 'v_compound_std',\n",
       "       'v_compound_sum', 'v_compound_max', 't_pol_mean', 't_pol_std',\n",
       "       't_pol_sum', 't_pol_max', 't_sub_mean', 't_sub_std', 't_sub_sum',\n",
       "       't_sub_max', 'v_neu_score_mean', 'v_neu_score_sum', 'v_neu_score_std',\n",
       "       'v_neu_score_max', 'v_pos_score_mean', 'v_pos_score_std',\n",
       "       'v_pos_score_sum', 'v_pos_score_max', 'v_compound_score_mean',\n",
       "       'v_compound_score_std', 'v_compound_score_sum', 'v_compound_score_max',\n",
       "       't_pol_score_mean', 't_pol_score_std', 't_pol_score_sum',\n",
       "       't_pol_score_max', 't_sub_score_mean', 't_sub_score_std',\n",
       "       't_sub_score_sum', 't_sub_score_max', 'v_neg_score_mean',\n",
       "       'v_neg_score_std', 'v_neg_score_sum', 'v_neg_score_max', 'date_count',\n",
       "       'Date', 'High', 'Low', 'Open', 'Close', 'Volume', 'tenkan_sen',\n",
       "       'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'chikou_span',\n",
       "       'diff_kijun', 'diff_tenkan', 'diff_chikou', 'kijun_signal',\n",
       "       'tenkan_signal', 'chikou_signal', 'indice', 'date_time', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>Close</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>7195.230000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  v_neg_mean  v_neu_mean  v_pos_mean  \\\n",
       "0     2017-01-01    4.095483    0.061278    0.829154    0.109578   \n",
       "1     2017-01-02    7.143617    0.061524    0.829861    0.108612   \n",
       "2     2017-01-03    5.862605    0.060374    0.830037    0.109590   \n",
       "3     2017-01-04    4.271773    0.065616    0.824412    0.109981   \n",
       "4     2017-01-05    3.549738    0.060350    0.832056    0.107598   \n",
       "...          ...         ...         ...         ...         ...   \n",
       "1090  2019-12-27    3.924113    0.065896    0.827884    0.106214   \n",
       "1091  2019-12-28    5.111582    0.062417    0.834208    0.103384   \n",
       "1092  2019-12-29    4.245443    0.063334    0.829598    0.107070   \n",
       "1093  2019-12-30    5.417252    0.061741    0.833728    0.104532   \n",
       "1094  2019-12-31    3.635659    0.072388    0.816814    0.110788   \n",
       "\n",
       "      v_compound_mean  t_pol_mean  t_sub_mean        Close  class  \n",
       "0            0.247951    0.102978    0.447966   998.325012      1  \n",
       "1            0.240968    0.095610    0.444555  1021.750000      1  \n",
       "2            0.258877    0.101119    0.451831  1043.839966      1  \n",
       "3            0.227197    0.100390    0.458361  1154.729980      1  \n",
       "4            0.244085    0.105134    0.436922  1013.380005      0  \n",
       "...               ...         ...         ...          ...    ...  \n",
       "1090         0.192902    0.093764    0.438650  7254.740000      1  \n",
       "1091         0.201651    0.098039    0.430993  7316.140000      1  \n",
       "1092         0.211699    0.094416    0.437290  7388.240000      1  \n",
       "1093         0.230646    0.098488    0.428165  7246.000000      0  \n",
       "1094         0.158129    0.088754    0.444779  7195.230000      0  \n",
       "\n",
       "[1095 rows x 10 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_mean = daily[['date', 'score_mean', \n",
    "       'v_neg_mean','v_neu_mean',\n",
    "       'v_pos_mean','v_compound_mean', 't_pol_mean', 't_sub_mean',\n",
    "        'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_std = daily[['date', 'score_std', \n",
    "       'v_neg_std','v_neu_std',\n",
    "       'v_pos_std','v_compound_std', 't_pol_std', 't_sub_std',\n",
    "        'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_max = daily[['date', 'score_max', \n",
    "       'v_neg_max','v_neu_max',\n",
    "       'v_pos_max','v_compound_max', 't_pol_max', 't_sub_max',\n",
    "       'Close'\n",
    "       ,'class']].copy()\n",
    "\n",
    "test_2_sum = daily[['date', 'score_sum', \n",
    "       'v_neg_sum','v_neu_sum',\n",
    "       'v_pos_sum','v_compound_sum', 't_pol_sum', 't_sub_sum',\n",
    "       'Close'\n",
    "       ,'class']].copy()\n",
    "test_2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_mean[['score_mean', \n",
    "       'v_neg_mean','v_neu_mean',\n",
    "       'v_pos_mean','v_compound_mean', 't_pol_mean', 't_sub_mean']]=test_2_mean[['score_mean', \n",
    "       'v_neg_mean','v_neu_mean',\n",
    "       'v_pos_mean','v_compound_mean', 't_pol_mean', 't_sub_mean',]].shift(periods=-1)\n",
    "test_2_mean = test_2_mean[0:len(test_2_mean)-1]\n",
    "\n",
    "test_2_std[['score_std', \n",
    "       'v_neg_std','v_neu_std',\n",
    "       'v_pos_std','v_compound_std', 't_pol_std', 't_sub_std']]=test_2_std[['score_std', \n",
    "       'v_neg_std','v_neu_std',\n",
    "       'v_pos_std','v_compound_std', 't_pol_std', 't_sub_std',]].shift(periods=-1)\n",
    "test_2_std =test_2_std[0:len(test_2_std)-1]\n",
    "\n",
    "test_2_max[['score_max', \n",
    "       'v_neg_max','v_neu_max',\n",
    "       'v_pos_max','v_compound_max', 't_pol_max', 't_sub_max']]=test_2_max[['score_max', \n",
    "       'v_neg_max','v_neu_max',\n",
    "       'v_pos_max','v_compound_max', 't_pol_max', 't_sub_max',]].shift(periods=-1)\n",
    "test_2_max = test_2_max[0:len(test_2_max)-1]\n",
    "\n",
    "test_2_sum[['score_sum', \n",
    "       'v_neg_sum','v_neu_sum',\n",
    "       'v_pos_sum','v_compound_sum', 't_pol_sum', 't_sub_sum']]=test_2_sum[['score_sum', \n",
    "       'v_neg_sum','v_neu_sum',\n",
    "       'v_pos_sum','v_compound_sum', 't_pol_sum', 't_sub_sum',]].shift(periods=-1)\n",
    "test_2_sum =test_2_sum[0:len(test_2_sum)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>Close</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.531</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1094 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_max  v_neg_max  v_neu_max  v_pos_max  v_compound_max  \\\n",
       "0     2017-01-01     1859.0      0.531        1.0      0.674          0.9997   \n",
       "1     2017-01-02      570.0      0.579        1.0      0.636          0.9987   \n",
       "2     2017-01-03      599.0      0.582        1.0      0.735          0.9998   \n",
       "3     2017-01-04      146.0      0.565        1.0      0.627          0.9984   \n",
       "4     2017-01-05     1755.0      0.508        1.0      0.560          0.9972   \n",
       "...          ...        ...        ...        ...        ...             ...   \n",
       "1089  2019-12-26      207.0      0.600        1.0      0.620          0.9979   \n",
       "1090  2019-12-27      281.0      0.461        1.0      0.482          0.9988   \n",
       "1091  2019-12-28      292.0      0.469        1.0      0.502          0.9979   \n",
       "1092  2019-12-29      522.0      0.753        1.0      0.592          0.9979   \n",
       "1093  2019-12-30      102.0      0.427        1.0      0.645          0.9980   \n",
       "\n",
       "      t_pol_max  t_sub_max        Close  class  \n",
       "0           1.0        1.0   998.325012      1  \n",
       "1           1.0        1.0  1021.750000      1  \n",
       "2           1.0        1.0  1043.839966      1  \n",
       "3           1.0        1.0  1154.729980      1  \n",
       "4           1.0        1.0  1013.380005      0  \n",
       "...         ...        ...          ...    ...  \n",
       "1089        1.0        1.0  7202.000000      0  \n",
       "1090        1.0        1.0  7254.740000      1  \n",
       "1091        1.0        1.0  7316.140000      1  \n",
       "1092        1.0        1.0  7388.240000      1  \n",
       "1093        1.0        1.0  7246.000000      0  \n",
       "\n",
       "[1094 rows x 10 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>Close</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>6.025253</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.827938</td>\n",
       "      <td>0.104296</td>\n",
       "      <td>0.207523</td>\n",
       "      <td>0.095999</td>\n",
       "      <td>0.443516</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1094 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  v_neg_mean  v_neu_mean  v_pos_mean  \\\n",
       "0     2017-01-01    7.143617    0.061524    0.829861    0.108612   \n",
       "1     2017-01-02    5.862605    0.060374    0.830037    0.109590   \n",
       "2     2017-01-03    4.271773    0.065616    0.824412    0.109981   \n",
       "3     2017-01-04    3.549738    0.060350    0.832056    0.107598   \n",
       "4     2017-01-05    6.025253    0.067773    0.827938    0.104296   \n",
       "...          ...         ...         ...         ...         ...   \n",
       "1089  2019-12-26    3.924113    0.065896    0.827884    0.106214   \n",
       "1090  2019-12-27    5.111582    0.062417    0.834208    0.103384   \n",
       "1091  2019-12-28    4.245443    0.063334    0.829598    0.107070   \n",
       "1092  2019-12-29    5.417252    0.061741    0.833728    0.104532   \n",
       "1093  2019-12-30    3.635659    0.072388    0.816814    0.110788   \n",
       "\n",
       "      v_compound_mean  t_pol_mean  t_sub_mean        Close  class  \n",
       "0            0.240968    0.095610    0.444555   998.325012      1  \n",
       "1            0.258877    0.101119    0.451831  1021.750000      1  \n",
       "2            0.227197    0.100390    0.458361  1043.839966      1  \n",
       "3            0.244085    0.105134    0.436922  1154.729980      1  \n",
       "4            0.207523    0.095999    0.443516  1013.380005      0  \n",
       "...               ...         ...         ...          ...    ...  \n",
       "1089         0.192902    0.093764    0.438650  7202.000000      0  \n",
       "1090         0.201651    0.098039    0.430993  7254.740000      1  \n",
       "1091         0.211699    0.094416    0.437290  7316.140000      1  \n",
       "1092         0.230646    0.098488    0.428165  7388.240000      1  \n",
       "1093         0.158129    0.088754    0.444779  7246.000000      0  \n",
       "\n",
       "[1094 rows x 10 columns]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2,X_train_std_2,X_test_std_2,y_train_std_2,y_test_std_2,\\\n",
    "X_train_max_2,X_test_max_2,y_train_max_2,y_test_max_2,X_train_sum_2,X_test_sum_2,y_train_sum_2,y_test_sum_2\\\n",
    "=create_all_data(test_2_mean,test_2_std,test_2_max,test_2_sum,1500,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = models.Sequential()\n",
    "    # model.add(normalizer)\n",
    "    neurons = 64\n",
    "    dropout = 0.0002\n",
    "    activ_func = \"linear\"\n",
    "\n",
    "    model.add(layers.LSTM(neurons,input_shape=(X_mean.shape[1],X_mean.shape[2] ), activation=\"tanh\"))\n",
    "    # model.add(layers.LSTM(neurons, activation=\"tanh\"))\n",
    "\n",
    "    # model.add(layers.LSTM(neurons, activation=\"tanh\"))\n",
    "\n",
    "    # model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # model.add(layers.LSTM(neurons, return_sequences=True, activation=activ_func))\n",
    "    # model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # model.add(layers.LSTM(neurons, return_sequences=False, activation=activ_func))\n",
    "    # model.add(layers.Dropout(dropout))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    # model.add(layers.Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_72_input'), name='lstm_72_input', description=\"created by layer 'lstm_72_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_72_input'), name='lstm_72_input', description=\"created by layer 'lstm_72_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "465/480 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.5215WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_72_input'), name='lstm_72_input', description=\"created by layer 'lstm_72_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "480/480 [==============================] - 2s 2ms/step - loss: 0.6942 - accuracy: 0.5177 - val_loss: 0.6951 - val_accuracy: 0.4167\n",
      "Epoch 2/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5125 - val_loss: 0.6865 - val_accuracy: 0.5833\n",
      "Epoch 3/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5229 - val_loss: 0.6882 - val_accuracy: 0.5833\n",
      "Epoch 4/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5229 - val_loss: 0.6864 - val_accuracy: 0.5833\n",
      "Epoch 5/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5188 - val_loss: 0.6862 - val_accuracy: 0.5833\n",
      "Epoch 6/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5229 - val_loss: 0.6887 - val_accuracy: 0.5833\n",
      "Epoch 7/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5146 - val_loss: 0.6898 - val_accuracy: 0.5833\n",
      "Epoch 8/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5021 - val_loss: 0.6820 - val_accuracy: 0.5833\n",
      "Epoch 9/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.5208 - val_loss: 0.6849 - val_accuracy: 0.5833\n",
      "Epoch 10/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5125 - val_loss: 0.6884 - val_accuracy: 0.5833\n",
      "Epoch 11/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5188 - val_loss: 0.6819 - val_accuracy: 0.5833\n",
      "Epoch 12/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5229 - val_loss: 0.6867 - val_accuracy: 0.5833\n",
      "Epoch 13/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5021 - val_loss: 0.6807 - val_accuracy: 0.5833\n",
      "Epoch 14/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5146 - val_loss: 0.6882 - val_accuracy: 0.5833\n",
      "Epoch 15/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5083 - val_loss: 0.6909 - val_accuracy: 0.5833\n",
      "Epoch 16/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5229 - val_loss: 0.6864 - val_accuracy: 0.5833\n",
      "Epoch 17/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.5229 - val_loss: 0.6947 - val_accuracy: 0.4167\n",
      "Epoch 18/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5188 - val_loss: 0.6900 - val_accuracy: 0.5833\n",
      "Epoch 19/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5188 - val_loss: 0.6890 - val_accuracy: 0.5833\n",
      "Epoch 20/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5125 - val_loss: 0.6874 - val_accuracy: 0.5833\n",
      "Epoch 21/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5104 - val_loss: 0.6831 - val_accuracy: 0.5833\n",
      "Epoch 22/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5208 - val_loss: 0.6955 - val_accuracy: 0.4167\n",
      "Epoch 23/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.5292 - val_loss: 0.6871 - val_accuracy: 0.5833\n",
      "Epoch 24/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.5229 - val_loss: 0.6889 - val_accuracy: 0.5833\n",
      "Epoch 25/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.5292 - val_loss: 0.6997 - val_accuracy: 0.4167\n",
      "Epoch 26/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6843 - val_accuracy: 0.5833\n",
      "Epoch 27/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.5104 - val_loss: 0.6838 - val_accuracy: 0.5833\n",
      "Epoch 28/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5229 - val_loss: 0.6908 - val_accuracy: 0.5833\n",
      "Epoch 29/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6929 - accuracy: 0.5208 - val_loss: 0.6927 - val_accuracy: 0.5833\n",
      "Epoch 30/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6931 - accuracy: 0.5042 - val_loss: 0.6883 - val_accuracy: 0.5833\n",
      "Epoch 31/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6928 - accuracy: 0.5271 - val_loss: 0.6806 - val_accuracy: 0.5833\n",
      "Epoch 32/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5146 - val_loss: 0.6854 - val_accuracy: 0.5833\n",
      "Epoch 33/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.5188 - val_loss: 0.6875 - val_accuracy: 0.5833\n",
      "Epoch 34/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6932 - accuracy: 0.5083 - val_loss: 0.6945 - val_accuracy: 0.4167\n",
      "Epoch 35/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5063 - val_loss: 0.6879 - val_accuracy: 0.5833\n",
      "Epoch 36/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5125 - val_loss: 0.6859 - val_accuracy: 0.5833\n",
      "Epoch 37/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6932 - accuracy: 0.5167 - val_loss: 0.6819 - val_accuracy: 0.5833\n",
      "Epoch 38/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5188 - val_loss: 0.6839 - val_accuracy: 0.5833\n",
      "Epoch 39/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.5021 - val_loss: 0.6834 - val_accuracy: 0.5833\n",
      "Epoch 40/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5229 - val_loss: 0.6837 - val_accuracy: 0.5833\n",
      "Epoch 41/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6931 - accuracy: 0.5083 - val_loss: 0.6820 - val_accuracy: 0.5833\n",
      "Epoch 42/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5229 - val_loss: 0.6841 - val_accuracy: 0.5833\n",
      "Epoch 43/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5188 - val_loss: 0.6822 - val_accuracy: 0.5833\n",
      "Epoch 44/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.5292 - val_loss: 0.7000 - val_accuracy: 0.4167\n",
      "Epoch 45/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6920 - accuracy: 0.5167 - val_loss: 0.6793 - val_accuracy: 0.5833\n",
      "Epoch 46/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5167 - val_loss: 0.6945 - val_accuracy: 0.4167\n",
      "Epoch 47/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.4792 - val_loss: 0.6849 - val_accuracy: 0.5833\n",
      "Epoch 48/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5125 - val_loss: 0.6878 - val_accuracy: 0.5833\n",
      "Epoch 49/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5146 - val_loss: 0.6853 - val_accuracy: 0.5833\n",
      "Epoch 50/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5229 - val_loss: 0.6913 - val_accuracy: 0.5833\n",
      "Epoch 51/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5208 - val_loss: 0.6845 - val_accuracy: 0.5833\n",
      "Epoch 52/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.4979 - val_loss: 0.6867 - val_accuracy: 0.5833\n",
      "Epoch 53/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.5188 - val_loss: 0.6820 - val_accuracy: 0.5833\n",
      "Epoch 54/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6948 - accuracy: 0.5063 - val_loss: 0.6855 - val_accuracy: 0.5833\n",
      "Epoch 55/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4854 - val_loss: 0.6798 - val_accuracy: 0.5833\n",
      "Epoch 56/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5167 - val_loss: 0.6851 - val_accuracy: 0.5833\n",
      "Epoch 57/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5167 - val_loss: 0.6871 - val_accuracy: 0.5833\n",
      "Epoch 58/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6924 - accuracy: 0.5063 - val_loss: 0.6808 - val_accuracy: 0.5833\n",
      "Epoch 59/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6931 - accuracy: 0.5250 - val_loss: 0.6945 - val_accuracy: 0.4167\n",
      "Epoch 60/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.5083 - val_loss: 0.6917 - val_accuracy: 0.5833\n",
      "Epoch 61/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.4938 - val_loss: 0.6860 - val_accuracy: 0.5833\n",
      "Epoch 62/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.5229 - val_loss: 0.6825 - val_accuracy: 0.5833\n",
      "Epoch 63/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5083 - val_loss: 0.6831 - val_accuracy: 0.5833\n",
      "Epoch 64/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.5250 - val_loss: 0.7001 - val_accuracy: 0.4167\n",
      "Epoch 65/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5208 - val_loss: 0.6967 - val_accuracy: 0.4167\n",
      "Epoch 66/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6947 - accuracy: 0.4979 - val_loss: 0.6887 - val_accuracy: 0.5833\n",
      "Epoch 67/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5208 - val_loss: 0.6821 - val_accuracy: 0.5833\n",
      "Epoch 68/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.5083 - val_loss: 0.6934 - val_accuracy: 0.4167\n",
      "Epoch 69/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5146 - val_loss: 0.6905 - val_accuracy: 0.5833\n",
      "Epoch 70/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.5104 - val_loss: 0.6819 - val_accuracy: 0.5833\n",
      "Epoch 71/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6830 - val_accuracy: 0.5833\n",
      "Epoch 72/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5167 - val_loss: 0.6908 - val_accuracy: 0.5833\n",
      "Epoch 73/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5188 - val_loss: 0.6862 - val_accuracy: 0.5833\n",
      "Epoch 74/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5229 - val_loss: 0.6904 - val_accuracy: 0.5833\n",
      "Epoch 75/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5146 - val_loss: 0.6857 - val_accuracy: 0.5833\n",
      "Epoch 76/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5083 - val_loss: 0.6863 - val_accuracy: 0.5833\n",
      "Epoch 77/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.5167 - val_loss: 0.6847 - val_accuracy: 0.5833\n",
      "Epoch 78/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.5167 - val_loss: 0.6894 - val_accuracy: 0.5833\n",
      "Epoch 79/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5063 - val_loss: 0.6903 - val_accuracy: 0.5833\n",
      "Epoch 80/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5063 - val_loss: 0.6833 - val_accuracy: 0.5833\n",
      "Epoch 81/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6929 - accuracy: 0.5083 - val_loss: 0.6956 - val_accuracy: 0.4167\n",
      "Epoch 82/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.5021 - val_loss: 0.6867 - val_accuracy: 0.5833\n",
      "Epoch 83/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.5104 - val_loss: 0.6871 - val_accuracy: 0.5833\n",
      "Epoch 84/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.5083 - val_loss: 0.6806 - val_accuracy: 0.5833\n",
      "Epoch 85/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.5229 - val_loss: 0.6864 - val_accuracy: 0.5833\n",
      "Epoch 86/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.5104 - val_loss: 0.6859 - val_accuracy: 0.5833\n",
      "Epoch 87/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6936 - accuracy: 0.5125 - val_loss: 0.6840 - val_accuracy: 0.5833\n",
      "Epoch 88/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5125 - val_loss: 0.6840 - val_accuracy: 0.5833\n",
      "Epoch 89/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6932 - accuracy: 0.5229 - val_loss: 0.6826 - val_accuracy: 0.5833\n",
      "Epoch 90/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6831 - val_accuracy: 0.5833\n",
      "Epoch 91/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.4979 - val_loss: 0.6808 - val_accuracy: 0.5833\n",
      "Epoch 92/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6946 - accuracy: 0.5229 - val_loss: 0.6852 - val_accuracy: 0.5833\n",
      "Epoch 93/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5208 - val_loss: 0.6987 - val_accuracy: 0.4167\n",
      "Epoch 94/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6829 - val_accuracy: 0.5833\n",
      "Epoch 95/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6922 - accuracy: 0.5333 - val_loss: 0.7028 - val_accuracy: 0.4167\n",
      "Epoch 96/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4167\n",
      "Epoch 97/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5083 - val_loss: 0.6899 - val_accuracy: 0.5833\n",
      "Epoch 98/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6943 - accuracy: 0.5042 - val_loss: 0.6889 - val_accuracy: 0.5833\n",
      "Epoch 99/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5188 - val_loss: 0.6853 - val_accuracy: 0.5833\n",
      "Epoch 100/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6930 - accuracy: 0.5167 - val_loss: 0.6903 - val_accuracy: 0.5833\n",
      "Epoch 101/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5167 - val_loss: 0.6864 - val_accuracy: 0.5833\n",
      "Epoch 102/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5146 - val_loss: 0.6861 - val_accuracy: 0.5833\n",
      "Epoch 103/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6939 - accuracy: 0.5146 - val_loss: 0.6869 - val_accuracy: 0.5833\n",
      "Epoch 104/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6944 - accuracy: 0.5188 - val_loss: 0.6832 - val_accuracy: 0.5833\n",
      "Epoch 105/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6942 - accuracy: 0.5146 - val_loss: 0.6864 - val_accuracy: 0.5833\n",
      "Epoch 106/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5125 - val_loss: 0.6932 - val_accuracy: 0.4167\n",
      "Epoch 107/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5146 - val_loss: 0.6861 - val_accuracy: 0.5833\n",
      "Epoch 108/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5146 - val_loss: 0.6851 - val_accuracy: 0.5833\n",
      "Epoch 109/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6940 - accuracy: 0.5125 - val_loss: 0.6871 - val_accuracy: 0.5833\n",
      "Epoch 110/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5146 - val_loss: 0.6823 - val_accuracy: 0.5833\n",
      "Epoch 111/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5042 - val_loss: 0.6842 - val_accuracy: 0.5833\n",
      "Epoch 112/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5167 - val_loss: 0.6865 - val_accuracy: 0.5833\n",
      "Epoch 113/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5229 - val_loss: 0.6841 - val_accuracy: 0.5833\n",
      "Epoch 114/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.5208 - val_loss: 0.6953 - val_accuracy: 0.4167\n",
      "Epoch 115/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6930 - accuracy: 0.5208 - val_loss: 0.6819 - val_accuracy: 0.5833\n",
      "Epoch 116/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5104 - val_loss: 0.6850 - val_accuracy: 0.5833\n",
      "Epoch 117/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5229 - val_loss: 0.6851 - val_accuracy: 0.5833\n",
      "Epoch 118/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.5146 - val_loss: 0.6879 - val_accuracy: 0.5833\n",
      "Epoch 119/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6940 - accuracy: 0.5125 - val_loss: 0.6847 - val_accuracy: 0.5833\n",
      "Epoch 120/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.5021 - val_loss: 0.6905 - val_accuracy: 0.5833\n",
      "Epoch 121/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5083 - val_loss: 0.6829 - val_accuracy: 0.5833\n",
      "Epoch 122/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.5104 - val_loss: 0.6805 - val_accuracy: 0.5833\n",
      "Epoch 123/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6937 - accuracy: 0.5125 - val_loss: 0.6887 - val_accuracy: 0.5833\n",
      "Epoch 124/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6938 - accuracy: 0.5146 - val_loss: 0.6851 - val_accuracy: 0.5833\n",
      "Epoch 125/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6932 - accuracy: 0.5188 - val_loss: 0.6905 - val_accuracy: 0.5833\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_mean_4 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_mean_4 = model_mean_4.fit(X_train_2, y_train_2, \n",
    "          epochs=500, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=[es],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_73_input'), name='lstm_73_input', description=\"created by layer 'lstm_73_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_73_input'), name='lstm_73_input', description=\"created by layer 'lstm_73_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "210/210 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.5262WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_73_input'), name='lstm_73_input', description=\"created by layer 'lstm_73_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "210/210 [==============================] - 1s 2ms/step - loss: 0.7277 - accuracy: 0.5262 - val_loss: 0.6983 - val_accuracy: 0.4972\n",
      "Epoch 2/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 3/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5298 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 4/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5286 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 5/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5286 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 6/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5298 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 7/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5036 - val_loss: 0.6969 - val_accuracy: 0.4972\n",
      "Epoch 8/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5274 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 9/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5274 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 10/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5274 - val_loss: 0.6949 - val_accuracy: 0.4972\n",
      "Epoch 11/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5274 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 12/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 13/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5298 - val_loss: 0.6949 - val_accuracy: 0.4972\n",
      "Epoch 14/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5274 - val_loss: 0.6945 - val_accuracy: 0.4972\n",
      "Epoch 15/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5286 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 16/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5214 - val_loss: 0.6970 - val_accuracy: 0.4972\n",
      "Epoch 17/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5274 - val_loss: 0.6943 - val_accuracy: 0.4972\n",
      "Epoch 18/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5274 - val_loss: 0.6957 - val_accuracy: 0.4972\n",
      "Epoch 19/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5274 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 20/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5274 - val_loss: 0.6960 - val_accuracy: 0.4972\n",
      "Epoch 21/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5179 - val_loss: 0.6935 - val_accuracy: 0.4972\n",
      "Epoch 22/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5286 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 23/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5286 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 24/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5286 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 25/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5167 - val_loss: 0.6963 - val_accuracy: 0.4972\n",
      "Epoch 26/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5286 - val_loss: 0.6934 - val_accuracy: 0.4972\n",
      "Epoch 27/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5071 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 28/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5202 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 29/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6910 - accuracy: 0.5298 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 30/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5107 - val_loss: 0.6958 - val_accuracy: 0.4972\n",
      "Epoch 31/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5143 - val_loss: 0.6961 - val_accuracy: 0.4972\n",
      "Epoch 32/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5298 - val_loss: 0.6954 - val_accuracy: 0.4972\n",
      "Epoch 33/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5298 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 34/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5143 - val_loss: 0.6980 - val_accuracy: 0.4972\n",
      "Epoch 35/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.5095 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 36/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5048 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 37/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6909 - accuracy: 0.5071 - val_loss: 0.6958 - val_accuracy: 0.4972\n",
      "Epoch 38/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5298 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 39/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5286 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 40/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5298 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 41/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5298 - val_loss: 0.6963 - val_accuracy: 0.4972\n",
      "Epoch 42/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5286 - val_loss: 0.6965 - val_accuracy: 0.4972\n",
      "Epoch 43/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5274 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 44/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5274 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 45/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5274 - val_loss: 0.6949 - val_accuracy: 0.4972\n",
      "Epoch 46/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5286 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 47/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.7238 - accuracy: 0.4774 - val_loss: 0.6932 - val_accuracy: 0.4972\n",
      "Epoch 48/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.5274 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 49/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5274 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 50/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5167 - val_loss: 0.6961 - val_accuracy: 0.4972\n",
      "Epoch 51/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5274 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 52/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5226 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 53/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5274 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 54/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5274 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 55/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5274 - val_loss: 0.6965 - val_accuracy: 0.4972\n",
      "Epoch 56/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5274 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 57/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6953 - val_accuracy: 0.4972\n",
      "Epoch 58/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5274 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 59/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5274 - val_loss: 0.6932 - val_accuracy: 0.4972\n",
      "Epoch 60/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5274 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 61/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5274 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 62/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5274 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 63/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5274 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 64/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5274 - val_loss: 0.6958 - val_accuracy: 0.4972\n",
      "Epoch 65/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5274 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 66/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5274 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 67/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5274 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 68/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5274 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 69/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5095 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 70/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5274 - val_loss: 0.6957 - val_accuracy: 0.4972\n",
      "Epoch 71/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5274 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 72/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5274 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 73/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5143 - val_loss: 0.6961 - val_accuracy: 0.4972\n",
      "Epoch 74/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5262 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 75/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5298 - val_loss: 0.6960 - val_accuracy: 0.4972\n",
      "Epoch 76/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5298 - val_loss: 0.6964 - val_accuracy: 0.4972\n",
      "Epoch 77/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.5143 - val_loss: 0.6975 - val_accuracy: 0.4972\n",
      "Epoch 78/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5298 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 79/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5083 - val_loss: 0.6964 - val_accuracy: 0.4972\n",
      "Epoch 80/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5298 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 81/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6910 - accuracy: 0.5298 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 82/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5298 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 83/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5298 - val_loss: 0.6968 - val_accuracy: 0.4972\n",
      "Epoch 84/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5298 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 85/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5298 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 86/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5298 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 87/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5298 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 88/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5143 - val_loss: 0.6956 - val_accuracy: 0.4972\n",
      "Epoch 89/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6895 - accuracy: 0.5238 - val_loss: 0.6932 - val_accuracy: 0.4972\n",
      "Epoch 90/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5155 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 91/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5298 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 92/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5298 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 93/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.5298 - val_loss: 0.6954 - val_accuracy: 0.4972\n",
      "Epoch 94/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5298 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 95/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5298 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 96/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5298 - val_loss: 0.6943 - val_accuracy: 0.4972\n",
      "Epoch 97/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.5298 - val_loss: 0.6949 - val_accuracy: 0.4972\n",
      "Epoch 98/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5298 - val_loss: 0.6955 - val_accuracy: 0.4972\n",
      "Epoch 99/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.7637 - accuracy: 0.4619 - val_loss: 0.6936 - val_accuracy: 0.5028\n",
      "Epoch 100/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.4952 - val_loss: 0.6945 - val_accuracy: 0.4972\n",
      "Epoch 101/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6937 - accuracy: 0.5238 - val_loss: 0.6942 - val_accuracy: 0.4972\n",
      "Epoch 102/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 103/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6933 - val_accuracy: 0.4972\n",
      "Epoch 104/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 105/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6951 - val_accuracy: 0.4972\n",
      "Epoch 106/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 107/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 108/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6942 - val_accuracy: 0.4972\n",
      "Epoch 109/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 110/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5048 - val_loss: 0.6978 - val_accuracy: 0.4972\n",
      "Epoch 111/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6939 - accuracy: 0.5238 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 112/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6961 - val_accuracy: 0.4972\n",
      "Epoch 113/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 114/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5238 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 115/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5048 - val_loss: 0.6954 - val_accuracy: 0.4972\n",
      "Epoch 116/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 117/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 118/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 119/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 120/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5238 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 121/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5095 - val_loss: 0.6954 - val_accuracy: 0.4972\n",
      "Epoch 122/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5238 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 123/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5238 - val_loss: 0.6943 - val_accuracy: 0.4972\n",
      "Epoch 124/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5048 - val_loss: 0.6942 - val_accuracy: 0.4972\n",
      "Epoch 125/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 126/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5238 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 127/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 128/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5071 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 129/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5238 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 130/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6945 - val_accuracy: 0.4972\n",
      "Epoch 131/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5238 - val_loss: 0.6958 - val_accuracy: 0.4972\n",
      "Epoch 132/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6960 - val_accuracy: 0.4972\n",
      "Epoch 133/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 134/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 0.6967 - val_accuracy: 0.4972\n",
      "Epoch 135/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5238 - val_loss: 0.6950 - val_accuracy: 0.4972\n",
      "Epoch 136/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5238 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 137/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 138/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 139/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5167 - val_loss: 0.6958 - val_accuracy: 0.4972\n",
      "Epoch 140/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 141/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6938 - accuracy: 0.5238 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 142/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5238 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 143/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5238 - val_loss: 0.6943 - val_accuracy: 0.4972\n",
      "Epoch 144/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 145/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5048 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 146/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 147/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 148/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5107 - val_loss: 0.6979 - val_accuracy: 0.4972\n",
      "Epoch 149/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6959 - val_accuracy: 0.4972\n",
      "Epoch 150/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5238 - val_loss: 0.6957 - val_accuracy: 0.4972\n",
      "Epoch 151/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5238 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 152/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5238 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 153/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5238 - val_loss: 0.6932 - val_accuracy: 0.4972\n",
      "Epoch 154/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6937 - val_accuracy: 0.4972\n",
      "Epoch 155/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5238 - val_loss: 0.6947 - val_accuracy: 0.4972\n",
      "Epoch 156/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.4972\n",
      "Epoch 157/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 0.6949 - val_accuracy: 0.4972\n",
      "Epoch 158/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5238 - val_loss: 0.6939 - val_accuracy: 0.4972\n",
      "Epoch 159/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5238 - val_loss: 0.6955 - val_accuracy: 0.4972\n",
      "Epoch 160/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5143 - val_loss: 0.6981 - val_accuracy: 0.4972\n",
      "Epoch 161/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5238 - val_loss: 0.6944 - val_accuracy: 0.4972\n",
      "Epoch 162/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6946 - val_accuracy: 0.4972\n",
      "Epoch 163/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.5238 - val_loss: 0.6940 - val_accuracy: 0.4972\n",
      "Epoch 164/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5238 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 165/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5119 - val_loss: 0.6948 - val_accuracy: 0.4972\n",
      "Epoch 166/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5238 - val_loss: 0.6943 - val_accuracy: 0.4972\n",
      "Epoch 167/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5238 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
      "Epoch 168/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5238 - val_loss: 0.6936 - val_accuracy: 0.4972\n",
      "Epoch 169/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5238 - val_loss: 0.6942 - val_accuracy: 0.4972\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_std_4 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_std_4 = model_std_4.fit(X_train_std_2, y_train_std_2, \n",
    "          epochs=500, \n",
    "          batch_size=4, \n",
    "          verbose=1, \n",
    "          callbacks=[es],\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_74_input'), name='lstm_74_input', description=\"created by layer 'lstm_74_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_74_input'), name='lstm_74_input', description=\"created by layer 'lstm_74_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "  3/480 [..............................] - ETA: 51s - loss: 0.6978 - accuracy: 0.5000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 22:17:40.073581: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-06-08 22:17:40.073599: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-06-08 22:17:40.251799: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-06-08 22:17:40.253543: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-06-08 22:17:40.255027: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40\n",
      "2022-06-08 22:17:40.256508: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.trace.json.gz\n",
      "2022-06-08 22:17:40.258549: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40\n",
      "2022-06-08 22:17:40.258767: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.memory_profile.json.gz\n",
      "2022-06-08 22:17:40.259421: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40Dumped tool data for xplane.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/fit9/20220608-173013/train/plugins/profile/2022_06_08_22_17_40/MacBook-Air-de-christopher.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/480 [===========================>..] - ETA: 0s - loss: 0.7025 - accuracy: 0.5327WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_74_input'), name='lstm_74_input', description=\"created by layer 'lstm_74_input'\"), but it was called on an input with incompatible shape (2, 3, 9).\n",
      "480/480 [==============================] - 2s 3ms/step - loss: 0.7058 - accuracy: 0.5250 - val_loss: 0.6826 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6839 - accuracy: 0.5750 - val_loss: 0.6746 - val_accuracy: 0.6042\n",
      "Epoch 3/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6783 - accuracy: 0.5740 - val_loss: 0.6724 - val_accuracy: 0.6042\n",
      "Epoch 4/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.5656 - val_loss: 0.6786 - val_accuracy: 0.6042\n",
      "Epoch 5/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6826 - accuracy: 0.5667 - val_loss: 0.6764 - val_accuracy: 0.6000\n",
      "Epoch 6/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6843 - accuracy: 0.5719 - val_loss: 0.6783 - val_accuracy: 0.6000\n",
      "Epoch 7/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6821 - accuracy: 0.5688 - val_loss: 0.6727 - val_accuracy: 0.6000\n",
      "Epoch 8/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6799 - accuracy: 0.5729 - val_loss: 0.6794 - val_accuracy: 0.6000\n",
      "Epoch 9/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.5688 - val_loss: 0.6979 - val_accuracy: 0.4333\n",
      "Epoch 10/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6808 - accuracy: 0.5740 - val_loss: 0.6748 - val_accuracy: 0.6000\n",
      "Epoch 11/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.5688 - val_loss: 0.6750 - val_accuracy: 0.6000\n",
      "Epoch 12/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6784 - accuracy: 0.5677 - val_loss: 0.6773 - val_accuracy: 0.6000\n",
      "Epoch 13/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6800 - accuracy: 0.5688 - val_loss: 0.6816 - val_accuracy: 0.6000\n",
      "Epoch 14/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6801 - accuracy: 0.5625 - val_loss: 0.6772 - val_accuracy: 0.6000\n",
      "Epoch 15/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6803 - accuracy: 0.5698 - val_loss: 0.6784 - val_accuracy: 0.5958\n",
      "Epoch 16/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6796 - accuracy: 0.5708 - val_loss: 0.6869 - val_accuracy: 0.5917\n",
      "Epoch 17/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6807 - accuracy: 0.5656 - val_loss: 0.6841 - val_accuracy: 0.5958\n",
      "Epoch 18/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.5688 - val_loss: 0.6817 - val_accuracy: 0.5958\n",
      "Epoch 19/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.5677 - val_loss: 0.6827 - val_accuracy: 0.5958\n",
      "Epoch 20/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6798 - accuracy: 0.5688 - val_loss: 0.6796 - val_accuracy: 0.5958\n",
      "Epoch 21/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6801 - accuracy: 0.5719 - val_loss: 0.6813 - val_accuracy: 0.5958\n",
      "Epoch 22/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6792 - accuracy: 0.5615 - val_loss: 0.6804 - val_accuracy: 0.5958\n",
      "Epoch 23/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6800 - accuracy: 0.5688 - val_loss: 0.6864 - val_accuracy: 0.6000\n",
      "Epoch 24/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6784 - accuracy: 0.5698 - val_loss: 0.6832 - val_accuracy: 0.6000\n",
      "Epoch 25/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6787 - accuracy: 0.5708 - val_loss: 0.6890 - val_accuracy: 0.6000\n",
      "Epoch 26/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6786 - accuracy: 0.5708 - val_loss: 0.6846 - val_accuracy: 0.5958\n",
      "Epoch 27/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6785 - accuracy: 0.5656 - val_loss: 0.6807 - val_accuracy: 0.6000\n",
      "Epoch 28/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6784 - accuracy: 0.5719 - val_loss: 0.6831 - val_accuracy: 0.5958\n",
      "Epoch 29/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6784 - accuracy: 0.5688 - val_loss: 0.6791 - val_accuracy: 0.5917\n",
      "Epoch 30/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6803 - accuracy: 0.5708 - val_loss: 0.6830 - val_accuracy: 0.6000\n",
      "Epoch 31/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6787 - accuracy: 0.5708 - val_loss: 0.6815 - val_accuracy: 0.5958\n",
      "Epoch 32/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6790 - accuracy: 0.5729 - val_loss: 0.6809 - val_accuracy: 0.5958\n",
      "Epoch 33/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6780 - accuracy: 0.5729 - val_loss: 0.6878 - val_accuracy: 0.5917\n",
      "Epoch 34/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6772 - accuracy: 0.5708 - val_loss: 0.6842 - val_accuracy: 0.5958\n",
      "Epoch 35/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5719 - val_loss: 0.6840 - val_accuracy: 0.5958\n",
      "Epoch 36/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6774 - accuracy: 0.5729 - val_loss: 0.6825 - val_accuracy: 0.5958\n",
      "Epoch 37/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6763 - accuracy: 0.5708 - val_loss: 0.6916 - val_accuracy: 0.5875\n",
      "Epoch 38/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6765 - accuracy: 0.5719 - val_loss: 0.6965 - val_accuracy: 0.5875\n",
      "Epoch 39/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.5719 - val_loss: 0.6869 - val_accuracy: 0.5917\n",
      "Epoch 40/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6777 - accuracy: 0.5729 - val_loss: 0.6935 - val_accuracy: 0.5875\n",
      "Epoch 41/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6782 - accuracy: 0.5656 - val_loss: 0.6864 - val_accuracy: 0.5958\n",
      "Epoch 42/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6776 - accuracy: 0.5750 - val_loss: 0.6897 - val_accuracy: 0.5958\n",
      "Epoch 43/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.5719 - val_loss: 0.6856 - val_accuracy: 0.5958\n",
      "Epoch 44/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6771 - accuracy: 0.5719 - val_loss: 0.6822 - val_accuracy: 0.5958\n",
      "Epoch 45/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.5719 - val_loss: 0.6831 - val_accuracy: 0.5958\n",
      "Epoch 46/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6760 - accuracy: 0.5760 - val_loss: 0.6906 - val_accuracy: 0.5917\n",
      "Epoch 47/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6755 - accuracy: 0.5740 - val_loss: 0.6846 - val_accuracy: 0.5958\n",
      "Epoch 48/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6747 - accuracy: 0.5729 - val_loss: 0.7026 - val_accuracy: 0.5958\n",
      "Epoch 49/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5740 - val_loss: 0.6871 - val_accuracy: 0.5958\n",
      "Epoch 50/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6770 - accuracy: 0.5729 - val_loss: 0.6937 - val_accuracy: 0.5875\n",
      "Epoch 51/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.5635 - val_loss: 0.6911 - val_accuracy: 0.5875\n",
      "Epoch 52/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6756 - accuracy: 0.5719 - val_loss: 0.6866 - val_accuracy: 0.5958\n",
      "Epoch 53/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6752 - accuracy: 0.5594 - val_loss: 0.6857 - val_accuracy: 0.5958\n",
      "Epoch 54/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6770 - accuracy: 0.5740 - val_loss: 0.6915 - val_accuracy: 0.5875\n",
      "Epoch 55/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6760 - accuracy: 0.5740 - val_loss: 0.6978 - val_accuracy: 0.5875\n",
      "Epoch 56/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6768 - accuracy: 0.5729 - val_loss: 0.6938 - val_accuracy: 0.5875\n",
      "Epoch 57/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6770 - accuracy: 0.5740 - val_loss: 0.6895 - val_accuracy: 0.5958\n",
      "Epoch 58/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6776 - accuracy: 0.5740 - val_loss: 0.6887 - val_accuracy: 0.5958\n",
      "Epoch 59/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6740 - accuracy: 0.5708 - val_loss: 0.6905 - val_accuracy: 0.5917\n",
      "Epoch 60/500\n",
      "480/480 [==============================] - 1s 2ms/step - loss: 0.6771 - accuracy: 0.5740 - val_loss: 0.6903 - val_accuracy: 0.5958\n",
      "Epoch 61/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6777 - accuracy: 0.5708 - val_loss: 0.6909 - val_accuracy: 0.5958\n",
      "Epoch 62/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6795 - accuracy: 0.5708 - val_loss: 0.6864 - val_accuracy: 0.5917\n",
      "Epoch 63/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6776 - accuracy: 0.5708 - val_loss: 0.6979 - val_accuracy: 0.6000\n",
      "Epoch 64/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6770 - accuracy: 0.5698 - val_loss: 0.6893 - val_accuracy: 0.6000\n",
      "Epoch 65/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6773 - accuracy: 0.5729 - val_loss: 0.6872 - val_accuracy: 0.5958\n",
      "Epoch 66/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6774 - accuracy: 0.5729 - val_loss: 0.6894 - val_accuracy: 0.5958\n",
      "Epoch 67/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5729 - val_loss: 0.6909 - val_accuracy: 0.5875\n",
      "Epoch 68/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6769 - accuracy: 0.5729 - val_loss: 0.6875 - val_accuracy: 0.5958\n",
      "Epoch 69/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6782 - accuracy: 0.5698 - val_loss: 0.6897 - val_accuracy: 0.5958\n",
      "Epoch 70/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6774 - accuracy: 0.5708 - val_loss: 0.6894 - val_accuracy: 0.5958\n",
      "Epoch 71/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.5740 - val_loss: 0.6950 - val_accuracy: 0.5875\n",
      "Epoch 72/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6772 - accuracy: 0.5708 - val_loss: 0.6887 - val_accuracy: 0.5958\n",
      "Epoch 73/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6768 - accuracy: 0.5750 - val_loss: 0.6964 - val_accuracy: 0.5875\n",
      "Epoch 74/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6762 - accuracy: 0.5740 - val_loss: 0.6927 - val_accuracy: 0.5875\n",
      "Epoch 75/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6767 - accuracy: 0.5740 - val_loss: 0.6902 - val_accuracy: 0.5958\n",
      "Epoch 76/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6778 - accuracy: 0.5729 - val_loss: 0.6922 - val_accuracy: 0.5958\n",
      "Epoch 77/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6763 - accuracy: 0.5740 - val_loss: 0.6968 - val_accuracy: 0.5875\n",
      "Epoch 78/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6771 - accuracy: 0.5740 - val_loss: 0.6975 - val_accuracy: 0.5875\n",
      "Epoch 79/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6773 - accuracy: 0.5667 - val_loss: 0.6961 - val_accuracy: 0.5958\n",
      "Epoch 80/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6728 - accuracy: 0.5771 - val_loss: 0.7111 - val_accuracy: 0.5875\n",
      "Epoch 81/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6775 - accuracy: 0.5781 - val_loss: 0.6942 - val_accuracy: 0.5958\n",
      "Epoch 82/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6762 - accuracy: 0.5771 - val_loss: 0.6931 - val_accuracy: 0.5958\n",
      "Epoch 83/500\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 0.6764 - accuracy: 0.5771 - val_loss: 0.6937 - val_accuracy: 0.5958\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_max_4 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_max_4 = model_max_4.fit(X_train_max_2, y_train_max_2, \n",
    "          epochs=500, \n",
    "          batch_size=2, \n",
    "          verbose=1, \n",
    "          callbacks=[es,tensorboard_callback],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_75_input'), name='lstm_75_input', description=\"created by layer 'lstm_75_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_75_input'), name='lstm_75_input', description=\"created by layer 'lstm_75_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "203/210 [============================>.] - ETA: 0s - loss: 0.7161 - accuracy: 0.5000WARNING:tensorflow:Model was constructed with shape (None, 5, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 9), dtype=tf.float32, name='lstm_75_input'), name='lstm_75_input', description=\"created by layer 'lstm_75_input'\"), but it was called on an input with incompatible shape (4, 3, 9).\n",
      "210/210 [==============================] - 2s 3ms/step - loss: 0.7150 - accuracy: 0.5024 - val_loss: 0.7074 - val_accuracy: 0.4806\n",
      "Epoch 2/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6984 - accuracy: 0.5107 - val_loss: 0.6936 - val_accuracy: 0.5111\n",
      "Epoch 3/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6967 - accuracy: 0.5310 - val_loss: 0.6905 - val_accuracy: 0.5222\n",
      "Epoch 4/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6949 - accuracy: 0.5440 - val_loss: 0.6934 - val_accuracy: 0.5111\n",
      "Epoch 5/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5369 - val_loss: 0.6947 - val_accuracy: 0.5167\n",
      "Epoch 6/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.5381 - val_loss: 0.6978 - val_accuracy: 0.4861\n",
      "Epoch 7/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.5036 - val_loss: 0.6946 - val_accuracy: 0.5278\n",
      "Epoch 8/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6909 - accuracy: 0.5202 - val_loss: 0.6878 - val_accuracy: 0.5278\n",
      "Epoch 9/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5345 - val_loss: 0.6862 - val_accuracy: 0.5417\n",
      "Epoch 10/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.4845 - val_loss: 0.6875 - val_accuracy: 0.5333\n",
      "Epoch 11/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5214 - val_loss: 0.6938 - val_accuracy: 0.5417\n",
      "Epoch 12/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5357 - val_loss: 0.6894 - val_accuracy: 0.5444\n",
      "Epoch 13/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.5571 - val_loss: 0.6893 - val_accuracy: 0.5306\n",
      "Epoch 14/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5583 - val_loss: 0.6860 - val_accuracy: 0.5444\n",
      "Epoch 15/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5560 - val_loss: 0.6893 - val_accuracy: 0.5444\n",
      "Epoch 16/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5631 - val_loss: 0.6888 - val_accuracy: 0.5361\n",
      "Epoch 17/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6864 - accuracy: 0.5667 - val_loss: 0.6864 - val_accuracy: 0.5389\n",
      "Epoch 18/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5643 - val_loss: 0.6873 - val_accuracy: 0.5333\n",
      "Epoch 19/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5595 - val_loss: 0.6859 - val_accuracy: 0.5444\n",
      "Epoch 20/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.5690 - val_loss: 0.6883 - val_accuracy: 0.5444\n",
      "Epoch 21/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6878 - accuracy: 0.5512 - val_loss: 0.6872 - val_accuracy: 0.5250\n",
      "Epoch 22/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.5476 - val_loss: 0.6866 - val_accuracy: 0.5306\n",
      "Epoch 23/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5250 - val_loss: 0.6892 - val_accuracy: 0.5278\n",
      "Epoch 24/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6881 - accuracy: 0.5429 - val_loss: 0.6882 - val_accuracy: 0.5222\n",
      "Epoch 25/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5500 - val_loss: 0.6869 - val_accuracy: 0.5361\n",
      "Epoch 26/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.5583 - val_loss: 0.6888 - val_accuracy: 0.5389\n",
      "Epoch 27/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5619 - val_loss: 0.6888 - val_accuracy: 0.5389\n",
      "Epoch 28/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.5571 - val_loss: 0.6923 - val_accuracy: 0.5333\n",
      "Epoch 29/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5643 - val_loss: 0.6886 - val_accuracy: 0.5111\n",
      "Epoch 30/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.5536 - val_loss: 0.6909 - val_accuracy: 0.5361\n",
      "Epoch 31/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5500 - val_loss: 0.6892 - val_accuracy: 0.5167\n",
      "Epoch 32/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6838 - accuracy: 0.5560 - val_loss: 0.6929 - val_accuracy: 0.5333\n",
      "Epoch 33/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5524 - val_loss: 0.6911 - val_accuracy: 0.5389\n",
      "Epoch 34/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6833 - accuracy: 0.5631 - val_loss: 0.6897 - val_accuracy: 0.5278\n",
      "Epoch 35/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6831 - accuracy: 0.5607 - val_loss: 0.6909 - val_accuracy: 0.5306\n",
      "Epoch 36/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6830 - accuracy: 0.5643 - val_loss: 0.6882 - val_accuracy: 0.5444\n",
      "Epoch 37/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6834 - accuracy: 0.5655 - val_loss: 0.6911 - val_accuracy: 0.5250\n",
      "Epoch 38/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6822 - accuracy: 0.5690 - val_loss: 0.6926 - val_accuracy: 0.5278\n",
      "Epoch 39/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.5643 - val_loss: 0.6893 - val_accuracy: 0.5222\n",
      "Epoch 40/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6854 - accuracy: 0.5619 - val_loss: 0.6937 - val_accuracy: 0.5139\n",
      "Epoch 41/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.5631 - val_loss: 0.6931 - val_accuracy: 0.5167\n",
      "Epoch 42/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5631 - val_loss: 0.6949 - val_accuracy: 0.5167\n",
      "Epoch 43/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6854 - accuracy: 0.5607 - val_loss: 0.6947 - val_accuracy: 0.5167\n",
      "Epoch 44/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5631 - val_loss: 0.6954 - val_accuracy: 0.5194\n",
      "Epoch 45/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5595 - val_loss: 0.6973 - val_accuracy: 0.5194\n",
      "Epoch 46/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.6934 - val_accuracy: 0.5222\n",
      "Epoch 47/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5440 - val_loss: 0.6917 - val_accuracy: 0.5250\n",
      "Epoch 48/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.5476 - val_loss: 0.6929 - val_accuracy: 0.5250\n",
      "Epoch 49/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6936 - val_accuracy: 0.5250\n",
      "Epoch 50/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.5488 - val_loss: 0.6935 - val_accuracy: 0.5250\n",
      "Epoch 51/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6855 - accuracy: 0.5488 - val_loss: 0.6935 - val_accuracy: 0.5250\n",
      "Epoch 52/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.5488 - val_loss: 0.6920 - val_accuracy: 0.5250\n",
      "Epoch 53/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.5357 - val_loss: 0.6966 - val_accuracy: 0.5250\n",
      "Epoch 54/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.5488 - val_loss: 0.6954 - val_accuracy: 0.5250\n",
      "Epoch 55/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6864 - accuracy: 0.5488 - val_loss: 0.6944 - val_accuracy: 0.5250\n",
      "Epoch 56/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5488 - val_loss: 0.6942 - val_accuracy: 0.5250\n",
      "Epoch 57/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5488 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
      "Epoch 58/500\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.5488 - val_loss: 0.6940 - val_accuracy: 0.5250\n",
      "Epoch 59/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5488 - val_loss: 0.6948 - val_accuracy: 0.5250\n",
      "Epoch 60/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5488 - val_loss: 0.6965 - val_accuracy: 0.5250\n",
      "Epoch 61/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5488 - val_loss: 0.6944 - val_accuracy: 0.5250\n",
      "Epoch 62/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6870 - accuracy: 0.5488 - val_loss: 0.6949 - val_accuracy: 0.5250\n",
      "Epoch 63/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5488 - val_loss: 0.6976 - val_accuracy: 0.5250\n",
      "Epoch 64/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5488 - val_loss: 0.6948 - val_accuracy: 0.5250\n",
      "Epoch 65/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5488 - val_loss: 0.6933 - val_accuracy: 0.5250\n",
      "Epoch 66/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5321 - val_loss: 0.6945 - val_accuracy: 0.5250\n",
      "Epoch 67/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.5405 - val_loss: 0.6976 - val_accuracy: 0.5250\n",
      "Epoch 68/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5476 - val_loss: 0.6977 - val_accuracy: 0.5250\n",
      "Epoch 69/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5488 - val_loss: 0.6970 - val_accuracy: 0.5250\n",
      "Epoch 70/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.5476 - val_loss: 0.6964 - val_accuracy: 0.5250\n",
      "Epoch 71/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5488 - val_loss: 0.6958 - val_accuracy: 0.5250\n",
      "Epoch 72/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.5488 - val_loss: 0.6994 - val_accuracy: 0.5250\n",
      "Epoch 73/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6869 - accuracy: 0.5488 - val_loss: 0.6954 - val_accuracy: 0.5250\n",
      "Epoch 74/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6861 - accuracy: 0.5488 - val_loss: 0.6955 - val_accuracy: 0.5250\n",
      "Epoch 75/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5488 - val_loss: 0.6944 - val_accuracy: 0.5250\n",
      "Epoch 76/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.5440 - val_loss: 0.6969 - val_accuracy: 0.5250\n",
      "Epoch 77/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5488 - val_loss: 0.6953 - val_accuracy: 0.5250\n",
      "Epoch 78/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.5488 - val_loss: 0.6958 - val_accuracy: 0.5250\n",
      "Epoch 79/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.5488 - val_loss: 0.6948 - val_accuracy: 0.5250\n",
      "Epoch 80/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.5488 - val_loss: 0.6953 - val_accuracy: 0.5250\n",
      "Epoch 81/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5488 - val_loss: 0.6946 - val_accuracy: 0.5250\n",
      "Epoch 82/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5476 - val_loss: 0.6966 - val_accuracy: 0.5250\n",
      "Epoch 83/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6860 - accuracy: 0.5488 - val_loss: 0.6961 - val_accuracy: 0.5250\n",
      "Epoch 84/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5488 - val_loss: 0.6966 - val_accuracy: 0.5250\n",
      "Epoch 85/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6853 - accuracy: 0.5512 - val_loss: 0.7007 - val_accuracy: 0.5111\n",
      "Epoch 86/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.5440 - val_loss: 0.7000 - val_accuracy: 0.5056\n",
      "Epoch 87/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6873 - accuracy: 0.5452 - val_loss: 0.7005 - val_accuracy: 0.5056\n",
      "Epoch 88/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5452 - val_loss: 0.7019 - val_accuracy: 0.5056\n",
      "Epoch 89/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6864 - accuracy: 0.5440 - val_loss: 0.7048 - val_accuracy: 0.5056\n",
      "Epoch 90/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5452 - val_loss: 0.7029 - val_accuracy: 0.5056\n",
      "Epoch 91/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6869 - accuracy: 0.5452 - val_loss: 0.7011 - val_accuracy: 0.5056\n",
      "Epoch 92/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5464 - val_loss: 0.6999 - val_accuracy: 0.5083\n",
      "Epoch 93/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5440 - val_loss: 0.7002 - val_accuracy: 0.5083\n",
      "Epoch 94/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.5440 - val_loss: 0.7008 - val_accuracy: 0.5083\n",
      "Epoch 95/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5452 - val_loss: 0.7003 - val_accuracy: 0.5083\n",
      "Epoch 96/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5452 - val_loss: 0.7014 - val_accuracy: 0.5083\n",
      "Epoch 97/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5452 - val_loss: 0.7034 - val_accuracy: 0.5056\n",
      "Epoch 98/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6880 - accuracy: 0.5464 - val_loss: 0.7021 - val_accuracy: 0.5083\n",
      "Epoch 99/500\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6864 - accuracy: 0.5452 - val_loss: 0.7015 - val_accuracy: 0.5083\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_sum_4 = init_model()\n",
    "\n",
    "es = EarlyStopping(patience=80, restore_best_weights=True,monitor='val_loss')\n",
    "history_sum_4 = model_sum_4.fit(X_train_sum_2, y_train_sum_2, \n",
    "          epochs=500, \n",
    "          batch_size=4, \n",
    "          verbose=1, \n",
    "          callbacks=[es],\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5367\n",
      "mean score [0.6910333037376404, 0.5366666913032532]\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5333\n",
      "std score [0.6934347152709961, 0.5333333611488342]\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5800\n",
      " max [0.685415506362915, 0.5799999833106995]\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.4733\n",
      " sum [0.6928543448448181, 0.47333332896232605]\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean score {model_mean_3.evaluate(X_test_2,y_test_2)}\")\n",
    "print(f\"std score {model_std_3.evaluate(X_test_std_2,y_test_std_2)}\")\n",
    "print(f\" max {model_max_3.evaluate(X_test_max_2,y_test_max_2)}\")\n",
    "print(f\" sum {model_sum_3.evaluate(X_test_sum_2,y_test_sum_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2,X_train_std_2,X_test_std_2,y_train_std_2,y_test_std_2,\\\n",
    "X_train_max_2,X_test_max_2,y_train_max_2,y_test_max_2,X_train_sum_2,X_test_sum_2,y_train_sum_2,y_test_sum_2\\\n",
    "=create_all_data(test_2_mean,test_2_std,test_2_max,test_2_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_btc_df = pd.read_csv(\"BTC_Dataframe_daily (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>147.488007</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.444000</td>\n",
       "      <td>144.539993</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>29886.640000</td>\n",
       "      <td>28019.560000</td>\n",
       "      <td>29542.140000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>94581.65463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>29397.660000</td>\n",
       "      <td>28282.900000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>28629.800000</td>\n",
       "      <td>90998.52010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>29266.000000</td>\n",
       "      <td>28450.000000</td>\n",
       "      <td>28629.810000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>34479.35127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>29587.780000</td>\n",
       "      <td>28839.210000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>27567.34764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>30928.710000</td>\n",
       "      <td>29299.620000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>30731.870000</td>\n",
       "      <td>31900.00955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3319 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date          High           Low          Open         Close  \\\n",
       "0     2013-04-29    147.488007    134.000000    134.444000    144.539993   \n",
       "1     2013-04-30    146.929993    134.050003    144.000000    139.000000   \n",
       "2     2013-05-01    139.889999    107.720001    139.000000    116.989998   \n",
       "3     2013-05-02    125.599998     92.281898    116.379997    105.209999   \n",
       "4     2013-05-03    108.127998     79.099998    106.250000     97.750000   \n",
       "...          ...           ...           ...           ...           ...   \n",
       "3314  2022-05-26  29886.640000  28019.560000  29542.140000  29201.350000   \n",
       "3315  2022-05-27  29397.660000  28282.900000  29201.350000  28629.800000   \n",
       "3316  2022-05-28  29266.000000  28450.000000  28629.810000  29031.330000   \n",
       "3317  2022-05-29  29587.780000  28839.210000  29031.330000  29468.100000   \n",
       "3318  2022-05-30  30928.710000  29299.620000  29468.100000  30731.870000   \n",
       "\n",
       "           Volume  \n",
       "0         0.00000  \n",
       "1         0.00000  \n",
       "2         0.00000  \n",
       "3         0.00000  \n",
       "4         0.00000  \n",
       "...           ...  \n",
       "3314  94581.65463  \n",
       "3315  90998.52010  \n",
       "3316  34479.35127  \n",
       "3317  27567.34764  \n",
       "3318  31900.00955  \n",
       "\n",
       "[3319 rows x 6 columns]"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_btc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_btc_df.merge(daily, left_on='Date',right_on=\"date\", how=\"left\").to_csv(\"nlp_btc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>8.406436</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>59.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>807.596</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.091042</td>\n",
       "      <td>106.729</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>241.5044</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.202279</td>\n",
       "      <td>100.300454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>0.221794</td>\n",
       "      <td>436.318864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>3606.952</td>\n",
       "      <td>6.537687</td>\n",
       "      <td>92.950</td>\n",
       "      <td>0.540065</td>\n",
       "      <td>1.899358</td>\n",
       "      <td>526.023</td>\n",
       "      <td>37.584</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>4.894707</td>\n",
       "      <td>912.0243</td>\n",
       "      <td>40.8155</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>1.918625</td>\n",
       "      <td>458.130558</td>\n",
       "      <td>24.70000</td>\n",
       "      <td>1.890892</td>\n",
       "      <td>3.280482</td>\n",
       "      <td>1841.728790</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.285408</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>277.987</td>\n",
       "      <td>17.050</td>\n",
       "      <td>974</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1003.080017</td>\n",
       "      <td>958.698975</td>\n",
       "      <td>963.658020</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1.477750e+08</td>\n",
       "      <td>932.752014</td>\n",
       "      <td>881.415009</td>\n",
       "      <td>745.597763</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>116.910004</td>\n",
       "      <td>65.572998</td>\n",
       "      <td>78.575012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>44.739341</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>173.499</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>2340.209</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>306.287</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.569902</td>\n",
       "      <td>679.5298</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.186729</td>\n",
       "      <td>269.620143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>0.214772</td>\n",
       "      <td>1253.644989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.206413</td>\n",
       "      <td>17502.086</td>\n",
       "      <td>39.521719</td>\n",
       "      <td>1713.998</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>5.131474</td>\n",
       "      <td>2405.074</td>\n",
       "      <td>128.940</td>\n",
       "      <td>2.127162</td>\n",
       "      <td>25.927811</td>\n",
       "      <td>5998.5965</td>\n",
       "      <td>1124.5091</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>9.536864</td>\n",
       "      <td>2478.505829</td>\n",
       "      <td>290.46875</td>\n",
       "      <td>3.178987</td>\n",
       "      <td>19.136392</td>\n",
       "      <td>8964.742385</td>\n",
       "      <td>826.867708</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>2.455393</td>\n",
       "      <td>1161.867</td>\n",
       "      <td>71.424</td>\n",
       "      <td>2820</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1031.390015</td>\n",
       "      <td>996.702026</td>\n",
       "      <td>998.617004</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>2.221850e+08</td>\n",
       "      <td>946.907013</td>\n",
       "      <td>898.401001</td>\n",
       "      <td>746.116516</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>921.590027</td>\n",
       "      <td>123.348999</td>\n",
       "      <td>74.842987</td>\n",
       "      <td>100.159973</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>29.575341</td>\n",
       "      <td>9814.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>101.066</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>1389.482</td>\n",
       "      <td>0.104272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.087896</td>\n",
       "      <td>183.454</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>433.3606</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>169.272860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>756.365240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.851201</td>\n",
       "      <td>8120.911</td>\n",
       "      <td>22.470165</td>\n",
       "      <td>457.140</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>1356.676</td>\n",
       "      <td>189.144</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>15.705079</td>\n",
       "      <td>1527.9880</td>\n",
       "      <td>240.2550</td>\n",
       "      <td>1.087970</td>\n",
       "      <td>17.264859</td>\n",
       "      <td>1821.262586</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>2.816607</td>\n",
       "      <td>15.955071</td>\n",
       "      <td>4714.999394</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.404212</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>676.651</td>\n",
       "      <td>110.028</td>\n",
       "      <td>1674</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1044.079956</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1.851680e+08</td>\n",
       "      <td>970.488983</td>\n",
       "      <td>904.745972</td>\n",
       "      <td>746.543015</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.495972</td>\n",
       "      <td>139.093994</td>\n",
       "      <td>73.350983</td>\n",
       "      <td>124.343994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>17.128804</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>125.065</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>1571.329</td>\n",
       "      <td>0.106123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>209.624</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>433.0382</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.206345</td>\n",
       "      <td>191.342678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>873.635908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.674829</td>\n",
       "      <td>7004.225</td>\n",
       "      <td>13.556661</td>\n",
       "      <td>460.631</td>\n",
       "      <td>0.504505</td>\n",
       "      <td>2.465480</td>\n",
       "      <td>961.587</td>\n",
       "      <td>68.885</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>7.153452</td>\n",
       "      <td>1501.3400</td>\n",
       "      <td>182.1425</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>3.057769</td>\n",
       "      <td>689.644645</td>\n",
       "      <td>76.87500</td>\n",
       "      <td>1.916536</td>\n",
       "      <td>4.794911</td>\n",
       "      <td>3652.918407</td>\n",
       "      <td>110.797619</td>\n",
       "      <td>0.338716</td>\n",
       "      <td>1.856071</td>\n",
       "      <td>645.592</td>\n",
       "      <td>68.885</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1159.420044</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>3.449460e+08</td>\n",
       "      <td>1031.837524</td>\n",
       "      <td>962.416016</td>\n",
       "      <td>752.111511</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>920.382019</td>\n",
       "      <td>192.313965</td>\n",
       "      <td>122.892456</td>\n",
       "      <td>234.347961</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>8.126814</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>103.742</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>1430.305</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>184.961</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.552156</td>\n",
       "      <td>419.5827</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>180.724524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>0.229343</td>\n",
       "      <td>751.068731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987909</td>\n",
       "      <td>5136.216</td>\n",
       "      <td>5.895492</td>\n",
       "      <td>89.270</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>2.164643</td>\n",
       "      <td>784.089</td>\n",
       "      <td>61.320</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>5.255419</td>\n",
       "      <td>1307.0479</td>\n",
       "      <td>104.6719</td>\n",
       "      <td>0.445339</td>\n",
       "      <td>3.005284</td>\n",
       "      <td>765.537930</td>\n",
       "      <td>102.20000</td>\n",
       "      <td>1.685690</td>\n",
       "      <td>4.534309</td>\n",
       "      <td>2897.701636</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>0.256962</td>\n",
       "      <td>0.913094</td>\n",
       "      <td>441.718</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1719</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1191.099976</td>\n",
       "      <td>910.416992</td>\n",
       "      <td>1156.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>5.101990e+08</td>\n",
       "      <td>1050.758484</td>\n",
       "      <td>978.255981</td>\n",
       "      <td>754.891266</td>\n",
       "      <td>704.777008</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>35.124023</td>\n",
       "      <td>-37.378479</td>\n",
       "      <td>42.976990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>12.858644</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.077027</td>\n",
       "      <td>53.837</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>676.381</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.093740</td>\n",
       "      <td>86.777</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.569012</td>\n",
       "      <td>157.6007</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.209669</td>\n",
       "      <td>76.605181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>358.376961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.997778</td>\n",
       "      <td>3266.185</td>\n",
       "      <td>11.091972</td>\n",
       "      <td>207.000</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>1.227200</td>\n",
       "      <td>436.993</td>\n",
       "      <td>13.542</td>\n",
       "      <td>0.919273</td>\n",
       "      <td>5.926655</td>\n",
       "      <td>751.0461</td>\n",
       "      <td>54.4425</td>\n",
       "      <td>0.336855</td>\n",
       "      <td>2.692034</td>\n",
       "      <td>275.210130</td>\n",
       "      <td>20.60000</td>\n",
       "      <td>2.223944</td>\n",
       "      <td>6.649812</td>\n",
       "      <td>1816.962092</td>\n",
       "      <td>135.585000</td>\n",
       "      <td>0.375542</td>\n",
       "      <td>1.531212</td>\n",
       "      <td>306.818</td>\n",
       "      <td>25.938</td>\n",
       "      <td>817</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>7275.860000</td>\n",
       "      <td>7076.420000</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>3.364270e+04</td>\n",
       "      <td>7366.845000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7580.227500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8682.360000</td>\n",
       "      <td>162.240000</td>\n",
       "      <td>-112.105000</td>\n",
       "      <td>-1427.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>17.568946</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>44.191</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>590.619</td>\n",
       "      <td>0.105560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.089176</td>\n",
       "      <td>73.196</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.560205</td>\n",
       "      <td>142.7691</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.204129</td>\n",
       "      <td>69.411507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>0.231762</td>\n",
       "      <td>305.142919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.995823</td>\n",
       "      <td>3537.043</td>\n",
       "      <td>14.679252</td>\n",
       "      <td>252.057</td>\n",
       "      <td>0.651510</td>\n",
       "      <td>2.487959</td>\n",
       "      <td>461.269</td>\n",
       "      <td>40.950</td>\n",
       "      <td>1.337016</td>\n",
       "      <td>8.914525</td>\n",
       "      <td>946.6073</td>\n",
       "      <td>173.8547</td>\n",
       "      <td>0.532183</td>\n",
       "      <td>3.815865</td>\n",
       "      <td>376.785520</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>2.651977</td>\n",
       "      <td>9.292295</td>\n",
       "      <td>1877.599807</td>\n",
       "      <td>145.963889</td>\n",
       "      <td>0.348597</td>\n",
       "      <td>1.205841</td>\n",
       "      <td>246.807</td>\n",
       "      <td>19.520</td>\n",
       "      <td>708</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>7365.010000</td>\n",
       "      <td>7238.670000</td>\n",
       "      <td>7254.770000</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>2.684898e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7563.750000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8404.520000</td>\n",
       "      <td>223.640000</td>\n",
       "      <td>-69.760000</td>\n",
       "      <td>-1088.380000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>17.229820</td>\n",
       "      <td>3494.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>52.124</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>682.759</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>88.119</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>174.2284</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.209074</td>\n",
       "      <td>77.704314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>0.241058</td>\n",
       "      <td>359.889446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.338086</td>\n",
       "      <td>3570.245</td>\n",
       "      <td>13.412452</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>4.685706</td>\n",
       "      <td>550.055</td>\n",
       "      <td>121.472</td>\n",
       "      <td>1.296249</td>\n",
       "      <td>10.637944</td>\n",
       "      <td>1066.8126</td>\n",
       "      <td>219.1752</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>7.563631</td>\n",
       "      <td>600.285630</td>\n",
       "      <td>189.80000</td>\n",
       "      <td>2.335994</td>\n",
       "      <td>8.791418</td>\n",
       "      <td>1922.523426</td>\n",
       "      <td>182.500000</td>\n",
       "      <td>0.284131</td>\n",
       "      <td>0.794803</td>\n",
       "      <td>233.840</td>\n",
       "      <td>9.240</td>\n",
       "      <td>823</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>7528.450000</td>\n",
       "      <td>7288.000000</td>\n",
       "      <td>7315.360000</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>3.138711e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7535.250000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8439.000000</td>\n",
       "      <td>295.740000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>-1050.760000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>22.933473</td>\n",
       "      <td>5401.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.075380</td>\n",
       "      <td>61.556</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>831.227</td>\n",
       "      <td>0.111242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>104.218</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>229.9543</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>98.192138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.222258</td>\n",
       "      <td>426.880618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.615048</td>\n",
       "      <td>5598.203</td>\n",
       "      <td>18.445569</td>\n",
       "      <td>423.342</td>\n",
       "      <td>0.720659</td>\n",
       "      <td>3.736793</td>\n",
       "      <td>718.497</td>\n",
       "      <td>98.658</td>\n",
       "      <td>0.974704</td>\n",
       "      <td>13.650818</td>\n",
       "      <td>971.7796</td>\n",
       "      <td>263.0880</td>\n",
       "      <td>0.689802</td>\n",
       "      <td>8.775038</td>\n",
       "      <td>687.732899</td>\n",
       "      <td>261.00000</td>\n",
       "      <td>3.041377</td>\n",
       "      <td>13.345471</td>\n",
       "      <td>3032.252472</td>\n",
       "      <td>326.250000</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>2.259901</td>\n",
       "      <td>508.023</td>\n",
       "      <td>39.933</td>\n",
       "      <td>997</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>7408.240000</td>\n",
       "      <td>7220.000000</td>\n",
       "      <td>7388.430000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>2.960591e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8340.580000</td>\n",
       "      <td>180.810000</td>\n",
       "      <td>-139.900000</td>\n",
       "      <td>-1094.580000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>7.954076</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>65.366</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>737.583</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.098716</td>\n",
       "      <td>100.042</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.585552</td>\n",
       "      <td>142.7909</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.213328</td>\n",
       "      <td>80.145271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>0.232719</td>\n",
       "      <td>401.635726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.670367</td>\n",
       "      <td>3314.341</td>\n",
       "      <td>6.540559</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.899961</td>\n",
       "      <td>414.253</td>\n",
       "      <td>9.504</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>4.905297</td>\n",
       "      <td>427.4661</td>\n",
       "      <td>33.4815</td>\n",
       "      <td>0.276536</td>\n",
       "      <td>1.864274</td>\n",
       "      <td>249.711727</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>1.890935</td>\n",
       "      <td>3.360099</td>\n",
       "      <td>1707.514271</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>0.326038</td>\n",
       "      <td>0.801756</td>\n",
       "      <td>294.412</td>\n",
       "      <td>14.790</td>\n",
       "      <td>903</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>7320.000000</td>\n",
       "      <td>7145.010000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>7195.230000</td>\n",
       "      <td>2.595445e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8615.000000</td>\n",
       "      <td>130.040000</td>\n",
       "      <td>-190.670000</td>\n",
       "      <td>-1419.770000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  score_std  score_sum  score_max  v_neg_mean  \\\n",
       "0     2017-01-01    4.095483   8.406436     3989.0      116.0    0.061278   \n",
       "1     2017-01-02    7.143617  44.739341    20145.0     1859.0    0.061524   \n",
       "2     2017-01-03    5.862605  29.575341     9814.0      570.0    0.060374   \n",
       "3     2017-01-04    4.271773  17.128804     8142.0      599.0    0.065616   \n",
       "4     2017-01-05    3.549738   8.126814     6102.0      146.0    0.060350   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "1090  2019-12-27    3.924113  12.858644     3206.0      207.0    0.065896   \n",
       "1091  2019-12-28    5.111582  17.568946     3619.0      281.0    0.062417   \n",
       "1092  2019-12-29    4.245443  17.229820     3494.0      292.0    0.063334   \n",
       "1093  2019-12-30    5.417252  22.933473     5401.0      522.0    0.061741   \n",
       "1094  2019-12-31    3.635659   7.954076     3283.0      102.0    0.072388   \n",
       "\n",
       "      v_neg_std  v_neg_sum  v_neg_max  v_neu_mean  v_neu_sum  v_neu_std  \\\n",
       "0      0.074932     59.685      0.674    0.829154    807.596   0.108736   \n",
       "1      0.067844    173.499      0.531    0.829861   2340.209   0.101984   \n",
       "2      0.071586    101.066      0.579    0.830037   1389.482   0.104272   \n",
       "3      0.073589    125.065      0.582    0.824412   1571.329   0.106123   \n",
       "4      0.072556    103.742      0.565    0.832056   1430.305   0.109600   \n",
       "...         ...        ...        ...         ...        ...        ...   \n",
       "1090   0.077027     53.837      0.600    0.827884    676.381   0.113621   \n",
       "1091   0.074158     44.191      0.461    0.834208    590.619   0.105560   \n",
       "1092   0.069795     52.124      0.469    0.829598    682.759   0.108458   \n",
       "1093   0.075380     61.556      0.753    0.833728    831.227   0.111242   \n",
       "1094   0.070688     65.366      0.427    0.816814    737.583   0.112108   \n",
       "\n",
       "      v_neu_max  v_pos_mean  v_pos_std  v_pos_sum  v_pos_max  v_compound_mean  \\\n",
       "0           1.0    0.109578   0.091042    106.729      0.579         0.247951   \n",
       "1           1.0    0.108612   0.087565    306.287      0.674         0.240968   \n",
       "2           1.0    0.109590   0.087896    183.454      0.636         0.258877   \n",
       "3           1.0    0.109981   0.090355    209.624      0.735         0.227197   \n",
       "4           1.0    0.107598   0.092280    184.961      0.627         0.244085   \n",
       "...         ...         ...        ...        ...        ...              ...   \n",
       "1090        1.0    0.106214   0.093740     86.777      0.620         0.192902   \n",
       "1091        1.0    0.103384   0.089176     73.196      0.482         0.201651   \n",
       "1092        1.0    0.107070   0.091100     88.119      0.502         0.211699   \n",
       "1093        1.0    0.104532   0.092085    104.218      0.592         0.230646   \n",
       "1094        1.0    0.110788   0.098716    100.042      0.645         0.158129   \n",
       "\n",
       "      v_compound_std  v_compound_sum  v_compound_max  t_pol_mean  t_pol_std  \\\n",
       "0           0.550243        241.5044          0.9984    0.102978   0.202279   \n",
       "1           0.569902        679.5298          0.9997    0.095610   0.186729   \n",
       "2           0.560860        433.3606          0.9987    0.101119   0.209033   \n",
       "3           0.575732        433.0382          0.9998    0.100390   0.206345   \n",
       "4           0.552156        419.5827          0.9984    0.105134   0.199678   \n",
       "...              ...             ...             ...         ...        ...   \n",
       "1090        0.569012        157.6007          0.9979    0.093764   0.209669   \n",
       "1091        0.560205        142.7691          0.9988    0.098039   0.204129   \n",
       "1092        0.561760        174.2284          0.9979    0.094416   0.209074   \n",
       "1093        0.564663        229.9543          0.9979    0.098488   0.195556   \n",
       "1094        0.585552        142.7909          0.9980    0.088754   0.213328   \n",
       "\n",
       "       t_pol_sum  t_pol_max  t_sub_mean  t_sub_std    t_sub_sum  t_sub_max  \\\n",
       "0     100.300454        1.0    0.447966   0.221794   436.318864        1.0   \n",
       "1     269.620143        1.0    0.444555   0.214772  1253.644989        1.0   \n",
       "2     169.272860        1.0    0.451831   0.216360   756.365240        1.0   \n",
       "3     191.342678        1.0    0.458361   0.207941   873.635908        1.0   \n",
       "4     180.724524        1.0    0.436922   0.229343   751.068731        1.0   \n",
       "...          ...        ...         ...        ...          ...        ...   \n",
       "1090   76.605181        1.0    0.438650   0.241026   358.376961        1.0   \n",
       "1091   69.411507        1.0    0.430993   0.231762   305.142919        1.0   \n",
       "1092   77.704314        1.0    0.437290   0.241058   359.889446        1.0   \n",
       "1093   98.192138        1.0    0.428165   0.222258   426.880618        1.0   \n",
       "1094   80.145271        1.0    0.444779   0.232719   401.635726        1.0   \n",
       "\n",
       "      v_neu_score_mean  v_neu_score_sum  v_neu_score_std  v_neu_score_max  \\\n",
       "0             3.703236         3606.952         6.537687           92.950   \n",
       "1             6.206413        17502.086        39.521719         1713.998   \n",
       "2             4.851201         8120.911        22.470165          457.140   \n",
       "3             3.674829         7004.225        13.556661          460.631   \n",
       "4             2.987909         5136.216         5.895492           89.270   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          3.997778         3266.185        11.091972          207.000   \n",
       "1091          4.995823         3537.043        14.679252          252.057   \n",
       "1092          4.338086         3570.245        13.412452          229.000   \n",
       "1093          5.615048         5598.203        18.445569          423.342   \n",
       "1094          3.670367         3314.341         6.540559           90.000   \n",
       "\n",
       "      v_pos_score_mean  v_pos_score_std  v_pos_score_sum  v_pos_score_max  \\\n",
       "0             0.540065         1.899358          526.023           37.584   \n",
       "1             0.852863         5.131474         2405.074          128.940   \n",
       "2             0.810440         7.089782         1356.676          189.144   \n",
       "3             0.504505         2.465480          961.587           68.885   \n",
       "4             0.456131         2.164643          784.089           61.320   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          0.534875         1.227200          436.993           13.542   \n",
       "1091          0.651510         2.487959          461.269           40.950   \n",
       "1092          0.668354         4.685706          550.055          121.472   \n",
       "1093          0.720659         3.736793          718.497           98.658   \n",
       "1094          0.458752         0.899961          414.253            9.504   \n",
       "\n",
       "      v_compound_score_mean  v_compound_score_std  v_compound_score_sum  \\\n",
       "0                  0.936370              4.894707              912.0243   \n",
       "1                  2.127162             25.927811             5998.5965   \n",
       "2                  0.912777             15.705079             1527.9880   \n",
       "3                  0.787692              7.153452             1501.3400   \n",
       "4                  0.760354              5.255419             1307.0479   \n",
       "...                     ...                   ...                   ...   \n",
       "1090               0.919273              5.926655              751.0461   \n",
       "1091               1.337016              8.914525              946.6073   \n",
       "1092               1.296249             10.637944             1066.8126   \n",
       "1093               0.974704             13.650818              971.7796   \n",
       "1094               0.473384              4.905297              427.4661   \n",
       "\n",
       "      v_compound_score_max  t_pol_score_mean  t_pol_score_std  \\\n",
       "0                  40.8155          0.470360         1.918625   \n",
       "1                1124.5091          0.878903         9.536864   \n",
       "2                 240.2550          1.087970        17.264859   \n",
       "3                 182.1425          0.361828         3.057769   \n",
       "4                 104.6719          0.445339         3.005284   \n",
       "...                    ...               ...              ...   \n",
       "1090               54.4425          0.336855         2.692034   \n",
       "1091              173.8547          0.532183         3.815865   \n",
       "1092              219.1752          0.729387         7.563631   \n",
       "1093              263.0880          0.689802         8.775038   \n",
       "1094               33.4815          0.276536         1.864274   \n",
       "\n",
       "      t_pol_score_sum  t_pol_score_max  t_sub_score_mean  t_sub_score_std  \\\n",
       "0          458.130558         24.70000          1.890892         3.280482   \n",
       "1         2478.505829        290.46875          3.178987        19.136392   \n",
       "2         1821.262586        456.00000          2.816607        15.955071   \n",
       "3          689.644645         76.87500          1.916536         4.794911   \n",
       "4          765.537930        102.20000          1.685690         4.534309   \n",
       "...               ...              ...               ...              ...   \n",
       "1090       275.210130         20.60000          2.223944         6.649812   \n",
       "1091       376.785520         75.00000          2.651977         9.292295   \n",
       "1092       600.285630        189.80000          2.335994         8.791418   \n",
       "1093       687.732899        261.00000          3.041377        13.345471   \n",
       "1094       249.711727         11.25000          1.890935         3.360099   \n",
       "\n",
       "      t_sub_score_sum  t_sub_score_max  v_neg_score_mean  v_neg_score_std  \\\n",
       "0         1841.728790        32.400000          0.285408         0.883918   \n",
       "1         8964.742385       826.867708          0.412010         2.455393   \n",
       "2         4714.999394       399.000000          0.404212         3.511884   \n",
       "3         3652.918407       110.797619          0.338716         1.856071   \n",
       "4         2897.701636        87.600000          0.256962         0.913094   \n",
       "...               ...              ...               ...              ...   \n",
       "1090      1816.962092       135.585000          0.375542         1.531212   \n",
       "1091      1877.599807       145.963889          0.348597         1.205841   \n",
       "1092      1922.523426       182.500000          0.284131         0.794803   \n",
       "1093      3032.252472       326.250000          0.509552         2.259901   \n",
       "1094      1707.514271        38.250000          0.326038         0.801756   \n",
       "\n",
       "      v_neg_score_sum  v_neg_score_max  date_count        Date         High  \\\n",
       "0             277.987           17.050         974  2017-01-01  1003.080017   \n",
       "1            1161.867           71.424        2820  2017-01-02  1031.390015   \n",
       "2             676.651          110.028        1674  2017-01-03  1044.079956   \n",
       "3             645.592           68.885        1906  2017-01-04  1159.420044   \n",
       "4             441.718           16.800        1719  2017-01-05  1191.099976   \n",
       "...               ...              ...         ...         ...          ...   \n",
       "1090          306.818           25.938         817  2019-12-27  7275.860000   \n",
       "1091          246.807           19.520         708  2019-12-28  7365.010000   \n",
       "1092          233.840            9.240         823  2019-12-29  7528.450000   \n",
       "1093          508.023           39.933         997  2019-12-30  7408.240000   \n",
       "1094          294.412           14.790         903  2019-12-31  7320.000000   \n",
       "\n",
       "              Low         Open        Close        Volume   tenkan_sen  \\\n",
       "0      958.698975   963.658020   998.325012  1.477750e+08   932.752014   \n",
       "1      996.702026   998.617004  1021.750000  2.221850e+08   946.907013   \n",
       "2     1021.599976  1021.599976  1043.839966  1.851680e+08   970.488983   \n",
       "3     1044.400024  1044.400024  1154.729980  3.449460e+08  1031.837524   \n",
       "4      910.416992  1156.729980  1013.380005  5.101990e+08  1050.758484   \n",
       "...           ...          ...          ...           ...          ...   \n",
       "1090  7076.420000  7202.000000  7254.740000  3.364270e+04  7366.845000   \n",
       "1091  7238.670000  7254.770000  7316.140000  2.684898e+04  7385.900000   \n",
       "1092  7288.000000  7315.360000  7388.240000  3.138711e+04  7385.900000   \n",
       "1093  7220.000000  7388.430000  7246.000000  2.960591e+04  7385.900000   \n",
       "1094  7145.010000  7246.000000  7195.230000  2.595445e+04  7385.900000   \n",
       "\n",
       "        kijun_sen  senkou_span_a  senkou_span_b  chikou_span  diff_kijun  \\\n",
       "0      881.415009     745.597763     704.654510   919.750000  116.910004   \n",
       "1      898.401001     746.116516     704.654510   921.590027  123.348999   \n",
       "2      904.745972     746.543015     704.654510   919.495972  139.093994   \n",
       "3      962.416016     752.111511     704.654510   920.382019  192.313965   \n",
       "4      978.255981     754.891266     704.777008   970.403015   35.124023   \n",
       "...           ...            ...            ...          ...         ...   \n",
       "1090  7092.500000    7580.227500    8442.500000  8682.360000  162.240000   \n",
       "1091  7092.500000    7563.750000    8442.500000  8404.520000  223.640000   \n",
       "1092  7092.500000    7535.250000    8442.500000  8439.000000  295.740000   \n",
       "1093  7065.190000    7588.047500    8442.500000  8340.580000  180.810000   \n",
       "1094  7065.190000    7588.047500    8442.500000  8615.000000  130.040000   \n",
       "\n",
       "      diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  chikou_signal  \\\n",
       "0       65.572998    78.575012             1              1              1   \n",
       "1       74.842987   100.159973             1              1              1   \n",
       "2       73.350983   124.343994             1              1              1   \n",
       "3      122.892456   234.347961             1              1              1   \n",
       "4      -37.378479    42.976990             0              0              0   \n",
       "...           ...          ...           ...            ...            ...   \n",
       "1090  -112.105000 -1427.620000             1             -1             -1   \n",
       "1091   -69.760000 -1088.380000             1             -1             -1   \n",
       "1092     2.340000 -1050.760000             1              0             -1   \n",
       "1093  -139.900000 -1094.580000             1             -1             -1   \n",
       "1094  -190.670000 -1419.770000             1             -1             -1   \n",
       "\n",
       "      indice  date_time  class  \n",
       "0          0 2017-01-01      1  \n",
       "1          0 2017-01-02      1  \n",
       "2          0 2017-01-03      1  \n",
       "3          0 2017-01-04      1  \n",
       "4          0 2017-01-05      0  \n",
       "...      ...        ...    ...  \n",
       "1090       0 2019-12-27      1  \n",
       "1091       0 2019-12-28      1  \n",
       "1092       0 2019-12-29      1  \n",
       "1093       0 2019-12-30      0  \n",
       "1094       0 2019-12-31      0  \n",
       "\n",
       "[1095 rows x 74 columns]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily[\"date_time\"]=pd.to_datetime(daily[\"date\"],format='%Y-%m-%d')  \n",
    "daily = daily[daily[\"date_time\"]>=pd.to_datetime(\"2017-01-01\",format='%Y-%m-%d')]\n",
    "daily = daily[daily[\"date_time\"]<=pd.to_datetime(\"2019-12-31\",format='%Y-%m-%d')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>8.406436</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>59.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>807.596</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.091042</td>\n",
       "      <td>106.729</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>241.5044</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.202279</td>\n",
       "      <td>100.300454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>0.221794</td>\n",
       "      <td>436.318864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>3606.952</td>\n",
       "      <td>6.537687</td>\n",
       "      <td>92.950</td>\n",
       "      <td>0.540065</td>\n",
       "      <td>1.899358</td>\n",
       "      <td>526.023</td>\n",
       "      <td>37.584</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>4.894707</td>\n",
       "      <td>912.0243</td>\n",
       "      <td>40.8155</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>1.918625</td>\n",
       "      <td>458.130558</td>\n",
       "      <td>24.70000</td>\n",
       "      <td>1.890892</td>\n",
       "      <td>3.280482</td>\n",
       "      <td>1841.728790</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.285408</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>277.987</td>\n",
       "      <td>17.050</td>\n",
       "      <td>974</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1003.080017</td>\n",
       "      <td>958.698975</td>\n",
       "      <td>963.658020</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1.477750e+08</td>\n",
       "      <td>932.752014</td>\n",
       "      <td>881.415009</td>\n",
       "      <td>745.597763</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>116.910004</td>\n",
       "      <td>65.572998</td>\n",
       "      <td>78.575012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>44.739341</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>173.499</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>2340.209</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>306.287</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.569902</td>\n",
       "      <td>679.5298</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.186729</td>\n",
       "      <td>269.620143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>0.214772</td>\n",
       "      <td>1253.644989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.206413</td>\n",
       "      <td>17502.086</td>\n",
       "      <td>39.521719</td>\n",
       "      <td>1713.998</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>5.131474</td>\n",
       "      <td>2405.074</td>\n",
       "      <td>128.940</td>\n",
       "      <td>2.127162</td>\n",
       "      <td>25.927811</td>\n",
       "      <td>5998.5965</td>\n",
       "      <td>1124.5091</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>9.536864</td>\n",
       "      <td>2478.505829</td>\n",
       "      <td>290.46875</td>\n",
       "      <td>3.178987</td>\n",
       "      <td>19.136392</td>\n",
       "      <td>8964.742385</td>\n",
       "      <td>826.867708</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>2.455393</td>\n",
       "      <td>1161.867</td>\n",
       "      <td>71.424</td>\n",
       "      <td>2820</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1031.390015</td>\n",
       "      <td>996.702026</td>\n",
       "      <td>998.617004</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>2.221850e+08</td>\n",
       "      <td>946.907013</td>\n",
       "      <td>898.401001</td>\n",
       "      <td>746.116516</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>921.590027</td>\n",
       "      <td>123.348999</td>\n",
       "      <td>74.842987</td>\n",
       "      <td>100.159973</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>29.575341</td>\n",
       "      <td>9814.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>101.066</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>1389.482</td>\n",
       "      <td>0.104272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.087896</td>\n",
       "      <td>183.454</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>433.3606</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>169.272860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>756.365240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.851201</td>\n",
       "      <td>8120.911</td>\n",
       "      <td>22.470165</td>\n",
       "      <td>457.140</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>1356.676</td>\n",
       "      <td>189.144</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>15.705079</td>\n",
       "      <td>1527.9880</td>\n",
       "      <td>240.2550</td>\n",
       "      <td>1.087970</td>\n",
       "      <td>17.264859</td>\n",
       "      <td>1821.262586</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>2.816607</td>\n",
       "      <td>15.955071</td>\n",
       "      <td>4714.999394</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.404212</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>676.651</td>\n",
       "      <td>110.028</td>\n",
       "      <td>1674</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1044.079956</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1.851680e+08</td>\n",
       "      <td>970.488983</td>\n",
       "      <td>904.745972</td>\n",
       "      <td>746.543015</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.495972</td>\n",
       "      <td>139.093994</td>\n",
       "      <td>73.350983</td>\n",
       "      <td>124.343994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>17.128804</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>125.065</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>1571.329</td>\n",
       "      <td>0.106123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>209.624</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>433.0382</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.206345</td>\n",
       "      <td>191.342678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>873.635908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.674829</td>\n",
       "      <td>7004.225</td>\n",
       "      <td>13.556661</td>\n",
       "      <td>460.631</td>\n",
       "      <td>0.504505</td>\n",
       "      <td>2.465480</td>\n",
       "      <td>961.587</td>\n",
       "      <td>68.885</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>7.153452</td>\n",
       "      <td>1501.3400</td>\n",
       "      <td>182.1425</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>3.057769</td>\n",
       "      <td>689.644645</td>\n",
       "      <td>76.87500</td>\n",
       "      <td>1.916536</td>\n",
       "      <td>4.794911</td>\n",
       "      <td>3652.918407</td>\n",
       "      <td>110.797619</td>\n",
       "      <td>0.338716</td>\n",
       "      <td>1.856071</td>\n",
       "      <td>645.592</td>\n",
       "      <td>68.885</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1159.420044</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>3.449460e+08</td>\n",
       "      <td>1031.837524</td>\n",
       "      <td>962.416016</td>\n",
       "      <td>752.111511</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>920.382019</td>\n",
       "      <td>192.313965</td>\n",
       "      <td>122.892456</td>\n",
       "      <td>234.347961</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>8.126814</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>103.742</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>1430.305</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>184.961</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.552156</td>\n",
       "      <td>419.5827</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>180.724524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>0.229343</td>\n",
       "      <td>751.068731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987909</td>\n",
       "      <td>5136.216</td>\n",
       "      <td>5.895492</td>\n",
       "      <td>89.270</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>2.164643</td>\n",
       "      <td>784.089</td>\n",
       "      <td>61.320</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>5.255419</td>\n",
       "      <td>1307.0479</td>\n",
       "      <td>104.6719</td>\n",
       "      <td>0.445339</td>\n",
       "      <td>3.005284</td>\n",
       "      <td>765.537930</td>\n",
       "      <td>102.20000</td>\n",
       "      <td>1.685690</td>\n",
       "      <td>4.534309</td>\n",
       "      <td>2897.701636</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>0.256962</td>\n",
       "      <td>0.913094</td>\n",
       "      <td>441.718</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1719</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1191.099976</td>\n",
       "      <td>910.416992</td>\n",
       "      <td>1156.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>5.101990e+08</td>\n",
       "      <td>1050.758484</td>\n",
       "      <td>978.255981</td>\n",
       "      <td>754.891266</td>\n",
       "      <td>704.777008</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>35.124023</td>\n",
       "      <td>-37.378479</td>\n",
       "      <td>42.976990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>12.858644</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.077027</td>\n",
       "      <td>53.837</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>676.381</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.093740</td>\n",
       "      <td>86.777</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.569012</td>\n",
       "      <td>157.6007</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.209669</td>\n",
       "      <td>76.605181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>358.376961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.997778</td>\n",
       "      <td>3266.185</td>\n",
       "      <td>11.091972</td>\n",
       "      <td>207.000</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>1.227200</td>\n",
       "      <td>436.993</td>\n",
       "      <td>13.542</td>\n",
       "      <td>0.919273</td>\n",
       "      <td>5.926655</td>\n",
       "      <td>751.0461</td>\n",
       "      <td>54.4425</td>\n",
       "      <td>0.336855</td>\n",
       "      <td>2.692034</td>\n",
       "      <td>275.210130</td>\n",
       "      <td>20.60000</td>\n",
       "      <td>2.223944</td>\n",
       "      <td>6.649812</td>\n",
       "      <td>1816.962092</td>\n",
       "      <td>135.585000</td>\n",
       "      <td>0.375542</td>\n",
       "      <td>1.531212</td>\n",
       "      <td>306.818</td>\n",
       "      <td>25.938</td>\n",
       "      <td>817</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>7275.860000</td>\n",
       "      <td>7076.420000</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>3.364270e+04</td>\n",
       "      <td>7366.845000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7580.227500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8682.360000</td>\n",
       "      <td>162.240000</td>\n",
       "      <td>-112.105000</td>\n",
       "      <td>-1427.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>17.568946</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>44.191</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>590.619</td>\n",
       "      <td>0.105560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.089176</td>\n",
       "      <td>73.196</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.560205</td>\n",
       "      <td>142.7691</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.204129</td>\n",
       "      <td>69.411507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>0.231762</td>\n",
       "      <td>305.142919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.995823</td>\n",
       "      <td>3537.043</td>\n",
       "      <td>14.679252</td>\n",
       "      <td>252.057</td>\n",
       "      <td>0.651510</td>\n",
       "      <td>2.487959</td>\n",
       "      <td>461.269</td>\n",
       "      <td>40.950</td>\n",
       "      <td>1.337016</td>\n",
       "      <td>8.914525</td>\n",
       "      <td>946.6073</td>\n",
       "      <td>173.8547</td>\n",
       "      <td>0.532183</td>\n",
       "      <td>3.815865</td>\n",
       "      <td>376.785520</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>2.651977</td>\n",
       "      <td>9.292295</td>\n",
       "      <td>1877.599807</td>\n",
       "      <td>145.963889</td>\n",
       "      <td>0.348597</td>\n",
       "      <td>1.205841</td>\n",
       "      <td>246.807</td>\n",
       "      <td>19.520</td>\n",
       "      <td>708</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>7365.010000</td>\n",
       "      <td>7238.670000</td>\n",
       "      <td>7254.770000</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>2.684898e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7563.750000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8404.520000</td>\n",
       "      <td>223.640000</td>\n",
       "      <td>-69.760000</td>\n",
       "      <td>-1088.380000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>17.229820</td>\n",
       "      <td>3494.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>52.124</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>682.759</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>88.119</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>174.2284</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.209074</td>\n",
       "      <td>77.704314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>0.241058</td>\n",
       "      <td>359.889446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.338086</td>\n",
       "      <td>3570.245</td>\n",
       "      <td>13.412452</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>4.685706</td>\n",
       "      <td>550.055</td>\n",
       "      <td>121.472</td>\n",
       "      <td>1.296249</td>\n",
       "      <td>10.637944</td>\n",
       "      <td>1066.8126</td>\n",
       "      <td>219.1752</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>7.563631</td>\n",
       "      <td>600.285630</td>\n",
       "      <td>189.80000</td>\n",
       "      <td>2.335994</td>\n",
       "      <td>8.791418</td>\n",
       "      <td>1922.523426</td>\n",
       "      <td>182.500000</td>\n",
       "      <td>0.284131</td>\n",
       "      <td>0.794803</td>\n",
       "      <td>233.840</td>\n",
       "      <td>9.240</td>\n",
       "      <td>823</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>7528.450000</td>\n",
       "      <td>7288.000000</td>\n",
       "      <td>7315.360000</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>3.138711e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7535.250000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8439.000000</td>\n",
       "      <td>295.740000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>-1050.760000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>22.933473</td>\n",
       "      <td>5401.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.075380</td>\n",
       "      <td>61.556</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>831.227</td>\n",
       "      <td>0.111242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>104.218</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>229.9543</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>98.192138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.222258</td>\n",
       "      <td>426.880618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.615048</td>\n",
       "      <td>5598.203</td>\n",
       "      <td>18.445569</td>\n",
       "      <td>423.342</td>\n",
       "      <td>0.720659</td>\n",
       "      <td>3.736793</td>\n",
       "      <td>718.497</td>\n",
       "      <td>98.658</td>\n",
       "      <td>0.974704</td>\n",
       "      <td>13.650818</td>\n",
       "      <td>971.7796</td>\n",
       "      <td>263.0880</td>\n",
       "      <td>0.689802</td>\n",
       "      <td>8.775038</td>\n",
       "      <td>687.732899</td>\n",
       "      <td>261.00000</td>\n",
       "      <td>3.041377</td>\n",
       "      <td>13.345471</td>\n",
       "      <td>3032.252472</td>\n",
       "      <td>326.250000</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>2.259901</td>\n",
       "      <td>508.023</td>\n",
       "      <td>39.933</td>\n",
       "      <td>997</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>7408.240000</td>\n",
       "      <td>7220.000000</td>\n",
       "      <td>7388.430000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>2.960591e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8340.580000</td>\n",
       "      <td>180.810000</td>\n",
       "      <td>-139.900000</td>\n",
       "      <td>-1094.580000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>7.954076</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>65.366</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>737.583</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.098716</td>\n",
       "      <td>100.042</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.585552</td>\n",
       "      <td>142.7909</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.213328</td>\n",
       "      <td>80.145271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>0.232719</td>\n",
       "      <td>401.635726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.670367</td>\n",
       "      <td>3314.341</td>\n",
       "      <td>6.540559</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.899961</td>\n",
       "      <td>414.253</td>\n",
       "      <td>9.504</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>4.905297</td>\n",
       "      <td>427.4661</td>\n",
       "      <td>33.4815</td>\n",
       "      <td>0.276536</td>\n",
       "      <td>1.864274</td>\n",
       "      <td>249.711727</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>1.890935</td>\n",
       "      <td>3.360099</td>\n",
       "      <td>1707.514271</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>0.326038</td>\n",
       "      <td>0.801756</td>\n",
       "      <td>294.412</td>\n",
       "      <td>14.790</td>\n",
       "      <td>903</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>7320.000000</td>\n",
       "      <td>7145.010000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>7195.230000</td>\n",
       "      <td>2.595445e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8615.000000</td>\n",
       "      <td>130.040000</td>\n",
       "      <td>-190.670000</td>\n",
       "      <td>-1419.770000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  score_std  score_sum  score_max  v_neg_mean  \\\n",
       "0     2017-01-01    4.095483   8.406436     3989.0      116.0    0.061278   \n",
       "1     2017-01-02    7.143617  44.739341    20145.0     1859.0    0.061524   \n",
       "2     2017-01-03    5.862605  29.575341     9814.0      570.0    0.060374   \n",
       "3     2017-01-04    4.271773  17.128804     8142.0      599.0    0.065616   \n",
       "4     2017-01-05    3.549738   8.126814     6102.0      146.0    0.060350   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "1090  2019-12-27    3.924113  12.858644     3206.0      207.0    0.065896   \n",
       "1091  2019-12-28    5.111582  17.568946     3619.0      281.0    0.062417   \n",
       "1092  2019-12-29    4.245443  17.229820     3494.0      292.0    0.063334   \n",
       "1093  2019-12-30    5.417252  22.933473     5401.0      522.0    0.061741   \n",
       "1094  2019-12-31    3.635659   7.954076     3283.0      102.0    0.072388   \n",
       "\n",
       "      v_neg_std  v_neg_sum  v_neg_max  v_neu_mean  v_neu_sum  v_neu_std  \\\n",
       "0      0.074932     59.685      0.674    0.829154    807.596   0.108736   \n",
       "1      0.067844    173.499      0.531    0.829861   2340.209   0.101984   \n",
       "2      0.071586    101.066      0.579    0.830037   1389.482   0.104272   \n",
       "3      0.073589    125.065      0.582    0.824412   1571.329   0.106123   \n",
       "4      0.072556    103.742      0.565    0.832056   1430.305   0.109600   \n",
       "...         ...        ...        ...         ...        ...        ...   \n",
       "1090   0.077027     53.837      0.600    0.827884    676.381   0.113621   \n",
       "1091   0.074158     44.191      0.461    0.834208    590.619   0.105560   \n",
       "1092   0.069795     52.124      0.469    0.829598    682.759   0.108458   \n",
       "1093   0.075380     61.556      0.753    0.833728    831.227   0.111242   \n",
       "1094   0.070688     65.366      0.427    0.816814    737.583   0.112108   \n",
       "\n",
       "      v_neu_max  v_pos_mean  v_pos_std  v_pos_sum  v_pos_max  v_compound_mean  \\\n",
       "0           1.0    0.109578   0.091042    106.729      0.579         0.247951   \n",
       "1           1.0    0.108612   0.087565    306.287      0.674         0.240968   \n",
       "2           1.0    0.109590   0.087896    183.454      0.636         0.258877   \n",
       "3           1.0    0.109981   0.090355    209.624      0.735         0.227197   \n",
       "4           1.0    0.107598   0.092280    184.961      0.627         0.244085   \n",
       "...         ...         ...        ...        ...        ...              ...   \n",
       "1090        1.0    0.106214   0.093740     86.777      0.620         0.192902   \n",
       "1091        1.0    0.103384   0.089176     73.196      0.482         0.201651   \n",
       "1092        1.0    0.107070   0.091100     88.119      0.502         0.211699   \n",
       "1093        1.0    0.104532   0.092085    104.218      0.592         0.230646   \n",
       "1094        1.0    0.110788   0.098716    100.042      0.645         0.158129   \n",
       "\n",
       "      v_compound_std  v_compound_sum  v_compound_max  t_pol_mean  t_pol_std  \\\n",
       "0           0.550243        241.5044          0.9984    0.102978   0.202279   \n",
       "1           0.569902        679.5298          0.9997    0.095610   0.186729   \n",
       "2           0.560860        433.3606          0.9987    0.101119   0.209033   \n",
       "3           0.575732        433.0382          0.9998    0.100390   0.206345   \n",
       "4           0.552156        419.5827          0.9984    0.105134   0.199678   \n",
       "...              ...             ...             ...         ...        ...   \n",
       "1090        0.569012        157.6007          0.9979    0.093764   0.209669   \n",
       "1091        0.560205        142.7691          0.9988    0.098039   0.204129   \n",
       "1092        0.561760        174.2284          0.9979    0.094416   0.209074   \n",
       "1093        0.564663        229.9543          0.9979    0.098488   0.195556   \n",
       "1094        0.585552        142.7909          0.9980    0.088754   0.213328   \n",
       "\n",
       "       t_pol_sum  t_pol_max  t_sub_mean  t_sub_std    t_sub_sum  t_sub_max  \\\n",
       "0     100.300454        1.0    0.447966   0.221794   436.318864        1.0   \n",
       "1     269.620143        1.0    0.444555   0.214772  1253.644989        1.0   \n",
       "2     169.272860        1.0    0.451831   0.216360   756.365240        1.0   \n",
       "3     191.342678        1.0    0.458361   0.207941   873.635908        1.0   \n",
       "4     180.724524        1.0    0.436922   0.229343   751.068731        1.0   \n",
       "...          ...        ...         ...        ...          ...        ...   \n",
       "1090   76.605181        1.0    0.438650   0.241026   358.376961        1.0   \n",
       "1091   69.411507        1.0    0.430993   0.231762   305.142919        1.0   \n",
       "1092   77.704314        1.0    0.437290   0.241058   359.889446        1.0   \n",
       "1093   98.192138        1.0    0.428165   0.222258   426.880618        1.0   \n",
       "1094   80.145271        1.0    0.444779   0.232719   401.635726        1.0   \n",
       "\n",
       "      v_neu_score_mean  v_neu_score_sum  v_neu_score_std  v_neu_score_max  \\\n",
       "0             3.703236         3606.952         6.537687           92.950   \n",
       "1             6.206413        17502.086        39.521719         1713.998   \n",
       "2             4.851201         8120.911        22.470165          457.140   \n",
       "3             3.674829         7004.225        13.556661          460.631   \n",
       "4             2.987909         5136.216         5.895492           89.270   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          3.997778         3266.185        11.091972          207.000   \n",
       "1091          4.995823         3537.043        14.679252          252.057   \n",
       "1092          4.338086         3570.245        13.412452          229.000   \n",
       "1093          5.615048         5598.203        18.445569          423.342   \n",
       "1094          3.670367         3314.341         6.540559           90.000   \n",
       "\n",
       "      v_pos_score_mean  v_pos_score_std  v_pos_score_sum  v_pos_score_max  \\\n",
       "0             0.540065         1.899358          526.023           37.584   \n",
       "1             0.852863         5.131474         2405.074          128.940   \n",
       "2             0.810440         7.089782         1356.676          189.144   \n",
       "3             0.504505         2.465480          961.587           68.885   \n",
       "4             0.456131         2.164643          784.089           61.320   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          0.534875         1.227200          436.993           13.542   \n",
       "1091          0.651510         2.487959          461.269           40.950   \n",
       "1092          0.668354         4.685706          550.055          121.472   \n",
       "1093          0.720659         3.736793          718.497           98.658   \n",
       "1094          0.458752         0.899961          414.253            9.504   \n",
       "\n",
       "      v_compound_score_mean  v_compound_score_std  v_compound_score_sum  \\\n",
       "0                  0.936370              4.894707              912.0243   \n",
       "1                  2.127162             25.927811             5998.5965   \n",
       "2                  0.912777             15.705079             1527.9880   \n",
       "3                  0.787692              7.153452             1501.3400   \n",
       "4                  0.760354              5.255419             1307.0479   \n",
       "...                     ...                   ...                   ...   \n",
       "1090               0.919273              5.926655              751.0461   \n",
       "1091               1.337016              8.914525              946.6073   \n",
       "1092               1.296249             10.637944             1066.8126   \n",
       "1093               0.974704             13.650818              971.7796   \n",
       "1094               0.473384              4.905297              427.4661   \n",
       "\n",
       "      v_compound_score_max  t_pol_score_mean  t_pol_score_std  \\\n",
       "0                  40.8155          0.470360         1.918625   \n",
       "1                1124.5091          0.878903         9.536864   \n",
       "2                 240.2550          1.087970        17.264859   \n",
       "3                 182.1425          0.361828         3.057769   \n",
       "4                 104.6719          0.445339         3.005284   \n",
       "...                    ...               ...              ...   \n",
       "1090               54.4425          0.336855         2.692034   \n",
       "1091              173.8547          0.532183         3.815865   \n",
       "1092              219.1752          0.729387         7.563631   \n",
       "1093              263.0880          0.689802         8.775038   \n",
       "1094               33.4815          0.276536         1.864274   \n",
       "\n",
       "      t_pol_score_sum  t_pol_score_max  t_sub_score_mean  t_sub_score_std  \\\n",
       "0          458.130558         24.70000          1.890892         3.280482   \n",
       "1         2478.505829        290.46875          3.178987        19.136392   \n",
       "2         1821.262586        456.00000          2.816607        15.955071   \n",
       "3          689.644645         76.87500          1.916536         4.794911   \n",
       "4          765.537930        102.20000          1.685690         4.534309   \n",
       "...               ...              ...               ...              ...   \n",
       "1090       275.210130         20.60000          2.223944         6.649812   \n",
       "1091       376.785520         75.00000          2.651977         9.292295   \n",
       "1092       600.285630        189.80000          2.335994         8.791418   \n",
       "1093       687.732899        261.00000          3.041377        13.345471   \n",
       "1094       249.711727         11.25000          1.890935         3.360099   \n",
       "\n",
       "      t_sub_score_sum  t_sub_score_max  v_neg_score_mean  v_neg_score_std  \\\n",
       "0         1841.728790        32.400000          0.285408         0.883918   \n",
       "1         8964.742385       826.867708          0.412010         2.455393   \n",
       "2         4714.999394       399.000000          0.404212         3.511884   \n",
       "3         3652.918407       110.797619          0.338716         1.856071   \n",
       "4         2897.701636        87.600000          0.256962         0.913094   \n",
       "...               ...              ...               ...              ...   \n",
       "1090      1816.962092       135.585000          0.375542         1.531212   \n",
       "1091      1877.599807       145.963889          0.348597         1.205841   \n",
       "1092      1922.523426       182.500000          0.284131         0.794803   \n",
       "1093      3032.252472       326.250000          0.509552         2.259901   \n",
       "1094      1707.514271        38.250000          0.326038         0.801756   \n",
       "\n",
       "      v_neg_score_sum  v_neg_score_max  date_count        Date         High  \\\n",
       "0             277.987           17.050         974  2017-01-01  1003.080017   \n",
       "1            1161.867           71.424        2820  2017-01-02  1031.390015   \n",
       "2             676.651          110.028        1674  2017-01-03  1044.079956   \n",
       "3             645.592           68.885        1906  2017-01-04  1159.420044   \n",
       "4             441.718           16.800        1719  2017-01-05  1191.099976   \n",
       "...               ...              ...         ...         ...          ...   \n",
       "1090          306.818           25.938         817  2019-12-27  7275.860000   \n",
       "1091          246.807           19.520         708  2019-12-28  7365.010000   \n",
       "1092          233.840            9.240         823  2019-12-29  7528.450000   \n",
       "1093          508.023           39.933         997  2019-12-30  7408.240000   \n",
       "1094          294.412           14.790         903  2019-12-31  7320.000000   \n",
       "\n",
       "              Low         Open        Close        Volume   tenkan_sen  \\\n",
       "0      958.698975   963.658020   998.325012  1.477750e+08   932.752014   \n",
       "1      996.702026   998.617004  1021.750000  2.221850e+08   946.907013   \n",
       "2     1021.599976  1021.599976  1043.839966  1.851680e+08   970.488983   \n",
       "3     1044.400024  1044.400024  1154.729980  3.449460e+08  1031.837524   \n",
       "4      910.416992  1156.729980  1013.380005  5.101990e+08  1050.758484   \n",
       "...           ...          ...          ...           ...          ...   \n",
       "1090  7076.420000  7202.000000  7254.740000  3.364270e+04  7366.845000   \n",
       "1091  7238.670000  7254.770000  7316.140000  2.684898e+04  7385.900000   \n",
       "1092  7288.000000  7315.360000  7388.240000  3.138711e+04  7385.900000   \n",
       "1093  7220.000000  7388.430000  7246.000000  2.960591e+04  7385.900000   \n",
       "1094  7145.010000  7246.000000  7195.230000  2.595445e+04  7385.900000   \n",
       "\n",
       "        kijun_sen  senkou_span_a  senkou_span_b  chikou_span  diff_kijun  \\\n",
       "0      881.415009     745.597763     704.654510   919.750000  116.910004   \n",
       "1      898.401001     746.116516     704.654510   921.590027  123.348999   \n",
       "2      904.745972     746.543015     704.654510   919.495972  139.093994   \n",
       "3      962.416016     752.111511     704.654510   920.382019  192.313965   \n",
       "4      978.255981     754.891266     704.777008   970.403015   35.124023   \n",
       "...           ...            ...            ...          ...         ...   \n",
       "1090  7092.500000    7580.227500    8442.500000  8682.360000  162.240000   \n",
       "1091  7092.500000    7563.750000    8442.500000  8404.520000  223.640000   \n",
       "1092  7092.500000    7535.250000    8442.500000  8439.000000  295.740000   \n",
       "1093  7065.190000    7588.047500    8442.500000  8340.580000  180.810000   \n",
       "1094  7065.190000    7588.047500    8442.500000  8615.000000  130.040000   \n",
       "\n",
       "      diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  chikou_signal  \\\n",
       "0       65.572998    78.575012             1              1              1   \n",
       "1       74.842987   100.159973             1              1              1   \n",
       "2       73.350983   124.343994             1              1              1   \n",
       "3      122.892456   234.347961             1              1              1   \n",
       "4      -37.378479    42.976990             0              0              0   \n",
       "...           ...          ...           ...            ...            ...   \n",
       "1090  -112.105000 -1427.620000             1             -1             -1   \n",
       "1091   -69.760000 -1088.380000             1             -1             -1   \n",
       "1092     2.340000 -1050.760000             1              0             -1   \n",
       "1093  -139.900000 -1094.580000             1             -1             -1   \n",
       "1094  -190.670000 -1419.770000             1             -1             -1   \n",
       "\n",
       "      indice  date_time  class  \n",
       "0          0 2017-01-01      1  \n",
       "1          0 2017-01-02      1  \n",
       "2          0 2017-01-03      1  \n",
       "3          0 2017-01-04      1  \n",
       "4          0 2017-01-05      0  \n",
       "...      ...        ...    ...  \n",
       "1090       0 2019-12-27      1  \n",
       "1091       0 2019-12-28      1  \n",
       "1092       0 2019-12-29      1  \n",
       "1093       0 2019-12-30      0  \n",
       "1094       0 2019-12-31      0  \n",
       "\n",
       "[1095 rows x 74 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-01-01 00:00:00')"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(\"2017-01-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq= pd.read_csv(\"DATA NASDAQ.csv\")\n",
    "s_pp= pd.read_csv(\"S&P DATA.csv\")\n",
    "s_pp.rename(columns={\" Close\":\"Close_ssp\"},inplace=True)\n",
    "nasdaq.rename(columns={\" Close\":\"Close_nasdaq\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close_ssp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06/08/22</td>\n",
       "      <td>4147.12</td>\n",
       "      <td>4160.14</td>\n",
       "      <td>4107.20</td>\n",
       "      <td>4115.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06/07/22</td>\n",
       "      <td>4096.47</td>\n",
       "      <td>4164.86</td>\n",
       "      <td>4080.19</td>\n",
       "      <td>4160.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06/06/22</td>\n",
       "      <td>4134.72</td>\n",
       "      <td>4168.78</td>\n",
       "      <td>4109.18</td>\n",
       "      <td>4121.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06/03/22</td>\n",
       "      <td>4137.57</td>\n",
       "      <td>4142.67</td>\n",
       "      <td>4098.67</td>\n",
       "      <td>4108.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06/02/22</td>\n",
       "      <td>4095.41</td>\n",
       "      <td>4177.51</td>\n",
       "      <td>4074.37</td>\n",
       "      <td>4176.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>01/08/13</td>\n",
       "      <td>1461.89</td>\n",
       "      <td>1461.89</td>\n",
       "      <td>1451.64</td>\n",
       "      <td>1457.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>01/07/13</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>1456.62</td>\n",
       "      <td>1461.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>01/04/13</td>\n",
       "      <td>1459.37</td>\n",
       "      <td>1467.94</td>\n",
       "      <td>1458.99</td>\n",
       "      <td>1466.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>01/03/13</td>\n",
       "      <td>1462.42</td>\n",
       "      <td>1465.47</td>\n",
       "      <td>1455.53</td>\n",
       "      <td>1459.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>01/02/13</td>\n",
       "      <td>1426.19</td>\n",
       "      <td>1462.43</td>\n",
       "      <td>1426.19</td>\n",
       "      <td>1462.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Open     High      Low  Close_ssp\n",
       "0     06/08/22  4147.12  4160.14  4107.20    4115.77\n",
       "1     06/07/22  4096.47  4164.86  4080.19    4160.68\n",
       "2     06/06/22  4134.72  4168.78  4109.18    4121.43\n",
       "3     06/03/22  4137.57  4142.67  4098.67    4108.54\n",
       "4     06/02/22  4095.41  4177.51  4074.37    4176.82\n",
       "...        ...      ...      ...      ...        ...\n",
       "2371  01/08/13  1461.89  1461.89  1451.64    1457.15\n",
       "2372  01/07/13  1466.47  1466.47  1456.62    1461.89\n",
       "2373  01/04/13  1459.37  1467.94  1458.99    1466.47\n",
       "2374  01/03/13  1462.42  1465.47  1455.53    1459.37\n",
       "2375  01/02/13  1426.19  1462.43  1426.19    1462.42\n",
       "\n",
       "[2376 rows x 5 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pp=s_pp[[\"Close_ssp\",\"Date\"]]\n",
    "nasdaq=nasdaq[[\"Close_nasdaq\",\"Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/1xvg6x991594dlshm5qcjr7w0000gn/T/ipykernel_23720/2962947602.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  s_pp[\"Date\"]=pd.to_datetime(s_pp[\"Date\"],format='%m/%d/%y')\n"
     ]
    }
   ],
   "source": [
    "s_pp[\"Date\"]=pd.to_datetime(s_pp[\"Date\"],format='%m/%d/%y')  \n",
    "nasdaq[\"Date\"]=pd.to_datetime(nasdaq[\"Date\"],format='%m/%d/%y') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range=pd.DataFrame(pd.date_range(start=\"2013-01-02\",end=\"2022-05-27\"),columns=[\"Date\"])\n",
    "date_df=date_range.merge(s_pp,on=\"Date\",how=\"left\")\n",
    "date_df_2=date_df.merge(nasdaq,on=\"Date\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close_ssp</th>\n",
       "      <th>Close_nasdaq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1462.42</td>\n",
       "      <td>3112.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1459.37</td>\n",
       "      <td>3100.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>3101.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>3101.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>1466.47</td>\n",
       "      <td>3101.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>3973.75</td>\n",
       "      <td>11535.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>3941.48</td>\n",
       "      <td>11264.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>3978.73</td>\n",
       "      <td>11434.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>4057.84</td>\n",
       "      <td>11740.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>4158.24</td>\n",
       "      <td>12131.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Close_ssp  Close_nasdaq\n",
       "0    2013-01-02    1462.42       3112.26\n",
       "1    2013-01-03    1459.37       3100.57\n",
       "2    2013-01-04    1466.47       3101.66\n",
       "3    2013-01-05    1466.47       3101.66\n",
       "4    2013-01-06    1466.47       3101.66\n",
       "...         ...        ...           ...\n",
       "3428 2022-05-23    3973.75      11535.27\n",
       "3429 2022-05-24    3941.48      11264.45\n",
       "3430 2022-05-25    3978.73      11434.74\n",
       "3431 2022-05-26    4057.84      11740.65\n",
       "3432 2022-05-27    4158.24      12131.13\n",
       "\n",
       "[3433 rows x 3 columns]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "date_df_2=date_df_2.fillna(method=\"ffill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_reddit = date_df_2.merge(daily, left_on=\"Date\",right_on=\"date_time\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>147.488007</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.444000</td>\n",
       "      <td>144.539993</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.979996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.559998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.479996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.520004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.744995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-12.754997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23.790001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.300003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-34.550003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>29886.640000</td>\n",
       "      <td>28019.560000</td>\n",
       "      <td>29542.140000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>94581.65463</td>\n",
       "      <td>29398.445</td>\n",
       "      <td>33361.885</td>\n",
       "      <td>40788.4275</td>\n",
       "      <td>42872.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4160.535</td>\n",
       "      <td>-197.095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>29397.660000</td>\n",
       "      <td>28282.900000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>28629.800000</td>\n",
       "      <td>90998.52010</td>\n",
       "      <td>29398.445</td>\n",
       "      <td>33361.885</td>\n",
       "      <td>40269.3025</td>\n",
       "      <td>42788.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4732.085</td>\n",
       "      <td>-768.645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>29266.000000</td>\n",
       "      <td>28450.000000</td>\n",
       "      <td>28629.810000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>34479.35127</td>\n",
       "      <td>29398.445</td>\n",
       "      <td>33361.885</td>\n",
       "      <td>39885.1725</td>\n",
       "      <td>42788.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4330.555</td>\n",
       "      <td>-367.115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>29587.780000</td>\n",
       "      <td>28839.210000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>27567.34764</td>\n",
       "      <td>29345.035</td>\n",
       "      <td>33361.885</td>\n",
       "      <td>39885.1725</td>\n",
       "      <td>42788.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3893.785</td>\n",
       "      <td>123.065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>30928.710000</td>\n",
       "      <td>29299.620000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>30731.870000</td>\n",
       "      <td>31900.00955</td>\n",
       "      <td>29474.135</td>\n",
       "      <td>33272.755</td>\n",
       "      <td>39745.0925</td>\n",
       "      <td>42788.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2540.885</td>\n",
       "      <td>1257.735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3319 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date          High           Low          Open         Close  \\\n",
       "0     2013-04-29    147.488007    134.000000    134.444000    144.539993   \n",
       "1     2013-04-30    146.929993    134.050003    144.000000    139.000000   \n",
       "2     2013-05-01    139.889999    107.720001    139.000000    116.989998   \n",
       "3     2013-05-02    125.599998     92.281898    116.379997    105.209999   \n",
       "4     2013-05-03    108.127998     79.099998    106.250000     97.750000   \n",
       "...          ...           ...           ...           ...           ...   \n",
       "3314  2022-05-26  29886.640000  28019.560000  29542.140000  29201.350000   \n",
       "3315  2022-05-27  29397.660000  28282.900000  29201.350000  28629.800000   \n",
       "3316  2022-05-28  29266.000000  28450.000000  28629.810000  29031.330000   \n",
       "3317  2022-05-29  29587.780000  28839.210000  29031.330000  29468.100000   \n",
       "3318  2022-05-30  30928.710000  29299.620000  29468.100000  30731.870000   \n",
       "\n",
       "           Volume  tenkan_sen  kijun_sen  senkou_span_a  senkou_span_b  \\\n",
       "0         0.00000         NaN        NaN            NaN            NaN   \n",
       "1         0.00000         NaN        NaN            NaN            NaN   \n",
       "2         0.00000         NaN        NaN            NaN            NaN   \n",
       "3         0.00000         NaN        NaN            NaN            NaN   \n",
       "4         0.00000         NaN        NaN            NaN            NaN   \n",
       "...           ...         ...        ...            ...            ...   \n",
       "3314  94581.65463   29398.445  33361.885     40788.4275       42872.42   \n",
       "3315  90998.52010   29398.445  33361.885     40269.3025       42788.11   \n",
       "3316  34479.35127   29398.445  33361.885     39885.1725       42788.11   \n",
       "3317  27567.34764   29345.035  33361.885     39885.1725       42788.11   \n",
       "3318  31900.00955   29474.135  33272.755     39745.0925       42788.11   \n",
       "\n",
       "      chikou_span  diff_kijun  diff_tenkan  diff_chikou  kijun_signal  \\\n",
       "0      131.979996         NaN          NaN    12.559998             0   \n",
       "1      133.479996         NaN          NaN     5.520004             0   \n",
       "2      129.744995         NaN          NaN   -12.754997             0   \n",
       "3      129.000000         NaN          NaN   -23.790001             0   \n",
       "4      132.300003         NaN          NaN   -34.550003             0   \n",
       "...           ...         ...          ...          ...           ...   \n",
       "3314          NaN   -4160.535     -197.095          NaN            -1   \n",
       "3315          NaN   -4732.085     -768.645          NaN            -1   \n",
       "3316          NaN   -4330.555     -367.115          NaN            -1   \n",
       "3317          NaN   -3893.785      123.065          NaN            -1   \n",
       "3318          NaN   -2540.885     1257.735          NaN            -1   \n",
       "\n",
       "      tenkan_signal  chikou_signal  indice  \n",
       "0                 0              0       0  \n",
       "1                 0              0       0  \n",
       "2                 0              0       0  \n",
       "3                 0              0       0  \n",
       "4                 0              0       0  \n",
       "...             ...            ...     ...  \n",
       "3314             -1              0       0  \n",
       "3315             -1              0       0  \n",
       "3316             -1              0       0  \n",
       "3317              1              0       0  \n",
       "3318              1              0       0  \n",
       "\n",
       "[3319 rows x 18 columns]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3319 entries, 0 to 3318\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    3319 non-null   object \n",
      " 1   High    3319 non-null   float64\n",
      " 2   Low     3319 non-null   float64\n",
      " 3   Open    3319 non-null   float64\n",
      " 4   Close   3319 non-null   float64\n",
      " 5   Volume  3319 non-null   float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 155.7+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_btc_df.info()\n",
    "daily_btc_df[\"Date\"]=pd.to_datetime(daily_btc_df[\"Date\"],format='%Y/%m/%d')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = merge_reddit.merge(daily_btc_df, left_on=\"Date_x\", right_on=\"Date\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={'High_y':'High_x',\n",
    "       'Low_y':'Low_x', 'Open_y':'Open_x', 'Close_y':'Close_x', 'Volume_y':\"Volume_x\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_x</th>\n",
       "      <th>Close_ssp</th>\n",
       "      <th>Close_nasdaq</th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date_y</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "      <th>class</th>\n",
       "      <th>Date</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>1593.61</td>\n",
       "      <td>3307.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>147.488007</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.444000</td>\n",
       "      <td>144.539993</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>1597.57</td>\n",
       "      <td>3328.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>1582.70</td>\n",
       "      <td>3299.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>1597.59</td>\n",
       "      <td>3340.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>1614.42</td>\n",
       "      <td>3378.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>4057.84</td>\n",
       "      <td>11740.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>29886.640000</td>\n",
       "      <td>28019.560000</td>\n",
       "      <td>29542.140000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>94581.65463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>4158.24</td>\n",
       "      <td>12131.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>29397.660000</td>\n",
       "      <td>28282.900000</td>\n",
       "      <td>29201.350000</td>\n",
       "      <td>28629.800000</td>\n",
       "      <td>90998.52010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-28</td>\n",
       "      <td>29266.000000</td>\n",
       "      <td>28450.000000</td>\n",
       "      <td>28629.810000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>34479.35127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>29587.780000</td>\n",
       "      <td>28839.210000</td>\n",
       "      <td>29031.330000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>27567.34764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>30928.710000</td>\n",
       "      <td>29299.620000</td>\n",
       "      <td>29468.100000</td>\n",
       "      <td>30731.870000</td>\n",
       "      <td>31900.00955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3319 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date_x  Close_ssp  Close_nasdaq date  score_mean  score_std  \\\n",
       "0    2013-04-29    1593.61       3307.02  NaN         NaN        NaN   \n",
       "1    2013-04-30    1597.57       3328.79  NaN         NaN        NaN   \n",
       "2    2013-05-01    1582.70       3299.13  NaN         NaN        NaN   \n",
       "3    2013-05-02    1597.59       3340.62  NaN         NaN        NaN   \n",
       "4    2013-05-03    1614.42       3378.63  NaN         NaN        NaN   \n",
       "...         ...        ...           ...  ...         ...        ...   \n",
       "3314 2022-05-26    4057.84      11740.65  NaN         NaN        NaN   \n",
       "3315 2022-05-27    4158.24      12131.13  NaN         NaN        NaN   \n",
       "3316        NaT        NaN           NaN  NaN         NaN        NaN   \n",
       "3317        NaT        NaN           NaN  NaN         NaN        NaN   \n",
       "3318        NaT        NaN           NaN  NaN         NaN        NaN   \n",
       "\n",
       "      score_sum  score_max  v_neg_mean  v_neg_std  v_neg_sum  v_neg_max  \\\n",
       "0           NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "1           NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "2           NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "3           NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "4           NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "...         ...        ...         ...        ...        ...        ...   \n",
       "3314        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "3315        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "3316        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "3317        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "3318        NaN        NaN         NaN        NaN        NaN        NaN   \n",
       "\n",
       "      v_neu_mean  v_neu_sum  v_neu_std  v_neu_max  v_pos_mean  v_pos_std  \\\n",
       "0            NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "1            NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "2            NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "3            NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "4            NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "...          ...        ...        ...        ...         ...        ...   \n",
       "3314         NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "3315         NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "3316         NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "3317         NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "3318         NaN        NaN        NaN        NaN         NaN        NaN   \n",
       "\n",
       "      v_pos_sum  v_pos_max  v_compound_mean  v_compound_std  v_compound_sum  \\\n",
       "0           NaN        NaN              NaN             NaN             NaN   \n",
       "1           NaN        NaN              NaN             NaN             NaN   \n",
       "2           NaN        NaN              NaN             NaN             NaN   \n",
       "3           NaN        NaN              NaN             NaN             NaN   \n",
       "4           NaN        NaN              NaN             NaN             NaN   \n",
       "...         ...        ...              ...             ...             ...   \n",
       "3314        NaN        NaN              NaN             NaN             NaN   \n",
       "3315        NaN        NaN              NaN             NaN             NaN   \n",
       "3316        NaN        NaN              NaN             NaN             NaN   \n",
       "3317        NaN        NaN              NaN             NaN             NaN   \n",
       "3318        NaN        NaN              NaN             NaN             NaN   \n",
       "\n",
       "      v_compound_max  t_pol_mean  t_pol_std  t_pol_sum  t_pol_max  t_sub_mean  \\\n",
       "0                NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "1                NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "2                NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "3                NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "4                NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "...              ...         ...        ...        ...        ...         ...   \n",
       "3314             NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "3315             NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "3316             NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "3317             NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "3318             NaN         NaN        NaN        NaN        NaN         NaN   \n",
       "\n",
       "      t_sub_std  t_sub_sum  t_sub_max  v_neu_score_mean  v_neu_score_sum  \\\n",
       "0           NaN        NaN        NaN               NaN              NaN   \n",
       "1           NaN        NaN        NaN               NaN              NaN   \n",
       "2           NaN        NaN        NaN               NaN              NaN   \n",
       "3           NaN        NaN        NaN               NaN              NaN   \n",
       "4           NaN        NaN        NaN               NaN              NaN   \n",
       "...         ...        ...        ...               ...              ...   \n",
       "3314        NaN        NaN        NaN               NaN              NaN   \n",
       "3315        NaN        NaN        NaN               NaN              NaN   \n",
       "3316        NaN        NaN        NaN               NaN              NaN   \n",
       "3317        NaN        NaN        NaN               NaN              NaN   \n",
       "3318        NaN        NaN        NaN               NaN              NaN   \n",
       "\n",
       "      v_neu_score_std  v_neu_score_max  v_pos_score_mean  v_pos_score_std  \\\n",
       "0                 NaN              NaN               NaN              NaN   \n",
       "1                 NaN              NaN               NaN              NaN   \n",
       "2                 NaN              NaN               NaN              NaN   \n",
       "3                 NaN              NaN               NaN              NaN   \n",
       "4                 NaN              NaN               NaN              NaN   \n",
       "...               ...              ...               ...              ...   \n",
       "3314              NaN              NaN               NaN              NaN   \n",
       "3315              NaN              NaN               NaN              NaN   \n",
       "3316              NaN              NaN               NaN              NaN   \n",
       "3317              NaN              NaN               NaN              NaN   \n",
       "3318              NaN              NaN               NaN              NaN   \n",
       "\n",
       "      v_pos_score_sum  v_pos_score_max  v_compound_score_mean  \\\n",
       "0                 NaN              NaN                    NaN   \n",
       "1                 NaN              NaN                    NaN   \n",
       "2                 NaN              NaN                    NaN   \n",
       "3                 NaN              NaN                    NaN   \n",
       "4                 NaN              NaN                    NaN   \n",
       "...               ...              ...                    ...   \n",
       "3314              NaN              NaN                    NaN   \n",
       "3315              NaN              NaN                    NaN   \n",
       "3316              NaN              NaN                    NaN   \n",
       "3317              NaN              NaN                    NaN   \n",
       "3318              NaN              NaN                    NaN   \n",
       "\n",
       "      v_compound_score_std  v_compound_score_sum  v_compound_score_max  \\\n",
       "0                      NaN                   NaN                   NaN   \n",
       "1                      NaN                   NaN                   NaN   \n",
       "2                      NaN                   NaN                   NaN   \n",
       "3                      NaN                   NaN                   NaN   \n",
       "4                      NaN                   NaN                   NaN   \n",
       "...                    ...                   ...                   ...   \n",
       "3314                   NaN                   NaN                   NaN   \n",
       "3315                   NaN                   NaN                   NaN   \n",
       "3316                   NaN                   NaN                   NaN   \n",
       "3317                   NaN                   NaN                   NaN   \n",
       "3318                   NaN                   NaN                   NaN   \n",
       "\n",
       "      t_pol_score_mean  t_pol_score_std  t_pol_score_sum  t_pol_score_max  \\\n",
       "0                  NaN              NaN              NaN              NaN   \n",
       "1                  NaN              NaN              NaN              NaN   \n",
       "2                  NaN              NaN              NaN              NaN   \n",
       "3                  NaN              NaN              NaN              NaN   \n",
       "4                  NaN              NaN              NaN              NaN   \n",
       "...                ...              ...              ...              ...   \n",
       "3314               NaN              NaN              NaN              NaN   \n",
       "3315               NaN              NaN              NaN              NaN   \n",
       "3316               NaN              NaN              NaN              NaN   \n",
       "3317               NaN              NaN              NaN              NaN   \n",
       "3318               NaN              NaN              NaN              NaN   \n",
       "\n",
       "      t_sub_score_mean  t_sub_score_std  t_sub_score_sum  t_sub_score_max  \\\n",
       "0                  NaN              NaN              NaN              NaN   \n",
       "1                  NaN              NaN              NaN              NaN   \n",
       "2                  NaN              NaN              NaN              NaN   \n",
       "3                  NaN              NaN              NaN              NaN   \n",
       "4                  NaN              NaN              NaN              NaN   \n",
       "...                ...              ...              ...              ...   \n",
       "3314               NaN              NaN              NaN              NaN   \n",
       "3315               NaN              NaN              NaN              NaN   \n",
       "3316               NaN              NaN              NaN              NaN   \n",
       "3317               NaN              NaN              NaN              NaN   \n",
       "3318               NaN              NaN              NaN              NaN   \n",
       "\n",
       "      v_neg_score_mean  v_neg_score_std  v_neg_score_sum  v_neg_score_max  \\\n",
       "0                  NaN              NaN              NaN              NaN   \n",
       "1                  NaN              NaN              NaN              NaN   \n",
       "2                  NaN              NaN              NaN              NaN   \n",
       "3                  NaN              NaN              NaN              NaN   \n",
       "4                  NaN              NaN              NaN              NaN   \n",
       "...                ...              ...              ...              ...   \n",
       "3314               NaN              NaN              NaN              NaN   \n",
       "3315               NaN              NaN              NaN              NaN   \n",
       "3316               NaN              NaN              NaN              NaN   \n",
       "3317               NaN              NaN              NaN              NaN   \n",
       "3318               NaN              NaN              NaN              NaN   \n",
       "\n",
       "      date_count Date_y  High_x  Low_x  Open_x  Close_x  Volume_x  tenkan_sen  \\\n",
       "0            NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "1            NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "2            NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "3            NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "4            NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "...          ...    ...     ...    ...     ...      ...       ...         ...   \n",
       "3314         NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "3315         NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "3316         NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "3317         NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "3318         NaN    NaN     NaN    NaN     NaN      NaN       NaN         NaN   \n",
       "\n",
       "      kijun_sen  senkou_span_a  senkou_span_b  chikou_span  diff_kijun  \\\n",
       "0           NaN            NaN            NaN          NaN         NaN   \n",
       "1           NaN            NaN            NaN          NaN         NaN   \n",
       "2           NaN            NaN            NaN          NaN         NaN   \n",
       "3           NaN            NaN            NaN          NaN         NaN   \n",
       "4           NaN            NaN            NaN          NaN         NaN   \n",
       "...         ...            ...            ...          ...         ...   \n",
       "3314        NaN            NaN            NaN          NaN         NaN   \n",
       "3315        NaN            NaN            NaN          NaN         NaN   \n",
       "3316        NaN            NaN            NaN          NaN         NaN   \n",
       "3317        NaN            NaN            NaN          NaN         NaN   \n",
       "3318        NaN            NaN            NaN          NaN         NaN   \n",
       "\n",
       "      diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  chikou_signal  \\\n",
       "0             NaN          NaN           NaN            NaN            NaN   \n",
       "1             NaN          NaN           NaN            NaN            NaN   \n",
       "2             NaN          NaN           NaN            NaN            NaN   \n",
       "3             NaN          NaN           NaN            NaN            NaN   \n",
       "4             NaN          NaN           NaN            NaN            NaN   \n",
       "...           ...          ...           ...            ...            ...   \n",
       "3314          NaN          NaN           NaN            NaN            NaN   \n",
       "3315          NaN          NaN           NaN            NaN            NaN   \n",
       "3316          NaN          NaN           NaN            NaN            NaN   \n",
       "3317          NaN          NaN           NaN            NaN            NaN   \n",
       "3318          NaN          NaN           NaN            NaN            NaN   \n",
       "\n",
       "      indice date_time  class       Date        High_x         Low_x  \\\n",
       "0        NaN       NaT    NaN 2013-04-29    147.488007    134.000000   \n",
       "1        NaN       NaT    NaN 2013-04-30    146.929993    134.050003   \n",
       "2        NaN       NaT    NaN 2013-05-01    139.889999    107.720001   \n",
       "3        NaN       NaT    NaN 2013-05-02    125.599998     92.281898   \n",
       "4        NaN       NaT    NaN 2013-05-03    108.127998     79.099998   \n",
       "...      ...       ...    ...        ...           ...           ...   \n",
       "3314     NaN       NaT    NaN 2022-05-26  29886.640000  28019.560000   \n",
       "3315     NaN       NaT    NaN 2022-05-27  29397.660000  28282.900000   \n",
       "3316     NaN       NaT    NaN 2022-05-28  29266.000000  28450.000000   \n",
       "3317     NaN       NaT    NaN 2022-05-29  29587.780000  28839.210000   \n",
       "3318     NaN       NaT    NaN 2022-05-30  30928.710000  29299.620000   \n",
       "\n",
       "            Open_x       Close_x     Volume_x  \n",
       "0       134.444000    144.539993      0.00000  \n",
       "1       144.000000    139.000000      0.00000  \n",
       "2       139.000000    116.989998      0.00000  \n",
       "3       116.379997    105.209999      0.00000  \n",
       "4       106.250000     97.750000      0.00000  \n",
       "...            ...           ...          ...  \n",
       "3314  29542.140000  29201.350000  94581.65463  \n",
       "3315  29201.350000  28629.800000  90998.52010  \n",
       "3316  28629.810000  29031.330000  34479.35127  \n",
       "3317  29031.330000  29468.100000  27567.34764  \n",
       "3318  29468.100000  30731.870000  31900.00955  \n",
       "\n",
       "[3319 rows x 83 columns]"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date_x', 'Close_ssp', 'Close_nasdaq', 'date', 'score_mean',\n",
       "       'score_std', 'score_sum', 'score_max', 'v_neg_mean', 'v_neg_std',\n",
       "       'v_neg_sum', 'v_neg_max', 'v_neu_mean', 'v_neu_sum', 'v_neu_std',\n",
       "       'v_neu_max', 'v_pos_mean', 'v_pos_std', 'v_pos_sum', 'v_pos_max',\n",
       "       'v_compound_mean', 'v_compound_std', 'v_compound_sum', 'v_compound_max',\n",
       "       't_pol_mean', 't_pol_std', 't_pol_sum', 't_pol_max', 't_sub_mean',\n",
       "       't_sub_std', 't_sub_sum', 't_sub_max', 'v_neu_score_mean',\n",
       "       'v_neu_score_sum', 'v_neu_score_std', 'v_neu_score_max',\n",
       "       'v_pos_score_mean', 'v_pos_score_std', 'v_pos_score_sum',\n",
       "       'v_pos_score_max', 'v_compound_score_mean', 'v_compound_score_std',\n",
       "       'v_compound_score_sum', 'v_compound_score_max', 't_pol_score_mean',\n",
       "       't_pol_score_std', 't_pol_score_sum', 't_pol_score_max',\n",
       "       't_sub_score_mean', 't_sub_score_std', 't_sub_score_sum',\n",
       "       't_sub_score_max', 'v_neg_score_mean', 'v_neg_score_std',\n",
       "       'v_neg_score_sum', 'v_neg_score_max', 'date_count', 'Date_y', 'High_x',\n",
       "       'Low_x', 'Open_x', 'Close_x', 'Volume_x', 'tenkan_sen', 'kijun_sen',\n",
       "       'senkou_span_a', 'senkou_span_b', 'chikou_span', 'diff_kijun',\n",
       "       'diff_tenkan', 'diff_chikou', 'kijun_signal', 'tenkan_signal',\n",
       "       'chikou_signal', 'indice', 'date_time', 'class', 'Date', 'High_y',\n",
       "       'Low_y', 'Open_y', 'Close_y', 'Volume_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range=pd.DataFrame(pd.date_range(start=\"2017-08-21\",end=\"2022-05-27\"),columns=[\"date_time\"])\n",
    "date_df=date_range.merge(s_pp,on=\"date_time\",how=\"left\")\n",
    "date_df_2=date_df.merge(nasdaq,on=\"date_time\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Date_x</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Date_y</th>\n",
       "      <th>Open_y</th>\n",
       "      <th>High_y</th>\n",
       "      <th>Low_y</th>\n",
       "      <th>Close_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>08/21/17</td>\n",
       "      <td>2425.50</td>\n",
       "      <td>2430.58</td>\n",
       "      <td>2417.35</td>\n",
       "      <td>2428.37</td>\n",
       "      <td>08/21/17</td>\n",
       "      <td>6216.32</td>\n",
       "      <td>6226.93</td>\n",
       "      <td>6177.18</td>\n",
       "      <td>6213.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>08/22/17</td>\n",
       "      <td>2433.75</td>\n",
       "      <td>2454.77</td>\n",
       "      <td>2433.67</td>\n",
       "      <td>2452.51</td>\n",
       "      <td>08/22/17</td>\n",
       "      <td>6241.21</td>\n",
       "      <td>6302.84</td>\n",
       "      <td>6241.21</td>\n",
       "      <td>6297.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-23</td>\n",
       "      <td>08/23/17</td>\n",
       "      <td>2444.88</td>\n",
       "      <td>2448.91</td>\n",
       "      <td>2441.42</td>\n",
       "      <td>2444.04</td>\n",
       "      <td>08/23/17</td>\n",
       "      <td>6263.47</td>\n",
       "      <td>6291.30</td>\n",
       "      <td>6263.28</td>\n",
       "      <td>6278.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>08/24/17</td>\n",
       "      <td>2447.91</td>\n",
       "      <td>2450.39</td>\n",
       "      <td>2436.19</td>\n",
       "      <td>2438.97</td>\n",
       "      <td>08/24/17</td>\n",
       "      <td>6294.82</td>\n",
       "      <td>6302.84</td>\n",
       "      <td>6244.57</td>\n",
       "      <td>6271.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>08/25/17</td>\n",
       "      <td>2444.72</td>\n",
       "      <td>2453.96</td>\n",
       "      <td>2442.22</td>\n",
       "      <td>2443.05</td>\n",
       "      <td>08/25/17</td>\n",
       "      <td>6293.81</td>\n",
       "      <td>6308.72</td>\n",
       "      <td>6257.10</td>\n",
       "      <td>6265.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>05/23/22</td>\n",
       "      <td>3919.42</td>\n",
       "      <td>3981.88</td>\n",
       "      <td>3909.04</td>\n",
       "      <td>3973.75</td>\n",
       "      <td>05/23/22</td>\n",
       "      <td>11396.28</td>\n",
       "      <td>11552.07</td>\n",
       "      <td>11304.56</td>\n",
       "      <td>11535.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>05/24/22</td>\n",
       "      <td>3942.94</td>\n",
       "      <td>3955.68</td>\n",
       "      <td>3875.13</td>\n",
       "      <td>3941.48</td>\n",
       "      <td>05/24/22</td>\n",
       "      <td>11326.44</td>\n",
       "      <td>11351.61</td>\n",
       "      <td>11092.48</td>\n",
       "      <td>11264.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>05/25/22</td>\n",
       "      <td>3929.59</td>\n",
       "      <td>3999.33</td>\n",
       "      <td>3925.03</td>\n",
       "      <td>3978.73</td>\n",
       "      <td>05/25/22</td>\n",
       "      <td>11225.03</td>\n",
       "      <td>11511.90</td>\n",
       "      <td>11211.85</td>\n",
       "      <td>11434.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>05/26/22</td>\n",
       "      <td>3984.60</td>\n",
       "      <td>4075.14</td>\n",
       "      <td>3984.60</td>\n",
       "      <td>4057.84</td>\n",
       "      <td>05/26/22</td>\n",
       "      <td>11409.84</td>\n",
       "      <td>11796.97</td>\n",
       "      <td>11406.16</td>\n",
       "      <td>11740.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>05/27/22</td>\n",
       "      <td>4077.25</td>\n",
       "      <td>4158.49</td>\n",
       "      <td>4077.43</td>\n",
       "      <td>4158.24</td>\n",
       "      <td>05/27/22</td>\n",
       "      <td>11869.69</td>\n",
       "      <td>12131.66</td>\n",
       "      <td>11856.82</td>\n",
       "      <td>12131.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1741 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_time    Date_x   Open_x   High_x    Low_x   Close_x    Date_y  \\\n",
       "0    2017-08-21  08/21/17  2425.50  2430.58  2417.35   2428.37  08/21/17   \n",
       "1    2017-08-22  08/22/17  2433.75  2454.77  2433.67   2452.51  08/22/17   \n",
       "2    2017-08-23  08/23/17  2444.88  2448.91  2441.42   2444.04  08/23/17   \n",
       "3    2017-08-24  08/24/17  2447.91  2450.39  2436.19   2438.97  08/24/17   \n",
       "4    2017-08-25  08/25/17  2444.72  2453.96  2442.22   2443.05  08/25/17   \n",
       "...         ...       ...      ...      ...      ...       ...       ...   \n",
       "1736 2022-05-23  05/23/22  3919.42  3981.88  3909.04   3973.75  05/23/22   \n",
       "1737 2022-05-24  05/24/22  3942.94  3955.68  3875.13   3941.48  05/24/22   \n",
       "1738 2022-05-25  05/25/22  3929.59  3999.33  3925.03   3978.73  05/25/22   \n",
       "1739 2022-05-26  05/26/22  3984.60  4075.14  3984.60   4057.84  05/26/22   \n",
       "1740 2022-05-27  05/27/22  4077.25  4158.49  4077.43   4158.24  05/27/22   \n",
       "\n",
       "        Open_y    High_y     Low_y   Close_y  \n",
       "0      6216.32   6226.93   6177.18   6213.13  \n",
       "1      6241.21   6302.84   6241.21   6297.48  \n",
       "2      6263.47   6291.30   6263.28   6278.41  \n",
       "3      6294.82   6302.84   6244.57   6271.33  \n",
       "4      6293.81   6308.72   6257.10   6265.64  \n",
       "...        ...       ...       ...       ...  \n",
       "1736  11396.28  11552.07  11304.56  11535.27  \n",
       "1737  11326.44  11351.61  11092.48  11264.45  \n",
       "1738  11225.03  11511.90  11211.85  11434.74  \n",
       "1739  11409.84  11796.97  11406.16  11740.65  \n",
       "1740  11869.69  12131.66  11856.82  12131.13  \n",
       "\n",
       "[1741 rows x 11 columns]"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06/08/22</td>\n",
       "      <td>4147.12</td>\n",
       "      <td>4160.14</td>\n",
       "      <td>4107.20</td>\n",
       "      <td>4115.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06/07/22</td>\n",
       "      <td>4096.47</td>\n",
       "      <td>4164.86</td>\n",
       "      <td>4080.19</td>\n",
       "      <td>4160.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06/06/22</td>\n",
       "      <td>4134.72</td>\n",
       "      <td>4168.78</td>\n",
       "      <td>4109.18</td>\n",
       "      <td>4121.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06/03/22</td>\n",
       "      <td>4137.57</td>\n",
       "      <td>4142.67</td>\n",
       "      <td>4098.67</td>\n",
       "      <td>4108.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06/02/22</td>\n",
       "      <td>4095.41</td>\n",
       "      <td>4177.51</td>\n",
       "      <td>4074.37</td>\n",
       "      <td>4176.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>01/09/17</td>\n",
       "      <td>2273.59</td>\n",
       "      <td>2275.49</td>\n",
       "      <td>2268.90</td>\n",
       "      <td>2268.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>01/06/17</td>\n",
       "      <td>2271.14</td>\n",
       "      <td>2282.10</td>\n",
       "      <td>2264.06</td>\n",
       "      <td>2276.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>01/05/17</td>\n",
       "      <td>2268.18</td>\n",
       "      <td>2271.50</td>\n",
       "      <td>2260.45</td>\n",
       "      <td>2269.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>01/04/17</td>\n",
       "      <td>2261.60</td>\n",
       "      <td>2272.82</td>\n",
       "      <td>2261.60</td>\n",
       "      <td>2270.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>01/03/17</td>\n",
       "      <td>2251.57</td>\n",
       "      <td>2263.88</td>\n",
       "      <td>2245.13</td>\n",
       "      <td>2257.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1368 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Open     High      Low    Close\n",
       "0     06/08/22  4147.12  4160.14  4107.20  4115.77\n",
       "1     06/07/22  4096.47  4164.86  4080.19  4160.68\n",
       "2     06/06/22  4134.72  4168.78  4109.18  4121.43\n",
       "3     06/03/22  4137.57  4142.67  4098.67  4108.54\n",
       "4     06/02/22  4095.41  4177.51  4074.37  4176.82\n",
       "...        ...      ...      ...      ...      ...\n",
       "1363  01/09/17  2273.59  2275.49  2268.90  2268.90\n",
       "1364  01/06/17  2271.14  2282.10  2264.06  2276.98\n",
       "1365  01/05/17  2268.18  2271.50  2260.45  2269.00\n",
       "1366  01/04/17  2261.60  2272.82  2261.60  2270.75\n",
       "1367  01/03/17  2251.57  2263.88  2245.13  2257.83\n",
       "\n",
       "[1368 rows x 5 columns]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq= pd.read_csv(\"HistoricalPrices (2).csv\")\n",
    "s_pp= pd.read_csv(\"HistoricalPrices (3).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pp_close = s_pp[[\" Close\",\"Date\"]].copy()\n",
    "s_nasdaq_close = nasdaq[[\" Close\",\"Date\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pp_close[\"date_time\"]=pd.to_datetime(s_pp_close[\"Date\"],format='%m/%d/%y')  \n",
    "nasdaq[\"date_time\"]=pd.to_datetime(nasdaq[\"Date\"],format='%m/%d/%y')  \n",
    "\n",
    "s_pp_close.rename(columns={\" Close\":\"Close_ssp\", \"date_time\":\"date_time_ssp\"},inplace=True)\n",
    "nasdaq.rename(columns={\" Close\":\"Close_nasdaq\", \"date_time\":\"date_time_nasdaq\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = s_pp_close[[\"Close_ssp\",\"date_time_ssp\"]].merge(nasdaq[[\"Close_nasdaq\",\"date_time_nasdaq\"]], left_on=\"date_time_ssp\",right_on=\"date_time_nasdaq\",how=\"inner\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>8.406436</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>59.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>807.596</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.091042</td>\n",
       "      <td>106.729</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>241.5044</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.202279</td>\n",
       "      <td>100.300454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>0.221794</td>\n",
       "      <td>436.318864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>3606.952</td>\n",
       "      <td>6.537687</td>\n",
       "      <td>92.950</td>\n",
       "      <td>0.540065</td>\n",
       "      <td>1.899358</td>\n",
       "      <td>526.023</td>\n",
       "      <td>37.584</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>4.894707</td>\n",
       "      <td>912.0243</td>\n",
       "      <td>40.8155</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>1.918625</td>\n",
       "      <td>458.130558</td>\n",
       "      <td>24.70000</td>\n",
       "      <td>1.890892</td>\n",
       "      <td>3.280482</td>\n",
       "      <td>1841.728790</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.285408</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>277.987</td>\n",
       "      <td>17.050</td>\n",
       "      <td>974</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1003.080017</td>\n",
       "      <td>958.698975</td>\n",
       "      <td>963.658020</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1.477750e+08</td>\n",
       "      <td>932.752014</td>\n",
       "      <td>881.415009</td>\n",
       "      <td>745.597763</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>116.910004</td>\n",
       "      <td>65.572998</td>\n",
       "      <td>78.575012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>44.739341</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>173.499</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>2340.209</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>306.287</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.569902</td>\n",
       "      <td>679.5298</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.186729</td>\n",
       "      <td>269.620143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>0.214772</td>\n",
       "      <td>1253.644989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.206413</td>\n",
       "      <td>17502.086</td>\n",
       "      <td>39.521719</td>\n",
       "      <td>1713.998</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>5.131474</td>\n",
       "      <td>2405.074</td>\n",
       "      <td>128.940</td>\n",
       "      <td>2.127162</td>\n",
       "      <td>25.927811</td>\n",
       "      <td>5998.5965</td>\n",
       "      <td>1124.5091</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>9.536864</td>\n",
       "      <td>2478.505829</td>\n",
       "      <td>290.46875</td>\n",
       "      <td>3.178987</td>\n",
       "      <td>19.136392</td>\n",
       "      <td>8964.742385</td>\n",
       "      <td>826.867708</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>2.455393</td>\n",
       "      <td>1161.867</td>\n",
       "      <td>71.424</td>\n",
       "      <td>2820</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1031.390015</td>\n",
       "      <td>996.702026</td>\n",
       "      <td>998.617004</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>2.221850e+08</td>\n",
       "      <td>946.907013</td>\n",
       "      <td>898.401001</td>\n",
       "      <td>746.116516</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>921.590027</td>\n",
       "      <td>123.348999</td>\n",
       "      <td>74.842987</td>\n",
       "      <td>100.159973</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>29.575341</td>\n",
       "      <td>9814.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>101.066</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>1389.482</td>\n",
       "      <td>0.104272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.087896</td>\n",
       "      <td>183.454</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>433.3606</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>169.272860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>756.365240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.851201</td>\n",
       "      <td>8120.911</td>\n",
       "      <td>22.470165</td>\n",
       "      <td>457.140</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>1356.676</td>\n",
       "      <td>189.144</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>15.705079</td>\n",
       "      <td>1527.9880</td>\n",
       "      <td>240.2550</td>\n",
       "      <td>1.087970</td>\n",
       "      <td>17.264859</td>\n",
       "      <td>1821.262586</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>2.816607</td>\n",
       "      <td>15.955071</td>\n",
       "      <td>4714.999394</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.404212</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>676.651</td>\n",
       "      <td>110.028</td>\n",
       "      <td>1674</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1044.079956</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1.851680e+08</td>\n",
       "      <td>970.488983</td>\n",
       "      <td>904.745972</td>\n",
       "      <td>746.543015</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.495972</td>\n",
       "      <td>139.093994</td>\n",
       "      <td>73.350983</td>\n",
       "      <td>124.343994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>17.128804</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>125.065</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>1571.329</td>\n",
       "      <td>0.106123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>209.624</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>433.0382</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.206345</td>\n",
       "      <td>191.342678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>873.635908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.674829</td>\n",
       "      <td>7004.225</td>\n",
       "      <td>13.556661</td>\n",
       "      <td>460.631</td>\n",
       "      <td>0.504505</td>\n",
       "      <td>2.465480</td>\n",
       "      <td>961.587</td>\n",
       "      <td>68.885</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>7.153452</td>\n",
       "      <td>1501.3400</td>\n",
       "      <td>182.1425</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>3.057769</td>\n",
       "      <td>689.644645</td>\n",
       "      <td>76.87500</td>\n",
       "      <td>1.916536</td>\n",
       "      <td>4.794911</td>\n",
       "      <td>3652.918407</td>\n",
       "      <td>110.797619</td>\n",
       "      <td>0.338716</td>\n",
       "      <td>1.856071</td>\n",
       "      <td>645.592</td>\n",
       "      <td>68.885</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1159.420044</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>3.449460e+08</td>\n",
       "      <td>1031.837524</td>\n",
       "      <td>962.416016</td>\n",
       "      <td>752.111511</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>920.382019</td>\n",
       "      <td>192.313965</td>\n",
       "      <td>122.892456</td>\n",
       "      <td>234.347961</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>8.126814</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>103.742</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>1430.305</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>184.961</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.552156</td>\n",
       "      <td>419.5827</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>180.724524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>0.229343</td>\n",
       "      <td>751.068731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987909</td>\n",
       "      <td>5136.216</td>\n",
       "      <td>5.895492</td>\n",
       "      <td>89.270</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>2.164643</td>\n",
       "      <td>784.089</td>\n",
       "      <td>61.320</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>5.255419</td>\n",
       "      <td>1307.0479</td>\n",
       "      <td>104.6719</td>\n",
       "      <td>0.445339</td>\n",
       "      <td>3.005284</td>\n",
       "      <td>765.537930</td>\n",
       "      <td>102.20000</td>\n",
       "      <td>1.685690</td>\n",
       "      <td>4.534309</td>\n",
       "      <td>2897.701636</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>0.256962</td>\n",
       "      <td>0.913094</td>\n",
       "      <td>441.718</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1719</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1191.099976</td>\n",
       "      <td>910.416992</td>\n",
       "      <td>1156.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>5.101990e+08</td>\n",
       "      <td>1050.758484</td>\n",
       "      <td>978.255981</td>\n",
       "      <td>754.891266</td>\n",
       "      <td>704.777008</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>35.124023</td>\n",
       "      <td>-37.378479</td>\n",
       "      <td>42.976990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>12.858644</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.077027</td>\n",
       "      <td>53.837</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>676.381</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.093740</td>\n",
       "      <td>86.777</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.569012</td>\n",
       "      <td>157.6007</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.209669</td>\n",
       "      <td>76.605181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>358.376961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.997778</td>\n",
       "      <td>3266.185</td>\n",
       "      <td>11.091972</td>\n",
       "      <td>207.000</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>1.227200</td>\n",
       "      <td>436.993</td>\n",
       "      <td>13.542</td>\n",
       "      <td>0.919273</td>\n",
       "      <td>5.926655</td>\n",
       "      <td>751.0461</td>\n",
       "      <td>54.4425</td>\n",
       "      <td>0.336855</td>\n",
       "      <td>2.692034</td>\n",
       "      <td>275.210130</td>\n",
       "      <td>20.60000</td>\n",
       "      <td>2.223944</td>\n",
       "      <td>6.649812</td>\n",
       "      <td>1816.962092</td>\n",
       "      <td>135.585000</td>\n",
       "      <td>0.375542</td>\n",
       "      <td>1.531212</td>\n",
       "      <td>306.818</td>\n",
       "      <td>25.938</td>\n",
       "      <td>817</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>7275.860000</td>\n",
       "      <td>7076.420000</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>3.364270e+04</td>\n",
       "      <td>7366.845000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7580.227500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8682.360000</td>\n",
       "      <td>162.240000</td>\n",
       "      <td>-112.105000</td>\n",
       "      <td>-1427.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>17.568946</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>44.191</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>590.619</td>\n",
       "      <td>0.105560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.089176</td>\n",
       "      <td>73.196</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.560205</td>\n",
       "      <td>142.7691</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.204129</td>\n",
       "      <td>69.411507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>0.231762</td>\n",
       "      <td>305.142919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.995823</td>\n",
       "      <td>3537.043</td>\n",
       "      <td>14.679252</td>\n",
       "      <td>252.057</td>\n",
       "      <td>0.651510</td>\n",
       "      <td>2.487959</td>\n",
       "      <td>461.269</td>\n",
       "      <td>40.950</td>\n",
       "      <td>1.337016</td>\n",
       "      <td>8.914525</td>\n",
       "      <td>946.6073</td>\n",
       "      <td>173.8547</td>\n",
       "      <td>0.532183</td>\n",
       "      <td>3.815865</td>\n",
       "      <td>376.785520</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>2.651977</td>\n",
       "      <td>9.292295</td>\n",
       "      <td>1877.599807</td>\n",
       "      <td>145.963889</td>\n",
       "      <td>0.348597</td>\n",
       "      <td>1.205841</td>\n",
       "      <td>246.807</td>\n",
       "      <td>19.520</td>\n",
       "      <td>708</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>7365.010000</td>\n",
       "      <td>7238.670000</td>\n",
       "      <td>7254.770000</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>2.684898e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7563.750000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8404.520000</td>\n",
       "      <td>223.640000</td>\n",
       "      <td>-69.760000</td>\n",
       "      <td>-1088.380000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>17.229820</td>\n",
       "      <td>3494.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>52.124</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>682.759</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>88.119</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>174.2284</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.209074</td>\n",
       "      <td>77.704314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>0.241058</td>\n",
       "      <td>359.889446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.338086</td>\n",
       "      <td>3570.245</td>\n",
       "      <td>13.412452</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>4.685706</td>\n",
       "      <td>550.055</td>\n",
       "      <td>121.472</td>\n",
       "      <td>1.296249</td>\n",
       "      <td>10.637944</td>\n",
       "      <td>1066.8126</td>\n",
       "      <td>219.1752</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>7.563631</td>\n",
       "      <td>600.285630</td>\n",
       "      <td>189.80000</td>\n",
       "      <td>2.335994</td>\n",
       "      <td>8.791418</td>\n",
       "      <td>1922.523426</td>\n",
       "      <td>182.500000</td>\n",
       "      <td>0.284131</td>\n",
       "      <td>0.794803</td>\n",
       "      <td>233.840</td>\n",
       "      <td>9.240</td>\n",
       "      <td>823</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>7528.450000</td>\n",
       "      <td>7288.000000</td>\n",
       "      <td>7315.360000</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>3.138711e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7535.250000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8439.000000</td>\n",
       "      <td>295.740000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>-1050.760000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>22.933473</td>\n",
       "      <td>5401.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.075380</td>\n",
       "      <td>61.556</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>831.227</td>\n",
       "      <td>0.111242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>104.218</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>229.9543</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>98.192138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.222258</td>\n",
       "      <td>426.880618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.615048</td>\n",
       "      <td>5598.203</td>\n",
       "      <td>18.445569</td>\n",
       "      <td>423.342</td>\n",
       "      <td>0.720659</td>\n",
       "      <td>3.736793</td>\n",
       "      <td>718.497</td>\n",
       "      <td>98.658</td>\n",
       "      <td>0.974704</td>\n",
       "      <td>13.650818</td>\n",
       "      <td>971.7796</td>\n",
       "      <td>263.0880</td>\n",
       "      <td>0.689802</td>\n",
       "      <td>8.775038</td>\n",
       "      <td>687.732899</td>\n",
       "      <td>261.00000</td>\n",
       "      <td>3.041377</td>\n",
       "      <td>13.345471</td>\n",
       "      <td>3032.252472</td>\n",
       "      <td>326.250000</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>2.259901</td>\n",
       "      <td>508.023</td>\n",
       "      <td>39.933</td>\n",
       "      <td>997</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>7408.240000</td>\n",
       "      <td>7220.000000</td>\n",
       "      <td>7388.430000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>2.960591e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8340.580000</td>\n",
       "      <td>180.810000</td>\n",
       "      <td>-139.900000</td>\n",
       "      <td>-1094.580000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>7.954076</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>65.366</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>737.583</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.098716</td>\n",
       "      <td>100.042</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.585552</td>\n",
       "      <td>142.7909</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.213328</td>\n",
       "      <td>80.145271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>0.232719</td>\n",
       "      <td>401.635726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.670367</td>\n",
       "      <td>3314.341</td>\n",
       "      <td>6.540559</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.899961</td>\n",
       "      <td>414.253</td>\n",
       "      <td>9.504</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>4.905297</td>\n",
       "      <td>427.4661</td>\n",
       "      <td>33.4815</td>\n",
       "      <td>0.276536</td>\n",
       "      <td>1.864274</td>\n",
       "      <td>249.711727</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>1.890935</td>\n",
       "      <td>3.360099</td>\n",
       "      <td>1707.514271</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>0.326038</td>\n",
       "      <td>0.801756</td>\n",
       "      <td>294.412</td>\n",
       "      <td>14.790</td>\n",
       "      <td>903</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>7320.000000</td>\n",
       "      <td>7145.010000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>7195.230000</td>\n",
       "      <td>2.595445e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8615.000000</td>\n",
       "      <td>130.040000</td>\n",
       "      <td>-190.670000</td>\n",
       "      <td>-1419.770000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  score_std  score_sum  score_max  v_neg_mean  \\\n",
       "0     2017-01-01    4.095483   8.406436     3989.0      116.0    0.061278   \n",
       "1     2017-01-02    7.143617  44.739341    20145.0     1859.0    0.061524   \n",
       "2     2017-01-03    5.862605  29.575341     9814.0      570.0    0.060374   \n",
       "3     2017-01-04    4.271773  17.128804     8142.0      599.0    0.065616   \n",
       "4     2017-01-05    3.549738   8.126814     6102.0      146.0    0.060350   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "1090  2019-12-27    3.924113  12.858644     3206.0      207.0    0.065896   \n",
       "1091  2019-12-28    5.111582  17.568946     3619.0      281.0    0.062417   \n",
       "1092  2019-12-29    4.245443  17.229820     3494.0      292.0    0.063334   \n",
       "1093  2019-12-30    5.417252  22.933473     5401.0      522.0    0.061741   \n",
       "1094  2019-12-31    3.635659   7.954076     3283.0      102.0    0.072388   \n",
       "\n",
       "      v_neg_std  v_neg_sum  v_neg_max  v_neu_mean  v_neu_sum  v_neu_std  \\\n",
       "0      0.074932     59.685      0.674    0.829154    807.596   0.108736   \n",
       "1      0.067844    173.499      0.531    0.829861   2340.209   0.101984   \n",
       "2      0.071586    101.066      0.579    0.830037   1389.482   0.104272   \n",
       "3      0.073589    125.065      0.582    0.824412   1571.329   0.106123   \n",
       "4      0.072556    103.742      0.565    0.832056   1430.305   0.109600   \n",
       "...         ...        ...        ...         ...        ...        ...   \n",
       "1090   0.077027     53.837      0.600    0.827884    676.381   0.113621   \n",
       "1091   0.074158     44.191      0.461    0.834208    590.619   0.105560   \n",
       "1092   0.069795     52.124      0.469    0.829598    682.759   0.108458   \n",
       "1093   0.075380     61.556      0.753    0.833728    831.227   0.111242   \n",
       "1094   0.070688     65.366      0.427    0.816814    737.583   0.112108   \n",
       "\n",
       "      v_neu_max  v_pos_mean  v_pos_std  v_pos_sum  v_pos_max  v_compound_mean  \\\n",
       "0           1.0    0.109578   0.091042    106.729      0.579         0.247951   \n",
       "1           1.0    0.108612   0.087565    306.287      0.674         0.240968   \n",
       "2           1.0    0.109590   0.087896    183.454      0.636         0.258877   \n",
       "3           1.0    0.109981   0.090355    209.624      0.735         0.227197   \n",
       "4           1.0    0.107598   0.092280    184.961      0.627         0.244085   \n",
       "...         ...         ...        ...        ...        ...              ...   \n",
       "1090        1.0    0.106214   0.093740     86.777      0.620         0.192902   \n",
       "1091        1.0    0.103384   0.089176     73.196      0.482         0.201651   \n",
       "1092        1.0    0.107070   0.091100     88.119      0.502         0.211699   \n",
       "1093        1.0    0.104532   0.092085    104.218      0.592         0.230646   \n",
       "1094        1.0    0.110788   0.098716    100.042      0.645         0.158129   \n",
       "\n",
       "      v_compound_std  v_compound_sum  v_compound_max  t_pol_mean  t_pol_std  \\\n",
       "0           0.550243        241.5044          0.9984    0.102978   0.202279   \n",
       "1           0.569902        679.5298          0.9997    0.095610   0.186729   \n",
       "2           0.560860        433.3606          0.9987    0.101119   0.209033   \n",
       "3           0.575732        433.0382          0.9998    0.100390   0.206345   \n",
       "4           0.552156        419.5827          0.9984    0.105134   0.199678   \n",
       "...              ...             ...             ...         ...        ...   \n",
       "1090        0.569012        157.6007          0.9979    0.093764   0.209669   \n",
       "1091        0.560205        142.7691          0.9988    0.098039   0.204129   \n",
       "1092        0.561760        174.2284          0.9979    0.094416   0.209074   \n",
       "1093        0.564663        229.9543          0.9979    0.098488   0.195556   \n",
       "1094        0.585552        142.7909          0.9980    0.088754   0.213328   \n",
       "\n",
       "       t_pol_sum  t_pol_max  t_sub_mean  t_sub_std    t_sub_sum  t_sub_max  \\\n",
       "0     100.300454        1.0    0.447966   0.221794   436.318864        1.0   \n",
       "1     269.620143        1.0    0.444555   0.214772  1253.644989        1.0   \n",
       "2     169.272860        1.0    0.451831   0.216360   756.365240        1.0   \n",
       "3     191.342678        1.0    0.458361   0.207941   873.635908        1.0   \n",
       "4     180.724524        1.0    0.436922   0.229343   751.068731        1.0   \n",
       "...          ...        ...         ...        ...          ...        ...   \n",
       "1090   76.605181        1.0    0.438650   0.241026   358.376961        1.0   \n",
       "1091   69.411507        1.0    0.430993   0.231762   305.142919        1.0   \n",
       "1092   77.704314        1.0    0.437290   0.241058   359.889446        1.0   \n",
       "1093   98.192138        1.0    0.428165   0.222258   426.880618        1.0   \n",
       "1094   80.145271        1.0    0.444779   0.232719   401.635726        1.0   \n",
       "\n",
       "      v_neu_score_mean  v_neu_score_sum  v_neu_score_std  v_neu_score_max  \\\n",
       "0             3.703236         3606.952         6.537687           92.950   \n",
       "1             6.206413        17502.086        39.521719         1713.998   \n",
       "2             4.851201         8120.911        22.470165          457.140   \n",
       "3             3.674829         7004.225        13.556661          460.631   \n",
       "4             2.987909         5136.216         5.895492           89.270   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          3.997778         3266.185        11.091972          207.000   \n",
       "1091          4.995823         3537.043        14.679252          252.057   \n",
       "1092          4.338086         3570.245        13.412452          229.000   \n",
       "1093          5.615048         5598.203        18.445569          423.342   \n",
       "1094          3.670367         3314.341         6.540559           90.000   \n",
       "\n",
       "      v_pos_score_mean  v_pos_score_std  v_pos_score_sum  v_pos_score_max  \\\n",
       "0             0.540065         1.899358          526.023           37.584   \n",
       "1             0.852863         5.131474         2405.074          128.940   \n",
       "2             0.810440         7.089782         1356.676          189.144   \n",
       "3             0.504505         2.465480          961.587           68.885   \n",
       "4             0.456131         2.164643          784.089           61.320   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          0.534875         1.227200          436.993           13.542   \n",
       "1091          0.651510         2.487959          461.269           40.950   \n",
       "1092          0.668354         4.685706          550.055          121.472   \n",
       "1093          0.720659         3.736793          718.497           98.658   \n",
       "1094          0.458752         0.899961          414.253            9.504   \n",
       "\n",
       "      v_compound_score_mean  v_compound_score_std  v_compound_score_sum  \\\n",
       "0                  0.936370              4.894707              912.0243   \n",
       "1                  2.127162             25.927811             5998.5965   \n",
       "2                  0.912777             15.705079             1527.9880   \n",
       "3                  0.787692              7.153452             1501.3400   \n",
       "4                  0.760354              5.255419             1307.0479   \n",
       "...                     ...                   ...                   ...   \n",
       "1090               0.919273              5.926655              751.0461   \n",
       "1091               1.337016              8.914525              946.6073   \n",
       "1092               1.296249             10.637944             1066.8126   \n",
       "1093               0.974704             13.650818              971.7796   \n",
       "1094               0.473384              4.905297              427.4661   \n",
       "\n",
       "      v_compound_score_max  t_pol_score_mean  t_pol_score_std  \\\n",
       "0                  40.8155          0.470360         1.918625   \n",
       "1                1124.5091          0.878903         9.536864   \n",
       "2                 240.2550          1.087970        17.264859   \n",
       "3                 182.1425          0.361828         3.057769   \n",
       "4                 104.6719          0.445339         3.005284   \n",
       "...                    ...               ...              ...   \n",
       "1090               54.4425          0.336855         2.692034   \n",
       "1091              173.8547          0.532183         3.815865   \n",
       "1092              219.1752          0.729387         7.563631   \n",
       "1093              263.0880          0.689802         8.775038   \n",
       "1094               33.4815          0.276536         1.864274   \n",
       "\n",
       "      t_pol_score_sum  t_pol_score_max  t_sub_score_mean  t_sub_score_std  \\\n",
       "0          458.130558         24.70000          1.890892         3.280482   \n",
       "1         2478.505829        290.46875          3.178987        19.136392   \n",
       "2         1821.262586        456.00000          2.816607        15.955071   \n",
       "3          689.644645         76.87500          1.916536         4.794911   \n",
       "4          765.537930        102.20000          1.685690         4.534309   \n",
       "...               ...              ...               ...              ...   \n",
       "1090       275.210130         20.60000          2.223944         6.649812   \n",
       "1091       376.785520         75.00000          2.651977         9.292295   \n",
       "1092       600.285630        189.80000          2.335994         8.791418   \n",
       "1093       687.732899        261.00000          3.041377        13.345471   \n",
       "1094       249.711727         11.25000          1.890935         3.360099   \n",
       "\n",
       "      t_sub_score_sum  t_sub_score_max  v_neg_score_mean  v_neg_score_std  \\\n",
       "0         1841.728790        32.400000          0.285408         0.883918   \n",
       "1         8964.742385       826.867708          0.412010         2.455393   \n",
       "2         4714.999394       399.000000          0.404212         3.511884   \n",
       "3         3652.918407       110.797619          0.338716         1.856071   \n",
       "4         2897.701636        87.600000          0.256962         0.913094   \n",
       "...               ...              ...               ...              ...   \n",
       "1090      1816.962092       135.585000          0.375542         1.531212   \n",
       "1091      1877.599807       145.963889          0.348597         1.205841   \n",
       "1092      1922.523426       182.500000          0.284131         0.794803   \n",
       "1093      3032.252472       326.250000          0.509552         2.259901   \n",
       "1094      1707.514271        38.250000          0.326038         0.801756   \n",
       "\n",
       "      v_neg_score_sum  v_neg_score_max  date_count        Date         High  \\\n",
       "0             277.987           17.050         974  2017-01-01  1003.080017   \n",
       "1            1161.867           71.424        2820  2017-01-02  1031.390015   \n",
       "2             676.651          110.028        1674  2017-01-03  1044.079956   \n",
       "3             645.592           68.885        1906  2017-01-04  1159.420044   \n",
       "4             441.718           16.800        1719  2017-01-05  1191.099976   \n",
       "...               ...              ...         ...         ...          ...   \n",
       "1090          306.818           25.938         817  2019-12-27  7275.860000   \n",
       "1091          246.807           19.520         708  2019-12-28  7365.010000   \n",
       "1092          233.840            9.240         823  2019-12-29  7528.450000   \n",
       "1093          508.023           39.933         997  2019-12-30  7408.240000   \n",
       "1094          294.412           14.790         903  2019-12-31  7320.000000   \n",
       "\n",
       "              Low         Open        Close        Volume   tenkan_sen  \\\n",
       "0      958.698975   963.658020   998.325012  1.477750e+08   932.752014   \n",
       "1      996.702026   998.617004  1021.750000  2.221850e+08   946.907013   \n",
       "2     1021.599976  1021.599976  1043.839966  1.851680e+08   970.488983   \n",
       "3     1044.400024  1044.400024  1154.729980  3.449460e+08  1031.837524   \n",
       "4      910.416992  1156.729980  1013.380005  5.101990e+08  1050.758484   \n",
       "...           ...          ...          ...           ...          ...   \n",
       "1090  7076.420000  7202.000000  7254.740000  3.364270e+04  7366.845000   \n",
       "1091  7238.670000  7254.770000  7316.140000  2.684898e+04  7385.900000   \n",
       "1092  7288.000000  7315.360000  7388.240000  3.138711e+04  7385.900000   \n",
       "1093  7220.000000  7388.430000  7246.000000  2.960591e+04  7385.900000   \n",
       "1094  7145.010000  7246.000000  7195.230000  2.595445e+04  7385.900000   \n",
       "\n",
       "        kijun_sen  senkou_span_a  senkou_span_b  chikou_span  diff_kijun  \\\n",
       "0      881.415009     745.597763     704.654510   919.750000  116.910004   \n",
       "1      898.401001     746.116516     704.654510   921.590027  123.348999   \n",
       "2      904.745972     746.543015     704.654510   919.495972  139.093994   \n",
       "3      962.416016     752.111511     704.654510   920.382019  192.313965   \n",
       "4      978.255981     754.891266     704.777008   970.403015   35.124023   \n",
       "...           ...            ...            ...          ...         ...   \n",
       "1090  7092.500000    7580.227500    8442.500000  8682.360000  162.240000   \n",
       "1091  7092.500000    7563.750000    8442.500000  8404.520000  223.640000   \n",
       "1092  7092.500000    7535.250000    8442.500000  8439.000000  295.740000   \n",
       "1093  7065.190000    7588.047500    8442.500000  8340.580000  180.810000   \n",
       "1094  7065.190000    7588.047500    8442.500000  8615.000000  130.040000   \n",
       "\n",
       "      diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  chikou_signal  \\\n",
       "0       65.572998    78.575012             1              1              1   \n",
       "1       74.842987   100.159973             1              1              1   \n",
       "2       73.350983   124.343994             1              1              1   \n",
       "3      122.892456   234.347961             1              1              1   \n",
       "4      -37.378479    42.976990             0              0              0   \n",
       "...           ...          ...           ...            ...            ...   \n",
       "1090  -112.105000 -1427.620000             1             -1             -1   \n",
       "1091   -69.760000 -1088.380000             1             -1             -1   \n",
       "1092     2.340000 -1050.760000             1              0             -1   \n",
       "1093  -139.900000 -1094.580000             1             -1             -1   \n",
       "1094  -190.670000 -1419.770000             1             -1             -1   \n",
       "\n",
       "      indice  date_time  class  \n",
       "0          0 2017-01-01      1  \n",
       "1          0 2017-01-02      1  \n",
       "2          0 2017-01-03      1  \n",
       "3          0 2017-01-04      1  \n",
       "4          0 2017-01-05      0  \n",
       "...      ...        ...    ...  \n",
       "1090       0 2019-12-27      1  \n",
       "1091       0 2019-12-28      1  \n",
       "1092       0 2019-12-29      1  \n",
       "1093       0 2019-12-30      0  \n",
       "1094       0 2019-12-31      0  \n",
       "\n",
       "[1095 rows x 74 columns]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_max</th>\n",
       "      <th>v_neg_mean</th>\n",
       "      <th>v_neg_std</th>\n",
       "      <th>v_neg_sum</th>\n",
       "      <th>v_neg_max</th>\n",
       "      <th>v_neu_mean</th>\n",
       "      <th>v_neu_sum</th>\n",
       "      <th>v_neu_std</th>\n",
       "      <th>v_neu_max</th>\n",
       "      <th>v_pos_mean</th>\n",
       "      <th>v_pos_std</th>\n",
       "      <th>v_pos_sum</th>\n",
       "      <th>v_pos_max</th>\n",
       "      <th>v_compound_mean</th>\n",
       "      <th>v_compound_std</th>\n",
       "      <th>v_compound_sum</th>\n",
       "      <th>v_compound_max</th>\n",
       "      <th>t_pol_mean</th>\n",
       "      <th>t_pol_std</th>\n",
       "      <th>t_pol_sum</th>\n",
       "      <th>t_pol_max</th>\n",
       "      <th>t_sub_mean</th>\n",
       "      <th>t_sub_std</th>\n",
       "      <th>t_sub_sum</th>\n",
       "      <th>t_sub_max</th>\n",
       "      <th>v_neu_score_mean</th>\n",
       "      <th>v_neu_score_sum</th>\n",
       "      <th>v_neu_score_std</th>\n",
       "      <th>v_neu_score_max</th>\n",
       "      <th>v_pos_score_mean</th>\n",
       "      <th>v_pos_score_std</th>\n",
       "      <th>v_pos_score_sum</th>\n",
       "      <th>v_pos_score_max</th>\n",
       "      <th>v_compound_score_mean</th>\n",
       "      <th>v_compound_score_std</th>\n",
       "      <th>v_compound_score_sum</th>\n",
       "      <th>v_compound_score_max</th>\n",
       "      <th>t_pol_score_mean</th>\n",
       "      <th>t_pol_score_std</th>\n",
       "      <th>t_pol_score_sum</th>\n",
       "      <th>t_pol_score_max</th>\n",
       "      <th>t_sub_score_mean</th>\n",
       "      <th>t_sub_score_std</th>\n",
       "      <th>t_sub_score_sum</th>\n",
       "      <th>t_sub_score_max</th>\n",
       "      <th>v_neg_score_mean</th>\n",
       "      <th>v_neg_score_std</th>\n",
       "      <th>v_neg_score_sum</th>\n",
       "      <th>v_neg_score_max</th>\n",
       "      <th>date_count</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>tenkan_sen</th>\n",
       "      <th>kijun_sen</th>\n",
       "      <th>senkou_span_a</th>\n",
       "      <th>senkou_span_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>diff_kijun</th>\n",
       "      <th>diff_tenkan</th>\n",
       "      <th>diff_chikou</th>\n",
       "      <th>kijun_signal</th>\n",
       "      <th>tenkan_signal</th>\n",
       "      <th>chikou_signal</th>\n",
       "      <th>indice</th>\n",
       "      <th>date_time</th>\n",
       "      <th>class</th>\n",
       "      <th>Close_ssp</th>\n",
       "      <th>Close_nasdaq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4.095483</td>\n",
       "      <td>8.406436</td>\n",
       "      <td>3989.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>59.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>807.596</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109578</td>\n",
       "      <td>0.091042</td>\n",
       "      <td>106.729</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.247951</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>241.5044</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.202279</td>\n",
       "      <td>100.300454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447966</td>\n",
       "      <td>0.221794</td>\n",
       "      <td>436.318864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>3606.952</td>\n",
       "      <td>6.537687</td>\n",
       "      <td>92.950</td>\n",
       "      <td>0.540065</td>\n",
       "      <td>1.899358</td>\n",
       "      <td>526.023</td>\n",
       "      <td>37.584</td>\n",
       "      <td>0.936370</td>\n",
       "      <td>4.894707</td>\n",
       "      <td>912.0243</td>\n",
       "      <td>40.8155</td>\n",
       "      <td>0.470360</td>\n",
       "      <td>1.918625</td>\n",
       "      <td>458.130558</td>\n",
       "      <td>24.70000</td>\n",
       "      <td>1.890892</td>\n",
       "      <td>3.280482</td>\n",
       "      <td>1841.728790</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.285408</td>\n",
       "      <td>0.883918</td>\n",
       "      <td>277.987</td>\n",
       "      <td>17.050</td>\n",
       "      <td>974</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1003.080017</td>\n",
       "      <td>958.698975</td>\n",
       "      <td>963.658020</td>\n",
       "      <td>998.325012</td>\n",
       "      <td>1.477750e+08</td>\n",
       "      <td>932.752014</td>\n",
       "      <td>881.415009</td>\n",
       "      <td>745.597763</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.750000</td>\n",
       "      <td>116.910004</td>\n",
       "      <td>65.572998</td>\n",
       "      <td>78.575012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>7.143617</td>\n",
       "      <td>44.739341</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>173.499</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>2340.209</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108612</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>306.287</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.569902</td>\n",
       "      <td>679.5298</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.095610</td>\n",
       "      <td>0.186729</td>\n",
       "      <td>269.620143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444555</td>\n",
       "      <td>0.214772</td>\n",
       "      <td>1253.644989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.206413</td>\n",
       "      <td>17502.086</td>\n",
       "      <td>39.521719</td>\n",
       "      <td>1713.998</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>5.131474</td>\n",
       "      <td>2405.074</td>\n",
       "      <td>128.940</td>\n",
       "      <td>2.127162</td>\n",
       "      <td>25.927811</td>\n",
       "      <td>5998.5965</td>\n",
       "      <td>1124.5091</td>\n",
       "      <td>0.878903</td>\n",
       "      <td>9.536864</td>\n",
       "      <td>2478.505829</td>\n",
       "      <td>290.46875</td>\n",
       "      <td>3.178987</td>\n",
       "      <td>19.136392</td>\n",
       "      <td>8964.742385</td>\n",
       "      <td>826.867708</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>2.455393</td>\n",
       "      <td>1161.867</td>\n",
       "      <td>71.424</td>\n",
       "      <td>2820</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1031.390015</td>\n",
       "      <td>996.702026</td>\n",
       "      <td>998.617004</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>2.221850e+08</td>\n",
       "      <td>946.907013</td>\n",
       "      <td>898.401001</td>\n",
       "      <td>746.116516</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>921.590027</td>\n",
       "      <td>123.348999</td>\n",
       "      <td>74.842987</td>\n",
       "      <td>100.159973</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5.862605</td>\n",
       "      <td>29.575341</td>\n",
       "      <td>9814.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>101.066</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.830037</td>\n",
       "      <td>1389.482</td>\n",
       "      <td>0.104272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>0.087896</td>\n",
       "      <td>183.454</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>433.3606</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.101119</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>169.272860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>756.365240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.851201</td>\n",
       "      <td>8120.911</td>\n",
       "      <td>22.470165</td>\n",
       "      <td>457.140</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>1356.676</td>\n",
       "      <td>189.144</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>15.705079</td>\n",
       "      <td>1527.9880</td>\n",
       "      <td>240.2550</td>\n",
       "      <td>1.087970</td>\n",
       "      <td>17.264859</td>\n",
       "      <td>1821.262586</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>2.816607</td>\n",
       "      <td>15.955071</td>\n",
       "      <td>4714.999394</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.404212</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>676.651</td>\n",
       "      <td>110.028</td>\n",
       "      <td>1674</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1044.079956</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1021.599976</td>\n",
       "      <td>1043.839966</td>\n",
       "      <td>1.851680e+08</td>\n",
       "      <td>970.488983</td>\n",
       "      <td>904.745972</td>\n",
       "      <td>746.543015</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>919.495972</td>\n",
       "      <td>139.093994</td>\n",
       "      <td>73.350983</td>\n",
       "      <td>124.343994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>2257.83</td>\n",
       "      <td>2257.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>4.271773</td>\n",
       "      <td>17.128804</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>125.065</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.824412</td>\n",
       "      <td>1571.329</td>\n",
       "      <td>0.106123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>209.624</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>433.0382</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.206345</td>\n",
       "      <td>191.342678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>873.635908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.674829</td>\n",
       "      <td>7004.225</td>\n",
       "      <td>13.556661</td>\n",
       "      <td>460.631</td>\n",
       "      <td>0.504505</td>\n",
       "      <td>2.465480</td>\n",
       "      <td>961.587</td>\n",
       "      <td>68.885</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>7.153452</td>\n",
       "      <td>1501.3400</td>\n",
       "      <td>182.1425</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>3.057769</td>\n",
       "      <td>689.644645</td>\n",
       "      <td>76.87500</td>\n",
       "      <td>1.916536</td>\n",
       "      <td>4.794911</td>\n",
       "      <td>3652.918407</td>\n",
       "      <td>110.797619</td>\n",
       "      <td>0.338716</td>\n",
       "      <td>1.856071</td>\n",
       "      <td>645.592</td>\n",
       "      <td>68.885</td>\n",
       "      <td>1906</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1159.420044</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1044.400024</td>\n",
       "      <td>1154.729980</td>\n",
       "      <td>3.449460e+08</td>\n",
       "      <td>1031.837524</td>\n",
       "      <td>962.416016</td>\n",
       "      <td>752.111511</td>\n",
       "      <td>704.654510</td>\n",
       "      <td>920.382019</td>\n",
       "      <td>192.313965</td>\n",
       "      <td>122.892456</td>\n",
       "      <td>234.347961</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>2270.75</td>\n",
       "      <td>2270.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3.549738</td>\n",
       "      <td>8.126814</td>\n",
       "      <td>6102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.060350</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>103.742</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>1430.305</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>184.961</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.552156</td>\n",
       "      <td>419.5827</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>180.724524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>0.229343</td>\n",
       "      <td>751.068731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987909</td>\n",
       "      <td>5136.216</td>\n",
       "      <td>5.895492</td>\n",
       "      <td>89.270</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>2.164643</td>\n",
       "      <td>784.089</td>\n",
       "      <td>61.320</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>5.255419</td>\n",
       "      <td>1307.0479</td>\n",
       "      <td>104.6719</td>\n",
       "      <td>0.445339</td>\n",
       "      <td>3.005284</td>\n",
       "      <td>765.537930</td>\n",
       "      <td>102.20000</td>\n",
       "      <td>1.685690</td>\n",
       "      <td>4.534309</td>\n",
       "      <td>2897.701636</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>0.256962</td>\n",
       "      <td>0.913094</td>\n",
       "      <td>441.718</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1719</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1191.099976</td>\n",
       "      <td>910.416992</td>\n",
       "      <td>1156.729980</td>\n",
       "      <td>1013.380005</td>\n",
       "      <td>5.101990e+08</td>\n",
       "      <td>1050.758484</td>\n",
       "      <td>978.255981</td>\n",
       "      <td>754.891266</td>\n",
       "      <td>704.777008</td>\n",
       "      <td>970.403015</td>\n",
       "      <td>35.124023</td>\n",
       "      <td>-37.378479</td>\n",
       "      <td>42.976990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>2269.00</td>\n",
       "      <td>2269.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>3.924113</td>\n",
       "      <td>12.858644</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.077027</td>\n",
       "      <td>53.837</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>676.381</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106214</td>\n",
       "      <td>0.093740</td>\n",
       "      <td>86.777</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.569012</td>\n",
       "      <td>157.6007</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.209669</td>\n",
       "      <td>76.605181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>358.376961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.997778</td>\n",
       "      <td>3266.185</td>\n",
       "      <td>11.091972</td>\n",
       "      <td>207.000</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>1.227200</td>\n",
       "      <td>436.993</td>\n",
       "      <td>13.542</td>\n",
       "      <td>0.919273</td>\n",
       "      <td>5.926655</td>\n",
       "      <td>751.0461</td>\n",
       "      <td>54.4425</td>\n",
       "      <td>0.336855</td>\n",
       "      <td>2.692034</td>\n",
       "      <td>275.210130</td>\n",
       "      <td>20.60000</td>\n",
       "      <td>2.223944</td>\n",
       "      <td>6.649812</td>\n",
       "      <td>1816.962092</td>\n",
       "      <td>135.585000</td>\n",
       "      <td>0.375542</td>\n",
       "      <td>1.531212</td>\n",
       "      <td>306.818</td>\n",
       "      <td>25.938</td>\n",
       "      <td>817</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>7275.860000</td>\n",
       "      <td>7076.420000</td>\n",
       "      <td>7202.000000</td>\n",
       "      <td>7254.740000</td>\n",
       "      <td>3.364270e+04</td>\n",
       "      <td>7366.845000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7580.227500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8682.360000</td>\n",
       "      <td>162.240000</td>\n",
       "      <td>-112.105000</td>\n",
       "      <td>-1427.620000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>1</td>\n",
       "      <td>3240.02</td>\n",
       "      <td>3240.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>5.111582</td>\n",
       "      <td>17.568946</td>\n",
       "      <td>3619.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>44.191</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834208</td>\n",
       "      <td>590.619</td>\n",
       "      <td>0.105560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.089176</td>\n",
       "      <td>73.196</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.560205</td>\n",
       "      <td>142.7691</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.204129</td>\n",
       "      <td>69.411507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430993</td>\n",
       "      <td>0.231762</td>\n",
       "      <td>305.142919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.995823</td>\n",
       "      <td>3537.043</td>\n",
       "      <td>14.679252</td>\n",
       "      <td>252.057</td>\n",
       "      <td>0.651510</td>\n",
       "      <td>2.487959</td>\n",
       "      <td>461.269</td>\n",
       "      <td>40.950</td>\n",
       "      <td>1.337016</td>\n",
       "      <td>8.914525</td>\n",
       "      <td>946.6073</td>\n",
       "      <td>173.8547</td>\n",
       "      <td>0.532183</td>\n",
       "      <td>3.815865</td>\n",
       "      <td>376.785520</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>2.651977</td>\n",
       "      <td>9.292295</td>\n",
       "      <td>1877.599807</td>\n",
       "      <td>145.963889</td>\n",
       "      <td>0.348597</td>\n",
       "      <td>1.205841</td>\n",
       "      <td>246.807</td>\n",
       "      <td>19.520</td>\n",
       "      <td>708</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>7365.010000</td>\n",
       "      <td>7238.670000</td>\n",
       "      <td>7254.770000</td>\n",
       "      <td>7316.140000</td>\n",
       "      <td>2.684898e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7563.750000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8404.520000</td>\n",
       "      <td>223.640000</td>\n",
       "      <td>-69.760000</td>\n",
       "      <td>-1088.380000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4.245443</td>\n",
       "      <td>17.229820</td>\n",
       "      <td>3494.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>52.124</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.829598</td>\n",
       "      <td>682.759</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>88.119</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.211699</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>174.2284</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.209074</td>\n",
       "      <td>77.704314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437290</td>\n",
       "      <td>0.241058</td>\n",
       "      <td>359.889446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.338086</td>\n",
       "      <td>3570.245</td>\n",
       "      <td>13.412452</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>4.685706</td>\n",
       "      <td>550.055</td>\n",
       "      <td>121.472</td>\n",
       "      <td>1.296249</td>\n",
       "      <td>10.637944</td>\n",
       "      <td>1066.8126</td>\n",
       "      <td>219.1752</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>7.563631</td>\n",
       "      <td>600.285630</td>\n",
       "      <td>189.80000</td>\n",
       "      <td>2.335994</td>\n",
       "      <td>8.791418</td>\n",
       "      <td>1922.523426</td>\n",
       "      <td>182.500000</td>\n",
       "      <td>0.284131</td>\n",
       "      <td>0.794803</td>\n",
       "      <td>233.840</td>\n",
       "      <td>9.240</td>\n",
       "      <td>823</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>7528.450000</td>\n",
       "      <td>7288.000000</td>\n",
       "      <td>7315.360000</td>\n",
       "      <td>7388.240000</td>\n",
       "      <td>3.138711e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7092.500000</td>\n",
       "      <td>7535.250000</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8439.000000</td>\n",
       "      <td>295.740000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>-1050.760000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>5.417252</td>\n",
       "      <td>22.933473</td>\n",
       "      <td>5401.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.075380</td>\n",
       "      <td>61.556</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.833728</td>\n",
       "      <td>831.227</td>\n",
       "      <td>0.111242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104532</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>104.218</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.230646</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>229.9543</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>98.192138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.222258</td>\n",
       "      <td>426.880618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.615048</td>\n",
       "      <td>5598.203</td>\n",
       "      <td>18.445569</td>\n",
       "      <td>423.342</td>\n",
       "      <td>0.720659</td>\n",
       "      <td>3.736793</td>\n",
       "      <td>718.497</td>\n",
       "      <td>98.658</td>\n",
       "      <td>0.974704</td>\n",
       "      <td>13.650818</td>\n",
       "      <td>971.7796</td>\n",
       "      <td>263.0880</td>\n",
       "      <td>0.689802</td>\n",
       "      <td>8.775038</td>\n",
       "      <td>687.732899</td>\n",
       "      <td>261.00000</td>\n",
       "      <td>3.041377</td>\n",
       "      <td>13.345471</td>\n",
       "      <td>3032.252472</td>\n",
       "      <td>326.250000</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>2.259901</td>\n",
       "      <td>508.023</td>\n",
       "      <td>39.933</td>\n",
       "      <td>997</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>7408.240000</td>\n",
       "      <td>7220.000000</td>\n",
       "      <td>7388.430000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>2.960591e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8340.580000</td>\n",
       "      <td>180.810000</td>\n",
       "      <td>-139.900000</td>\n",
       "      <td>-1094.580000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0</td>\n",
       "      <td>3221.29</td>\n",
       "      <td>3221.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>3.635659</td>\n",
       "      <td>7.954076</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.072388</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>65.366</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>737.583</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110788</td>\n",
       "      <td>0.098716</td>\n",
       "      <td>100.042</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.158129</td>\n",
       "      <td>0.585552</td>\n",
       "      <td>142.7909</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.213328</td>\n",
       "      <td>80.145271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>0.232719</td>\n",
       "      <td>401.635726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.670367</td>\n",
       "      <td>3314.341</td>\n",
       "      <td>6.540559</td>\n",
       "      <td>90.000</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.899961</td>\n",
       "      <td>414.253</td>\n",
       "      <td>9.504</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>4.905297</td>\n",
       "      <td>427.4661</td>\n",
       "      <td>33.4815</td>\n",
       "      <td>0.276536</td>\n",
       "      <td>1.864274</td>\n",
       "      <td>249.711727</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>1.890935</td>\n",
       "      <td>3.360099</td>\n",
       "      <td>1707.514271</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>0.326038</td>\n",
       "      <td>0.801756</td>\n",
       "      <td>294.412</td>\n",
       "      <td>14.790</td>\n",
       "      <td>903</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>7320.000000</td>\n",
       "      <td>7145.010000</td>\n",
       "      <td>7246.000000</td>\n",
       "      <td>7195.230000</td>\n",
       "      <td>2.595445e+04</td>\n",
       "      <td>7385.900000</td>\n",
       "      <td>7065.190000</td>\n",
       "      <td>7588.047500</td>\n",
       "      <td>8442.500000</td>\n",
       "      <td>8615.000000</td>\n",
       "      <td>130.040000</td>\n",
       "      <td>-190.670000</td>\n",
       "      <td>-1419.770000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>3230.78</td>\n",
       "      <td>3230.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  score_mean  score_std  score_sum  score_max  v_neg_mean  \\\n",
       "0     2017-01-01    4.095483   8.406436     3989.0      116.0    0.061278   \n",
       "1     2017-01-02    7.143617  44.739341    20145.0     1859.0    0.061524   \n",
       "2     2017-01-03    5.862605  29.575341     9814.0      570.0    0.060374   \n",
       "3     2017-01-04    4.271773  17.128804     8142.0      599.0    0.065616   \n",
       "4     2017-01-05    3.549738   8.126814     6102.0      146.0    0.060350   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "1090  2019-12-27    3.924113  12.858644     3206.0      207.0    0.065896   \n",
       "1091  2019-12-28    5.111582  17.568946     3619.0      281.0    0.062417   \n",
       "1092  2019-12-29    4.245443  17.229820     3494.0      292.0    0.063334   \n",
       "1093  2019-12-30    5.417252  22.933473     5401.0      522.0    0.061741   \n",
       "1094  2019-12-31    3.635659   7.954076     3283.0      102.0    0.072388   \n",
       "\n",
       "      v_neg_std  v_neg_sum  v_neg_max  v_neu_mean  v_neu_sum  v_neu_std  \\\n",
       "0      0.074932     59.685      0.674    0.829154    807.596   0.108736   \n",
       "1      0.067844    173.499      0.531    0.829861   2340.209   0.101984   \n",
       "2      0.071586    101.066      0.579    0.830037   1389.482   0.104272   \n",
       "3      0.073589    125.065      0.582    0.824412   1571.329   0.106123   \n",
       "4      0.072556    103.742      0.565    0.832056   1430.305   0.109600   \n",
       "...         ...        ...        ...         ...        ...        ...   \n",
       "1090   0.077027     53.837      0.600    0.827884    676.381   0.113621   \n",
       "1091   0.074158     44.191      0.461    0.834208    590.619   0.105560   \n",
       "1092   0.069795     52.124      0.469    0.829598    682.759   0.108458   \n",
       "1093   0.075380     61.556      0.753    0.833728    831.227   0.111242   \n",
       "1094   0.070688     65.366      0.427    0.816814    737.583   0.112108   \n",
       "\n",
       "      v_neu_max  v_pos_mean  v_pos_std  v_pos_sum  v_pos_max  v_compound_mean  \\\n",
       "0           1.0    0.109578   0.091042    106.729      0.579         0.247951   \n",
       "1           1.0    0.108612   0.087565    306.287      0.674         0.240968   \n",
       "2           1.0    0.109590   0.087896    183.454      0.636         0.258877   \n",
       "3           1.0    0.109981   0.090355    209.624      0.735         0.227197   \n",
       "4           1.0    0.107598   0.092280    184.961      0.627         0.244085   \n",
       "...         ...         ...        ...        ...        ...              ...   \n",
       "1090        1.0    0.106214   0.093740     86.777      0.620         0.192902   \n",
       "1091        1.0    0.103384   0.089176     73.196      0.482         0.201651   \n",
       "1092        1.0    0.107070   0.091100     88.119      0.502         0.211699   \n",
       "1093        1.0    0.104532   0.092085    104.218      0.592         0.230646   \n",
       "1094        1.0    0.110788   0.098716    100.042      0.645         0.158129   \n",
       "\n",
       "      v_compound_std  v_compound_sum  v_compound_max  t_pol_mean  t_pol_std  \\\n",
       "0           0.550243        241.5044          0.9984    0.102978   0.202279   \n",
       "1           0.569902        679.5298          0.9997    0.095610   0.186729   \n",
       "2           0.560860        433.3606          0.9987    0.101119   0.209033   \n",
       "3           0.575732        433.0382          0.9998    0.100390   0.206345   \n",
       "4           0.552156        419.5827          0.9984    0.105134   0.199678   \n",
       "...              ...             ...             ...         ...        ...   \n",
       "1090        0.569012        157.6007          0.9979    0.093764   0.209669   \n",
       "1091        0.560205        142.7691          0.9988    0.098039   0.204129   \n",
       "1092        0.561760        174.2284          0.9979    0.094416   0.209074   \n",
       "1093        0.564663        229.9543          0.9979    0.098488   0.195556   \n",
       "1094        0.585552        142.7909          0.9980    0.088754   0.213328   \n",
       "\n",
       "       t_pol_sum  t_pol_max  t_sub_mean  t_sub_std    t_sub_sum  t_sub_max  \\\n",
       "0     100.300454        1.0    0.447966   0.221794   436.318864        1.0   \n",
       "1     269.620143        1.0    0.444555   0.214772  1253.644989        1.0   \n",
       "2     169.272860        1.0    0.451831   0.216360   756.365240        1.0   \n",
       "3     191.342678        1.0    0.458361   0.207941   873.635908        1.0   \n",
       "4     180.724524        1.0    0.436922   0.229343   751.068731        1.0   \n",
       "...          ...        ...         ...        ...          ...        ...   \n",
       "1090   76.605181        1.0    0.438650   0.241026   358.376961        1.0   \n",
       "1091   69.411507        1.0    0.430993   0.231762   305.142919        1.0   \n",
       "1092   77.704314        1.0    0.437290   0.241058   359.889446        1.0   \n",
       "1093   98.192138        1.0    0.428165   0.222258   426.880618        1.0   \n",
       "1094   80.145271        1.0    0.444779   0.232719   401.635726        1.0   \n",
       "\n",
       "      v_neu_score_mean  v_neu_score_sum  v_neu_score_std  v_neu_score_max  \\\n",
       "0             3.703236         3606.952         6.537687           92.950   \n",
       "1             6.206413        17502.086        39.521719         1713.998   \n",
       "2             4.851201         8120.911        22.470165          457.140   \n",
       "3             3.674829         7004.225        13.556661          460.631   \n",
       "4             2.987909         5136.216         5.895492           89.270   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          3.997778         3266.185        11.091972          207.000   \n",
       "1091          4.995823         3537.043        14.679252          252.057   \n",
       "1092          4.338086         3570.245        13.412452          229.000   \n",
       "1093          5.615048         5598.203        18.445569          423.342   \n",
       "1094          3.670367         3314.341         6.540559           90.000   \n",
       "\n",
       "      v_pos_score_mean  v_pos_score_std  v_pos_score_sum  v_pos_score_max  \\\n",
       "0             0.540065         1.899358          526.023           37.584   \n",
       "1             0.852863         5.131474         2405.074          128.940   \n",
       "2             0.810440         7.089782         1356.676          189.144   \n",
       "3             0.504505         2.465480          961.587           68.885   \n",
       "4             0.456131         2.164643          784.089           61.320   \n",
       "...                ...              ...              ...              ...   \n",
       "1090          0.534875         1.227200          436.993           13.542   \n",
       "1091          0.651510         2.487959          461.269           40.950   \n",
       "1092          0.668354         4.685706          550.055          121.472   \n",
       "1093          0.720659         3.736793          718.497           98.658   \n",
       "1094          0.458752         0.899961          414.253            9.504   \n",
       "\n",
       "      v_compound_score_mean  v_compound_score_std  v_compound_score_sum  \\\n",
       "0                  0.936370              4.894707              912.0243   \n",
       "1                  2.127162             25.927811             5998.5965   \n",
       "2                  0.912777             15.705079             1527.9880   \n",
       "3                  0.787692              7.153452             1501.3400   \n",
       "4                  0.760354              5.255419             1307.0479   \n",
       "...                     ...                   ...                   ...   \n",
       "1090               0.919273              5.926655              751.0461   \n",
       "1091               1.337016              8.914525              946.6073   \n",
       "1092               1.296249             10.637944             1066.8126   \n",
       "1093               0.974704             13.650818              971.7796   \n",
       "1094               0.473384              4.905297              427.4661   \n",
       "\n",
       "      v_compound_score_max  t_pol_score_mean  t_pol_score_std  \\\n",
       "0                  40.8155          0.470360         1.918625   \n",
       "1                1124.5091          0.878903         9.536864   \n",
       "2                 240.2550          1.087970        17.264859   \n",
       "3                 182.1425          0.361828         3.057769   \n",
       "4                 104.6719          0.445339         3.005284   \n",
       "...                    ...               ...              ...   \n",
       "1090               54.4425          0.336855         2.692034   \n",
       "1091              173.8547          0.532183         3.815865   \n",
       "1092              219.1752          0.729387         7.563631   \n",
       "1093              263.0880          0.689802         8.775038   \n",
       "1094               33.4815          0.276536         1.864274   \n",
       "\n",
       "      t_pol_score_sum  t_pol_score_max  t_sub_score_mean  t_sub_score_std  \\\n",
       "0          458.130558         24.70000          1.890892         3.280482   \n",
       "1         2478.505829        290.46875          3.178987        19.136392   \n",
       "2         1821.262586        456.00000          2.816607        15.955071   \n",
       "3          689.644645         76.87500          1.916536         4.794911   \n",
       "4          765.537930        102.20000          1.685690         4.534309   \n",
       "...               ...              ...               ...              ...   \n",
       "1090       275.210130         20.60000          2.223944         6.649812   \n",
       "1091       376.785520         75.00000          2.651977         9.292295   \n",
       "1092       600.285630        189.80000          2.335994         8.791418   \n",
       "1093       687.732899        261.00000          3.041377        13.345471   \n",
       "1094       249.711727         11.25000          1.890935         3.360099   \n",
       "\n",
       "      t_sub_score_sum  t_sub_score_max  v_neg_score_mean  v_neg_score_std  \\\n",
       "0         1841.728790        32.400000          0.285408         0.883918   \n",
       "1         8964.742385       826.867708          0.412010         2.455393   \n",
       "2         4714.999394       399.000000          0.404212         3.511884   \n",
       "3         3652.918407       110.797619          0.338716         1.856071   \n",
       "4         2897.701636        87.600000          0.256962         0.913094   \n",
       "...               ...              ...               ...              ...   \n",
       "1090      1816.962092       135.585000          0.375542         1.531212   \n",
       "1091      1877.599807       145.963889          0.348597         1.205841   \n",
       "1092      1922.523426       182.500000          0.284131         0.794803   \n",
       "1093      3032.252472       326.250000          0.509552         2.259901   \n",
       "1094      1707.514271        38.250000          0.326038         0.801756   \n",
       "\n",
       "      v_neg_score_sum  v_neg_score_max  date_count        Date         High  \\\n",
       "0             277.987           17.050         974  2017-01-01  1003.080017   \n",
       "1            1161.867           71.424        2820  2017-01-02  1031.390015   \n",
       "2             676.651          110.028        1674  2017-01-03  1044.079956   \n",
       "3             645.592           68.885        1906  2017-01-04  1159.420044   \n",
       "4             441.718           16.800        1719  2017-01-05  1191.099976   \n",
       "...               ...              ...         ...         ...          ...   \n",
       "1090          306.818           25.938         817  2019-12-27  7275.860000   \n",
       "1091          246.807           19.520         708  2019-12-28  7365.010000   \n",
       "1092          233.840            9.240         823  2019-12-29  7528.450000   \n",
       "1093          508.023           39.933         997  2019-12-30  7408.240000   \n",
       "1094          294.412           14.790         903  2019-12-31  7320.000000   \n",
       "\n",
       "              Low         Open        Close        Volume   tenkan_sen  \\\n",
       "0      958.698975   963.658020   998.325012  1.477750e+08   932.752014   \n",
       "1      996.702026   998.617004  1021.750000  2.221850e+08   946.907013   \n",
       "2     1021.599976  1021.599976  1043.839966  1.851680e+08   970.488983   \n",
       "3     1044.400024  1044.400024  1154.729980  3.449460e+08  1031.837524   \n",
       "4      910.416992  1156.729980  1013.380005  5.101990e+08  1050.758484   \n",
       "...           ...          ...          ...           ...          ...   \n",
       "1090  7076.420000  7202.000000  7254.740000  3.364270e+04  7366.845000   \n",
       "1091  7238.670000  7254.770000  7316.140000  2.684898e+04  7385.900000   \n",
       "1092  7288.000000  7315.360000  7388.240000  3.138711e+04  7385.900000   \n",
       "1093  7220.000000  7388.430000  7246.000000  2.960591e+04  7385.900000   \n",
       "1094  7145.010000  7246.000000  7195.230000  2.595445e+04  7385.900000   \n",
       "\n",
       "        kijun_sen  senkou_span_a  senkou_span_b  chikou_span  diff_kijun  \\\n",
       "0      881.415009     745.597763     704.654510   919.750000  116.910004   \n",
       "1      898.401001     746.116516     704.654510   921.590027  123.348999   \n",
       "2      904.745972     746.543015     704.654510   919.495972  139.093994   \n",
       "3      962.416016     752.111511     704.654510   920.382019  192.313965   \n",
       "4      978.255981     754.891266     704.777008   970.403015   35.124023   \n",
       "...           ...            ...            ...          ...         ...   \n",
       "1090  7092.500000    7580.227500    8442.500000  8682.360000  162.240000   \n",
       "1091  7092.500000    7563.750000    8442.500000  8404.520000  223.640000   \n",
       "1092  7092.500000    7535.250000    8442.500000  8439.000000  295.740000   \n",
       "1093  7065.190000    7588.047500    8442.500000  8340.580000  180.810000   \n",
       "1094  7065.190000    7588.047500    8442.500000  8615.000000  130.040000   \n",
       "\n",
       "      diff_tenkan  diff_chikou  kijun_signal  tenkan_signal  chikou_signal  \\\n",
       "0       65.572998    78.575012             1              1              1   \n",
       "1       74.842987   100.159973             1              1              1   \n",
       "2       73.350983   124.343994             1              1              1   \n",
       "3      122.892456   234.347961             1              1              1   \n",
       "4      -37.378479    42.976990             0              0              0   \n",
       "...           ...          ...           ...            ...            ...   \n",
       "1090  -112.105000 -1427.620000             1             -1             -1   \n",
       "1091   -69.760000 -1088.380000             1             -1             -1   \n",
       "1092     2.340000 -1050.760000             1              0             -1   \n",
       "1093  -139.900000 -1094.580000             1             -1             -1   \n",
       "1094  -190.670000 -1419.770000             1             -1             -1   \n",
       "\n",
       "      indice  date_time  class  Close_ssp  Close_nasdaq  \n",
       "0          0 2017-01-01      1        NaN           NaN  \n",
       "1          0 2017-01-02      1        NaN           NaN  \n",
       "2          0 2017-01-03      1    2257.83       2257.83  \n",
       "3          0 2017-01-04      1    2270.75       2270.75  \n",
       "4          0 2017-01-05      0    2269.00       2269.00  \n",
       "...      ...        ...    ...        ...           ...  \n",
       "1090       0 2019-12-27      1    3240.02       3240.02  \n",
       "1091       0 2019-12-28      1        NaN           NaN  \n",
       "1092       0 2019-12-29      1        NaN           NaN  \n",
       "1093       0 2019-12-30      0    3221.29       3221.29  \n",
       "1094       0 2019-12-31      0    3230.78       3230.78  \n",
       "\n",
       "[1095 rows x 76 columns]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_daily = daily.merge(merge, left_on=\"date_time\", right_on=\"date_time_nasdaq\",how=\"left\")\n",
    "new_daily = new_daily.drop(columns=[\"date_time_ssp\",\"date_time_nasdaq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_daily[[\"Close_ssp\",\"Close_nasdaq\"]] = new_daily[[\"Close_ssp\",\"Close_nasdaq\"]].fillna(method=\"ffill\")\n",
    "new_daily = new_daily[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_daily.to_csv(\"with_nas_sep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Handling of data NASDAQ\"\"\"\n",
    "data_nasdaq=pd.read_csv(\"/Users/Axel/Desktop/^IXIC.csv\")\n",
    "data_nasdaq = data_nasdaq[[\"Date\",\"Adj Close\"]]\n",
    "data_nasdaq[\"Close Nasdaq\"] = data_nasdaq[\"Adj Close\"]\n",
    "data_nasdaq['Date'] = data_nasdaq['Date'].astype('datetime64[ns]')\n",
    "data_nasdaq=data_nasdaq.drop(columns=[\"Adj Close\"])\n",
    "data_nasdaq = data_nasdaq.iloc[1:]\n",
    "#data_nasdaq=data_nasdaq.drop(data_nasdaq.head(1))\n",
    "\"\"\"Handling of data S&P\"\"\"\n",
    "data_sp = pd.read_csv(\"/Users/Axel/Desktop/HistoricalPrices.csv\")\n",
    "data_sp = data_sp[[\"Date\",\" Close\"]]\n",
    "data_sp[\"Close S&P\"] = data_sp[\" Close\"]\n",
    "data_sp=data_sp.drop(columns=[\" Close\"])\n",
    "data_sp=data_sp.reindex(index=data_sp.index[::-1])\n",
    "data_sp['Date'] = data_sp['Date'].astype('datetime64[ns]')\n",
    "data_sp=data_sp.iloc[:-2]\n",
    "date_df=pd.DataFrame(pd.date_range(start=\"2017-08-21\",end=\"2022-05-27\"),columns=[\"Date\"])\n",
    "date_df=date_df.merge(data_sp,on=\"Date\",how=\"left\")\n",
    "date_df=date_df.merge(data_nasdaq,on=\"Date\",how=\"left\")\n",
    "date_df=date_df.fillna(method=\"ffill\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a817ea86a33780728826858f9043029cc6f1a8cbe632e9a079699ee2b0bc566"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
